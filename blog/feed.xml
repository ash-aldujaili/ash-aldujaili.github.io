<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://ash-aldujaili.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://ash-aldujaili.github.io/blog/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-02-20T19:42:24-08:00</updated><id>http://ash-aldujaili.github.io/blog/</id><title type="html">Hunting Optima</title><subtitle>Write-ups on optimization</subtitle><author><name>Abdullah Al-Dujaili</name></author><entry><title type="html">Expected Improvement for Bayesian Optimization: A Derivation</title><link href="http://ash-aldujaili.github.io/blog/2018/02/01/ei/" rel="alternate" type="text/html" title="Expected Improvement for Bayesian Optimization: A Derivation" /><published>2018-02-01T00:00:00-08:00</published><updated>2018-02-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/02/01/ei</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/02/01/ei/">&lt;p&gt;In this post, we derive the closed-form expression of the Expected Improvement $($EI$)$ criterion commonly used in Bayesian Optimization.&lt;/p&gt;

&lt;p&gt;Modelled with a Gaussian Process, the function value at a given point $x$ can be considered as a normal random variable with mean $\mu$ and variance $\sigma^2$. Given the best $($minimum in a minimization setup$)$ function value obtained so far-let’s denote it by 
$f^*$:&lt;/p&gt;

&lt;p&gt;we are interested in quantifying the improvement over 
$f^*$ 
we will have 
if we sample a point 
$x$ 
. Mathematically, the improvement at 
$x$ 
can be expressed as follows&lt;/p&gt;

&lt;p&gt;$I(x) = \max(f^* - Y,0)$&lt;/p&gt;

&lt;p&gt;where $Y$ is the random variable $\sim \mathcal{N}(\mu, \sigma^2)$ that corresponds to the function value at $x$. Since $I$ is a random variable, one can consider the average $($expected$)$ improvement $($EI$)$ to assess $x$:&lt;/p&gt;

&lt;p&gt;$EI(x) =  E_{Y\sim \mathcal{N}(\mu, \sigma^2)}[I(x)]$&lt;/p&gt;

&lt;p&gt;With the reparameterization trick, $Y=\mu + \sigma \epsilon$ where 
$\epsilon\sim\mathcal{N}(0,1)$, we have:&lt;/p&gt;

&lt;p&gt;$EI(x) =  E_{\epsilon\sim \mathcal{N}(0,1)}[I(x)]$&lt;/p&gt;

&lt;p&gt;which can be written as $($from linearity of integral, and the definition of  $\frac{d}{d\epsilon}e^{-\epsilon^2 / 2}$ derivative $)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x) = \int_{-\infty}^{\infty} I(x) \phi(\epsilon) d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x) =  \int_{-\infty}^{(f^*-\mu)/\sigma} (f^* - \mu - \sigma \epsilon) \phi(\epsilon) d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)= (f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) - \sigma \int_{-\infty}^{(f^*-\mu)/\sigma} \epsilon \phi(\epsilon) d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \frac{\sigma}{
\sqrt{2\pi}} \int_{-\infty}^{(f^*-\mu)/\sigma} (-\epsilon) e^{-\epsilon^2/2} d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \frac{\sigma}{
\sqrt{2\pi}}  e^{-\epsilon^2/2}|_{-\infty}^{(f^*-\mu)/\sigma}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \sigma
  \big(\phi(\frac{f^*-\mu}{\sigma}) - 0\big)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \sigma
  \phi(\frac{f^*-\mu}{\sigma})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $\phi, \Phi$ are the PDF, CDF of standard normal distribution, respectively.&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we derive the closed-form expression of the Expected Improvement $($EI$)$ criterion commonly used in Bayesian Optimization. Modelled with a Gaussian Process, the function value at a given point $x$ can be considered as a normal random variable with mean $\mu$ and variance $\sigma^2$. Given the best $($minimum in a minimization setup$)$ function value obtained so far-let’s denote it by $f^*$: we are interested in quantifying the improvement over $f^*$ we will have if we sample a point $x$ . Mathematically, the improvement at $x$ can be expressed as follows $I(x) = \max(f^* - Y,0)$ where $Y$ is the random variable $\sim \mathcal{N}(\mu, \sigma^2)$ that corresponds to the function value at $x$. Since $I$ is a random variable, one can consider the average $($expected$)$ improvement $($EI$)$ to assess $x$: $EI(x) = E_{Y\sim \mathcal{N}(\mu, \sigma^2)}[I(x)]$ With the reparameterization trick, $Y=\mu + \sigma \epsilon$ where $\epsilon\sim\mathcal{N}(0,1)$, we have: $EI(x) = E_{\epsilon\sim \mathcal{N}(0,1)}[I(x)]$ which can be written as $($from linearity of integral, and the definition of $\frac{d}{d\epsilon}e^{-\epsilon^2 / 2}$ derivative $)$ where $\phi, \Phi$ are the PDF, CDF of standard normal distribution, respectively.</summary></entry><entry><title type="html">Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts</title><link href="http://ash-aldujaili.github.io/blog/2018/01/08/wma/" rel="alternate" type="text/html" title="Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts" /><published>2018-01-08T00:00:00-08:00</published><updated>2018-01-08T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/01/08/wma</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/01/08/wma/">&lt;p&gt;In this post, we demonstrate the theoretical gurantee on the performance of the &lt;strong&gt;Weighted Majority Alogorithm&lt;/strong&gt; (&lt;em&gt;WMA&lt;/em&gt;) with empirical validation. WMA is one of the beautiful algorithms in the framework of online learning and sequential decision making under uncertainity.&lt;/p&gt;

&lt;p&gt;First, let’s describe a setup where WMA can be used. Assume that a friend of yours challenges to a &lt;strong&gt;True or False&lt;/strong&gt; quiz where you’re allowed to seek help / advice from $n$ advisors throughout the game.&lt;/p&gt;

&lt;p&gt;With this challenge at hand, WMA helps you play the game such that the number of your mistakes is upper bounded by roughly twice the number of mistakes made by your best advisor, i.e., the least number of mistakes made among your $n$ advisors. In the next paragraph, we’ll see how you can use WMA in the game.&lt;/p&gt;

&lt;p&gt;For each advisor $i$ and question (round) $t$, WMA associates a weight $w^t_i$. With $w^1_i=1\;, \forall i=1,\ldots, n.$ Assume the game has $T$ rounds. If advisor $i$ makes a mistake at round $t$, its weight is adjusted by a multiplicative factor $1-\eta$ where $\eta\in(0,1/2)$. That is, $w^{t+1}_i= (1-\eta) w^t_i$ if $i$ makes a mistake. Otherwise, $w^{t+1}_i=w^t_i$. At each round $t$, your decision is governed by the sum of weights of advisors who think the answer is &lt;strong&gt;True&lt;/strong&gt; versus that of those who think the answer should be &lt;strong&gt;False&lt;/strong&gt;. Mathematically, denote your answer at round $t$ by $y^t$, and the advisors’ answers by $x^t_i$. These variables take the values $0$ and $1$ to represent &lt;em&gt;True&lt;/em&gt; and &lt;em&gt;False&lt;/em&gt; respectively: we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^t = \mathbf{1}\big\{\sum_{i}x^t_i * w^t_i \geq \sum_{i} (1-x^t_i) * w^t_i \big\}&lt;/script&gt;

&lt;p&gt;Let’s put WMA in action. Below, I have created $n$ advisors whose decisions at each round are based on tossing coins whose bias is sampled uniformly, $p_i\sim \mathcal{U}(0,1)$. Likewise, the game generates questions whose answers follows a Bernoulli distribution with $p_g\sim\mathcal{U}(0,1)$. We denote the correct answer at round $t$ by $z^t$. Also, let’s try to validate the theoretical bound on its performance. From &lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall13/cos521/lecnotes/lec8.pdf&quot;&gt;this&lt;/a&gt;, after $\hat{t}$ rounds,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|\text{your mistakes}| \leq 2 (1+ \eta)\;|\text{best_advisors_mistakes}|\; + \frac{2 \ln n}{\eta}&lt;/script&gt;

&lt;p&gt;That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2(1+\eta) \min(\sum_{t=0}^{\hat{t}}\mathbf{1}\{x^t_1 \neq z^t\}, \ldots, \sum_{t=0}^{\hat{t}}\mathbf{1}\{x^t_n \neq z^t\}) + \frac{2\ln n}{\eta}-  \sum_{t=0}^{\hat{t}} \mathbf{1}\{y^t\neq z^t\} \geq 0\;, \hat{t}= 1,\ldots, T&lt;/script&gt;

&lt;p&gt;Let’s verify the above with the following 100 games with 1000 rounds each.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np

# let's try 100 games each with 1000 rounds,  
num_runs = 100
T = 1000

diff = np.zeros((num_runs, T))
for run in range(num_runs):
    n = np.random.randint(1,10)
    ps = np.random.random(n)
    ws = np.ones(n)
    p_g = np.random.random()
    eta = 0.25

    # your mistakes
    m = 0
    # advisors mistakes
    ms = np.zeros(n, dtype=np.int32)

    for t in range(T):
        # game correct answer
        z = np.random.random() &amp;gt;= p_g
        # advisors answers
        xs = np.random.random(n) &amp;gt;= ps
        # your answer
        y = np.sum(xs * ws) &amp;gt;= np.sum((1-xs) * ws)

        # your mistakes
        m += (y != z)
        # advistors mistakes
        ms[xs != z] += 1

        # theoretical bound
        th_ub = 2 * (1+eta) * min(ms) + 2. * np.log(n) / eta
        diff[run, t] = th_ub - m
        
        # update weights
        ws[xs != z] = (1-eta) * ws[xs != z]


assert np.min(diff) &amp;gt;= 0, &quot;Theoretical gap should be non-negative&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Our mistakes never exceeded the theoretical upper limit, as can be seen from the non-negative theoretical gap across all the runs and rounds of the game.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import matplotlib.pyplot as plt

_ = plt.plot(diff.T)
plt.xlabel('round')
plt.ylabel('Theoretical Gap')
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/wma/wma.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we demonstrate the theoretical gurantee on the performance of the Weighted Majority Alogorithm (WMA) with empirical validation. WMA is one of the beautiful algorithms in the framework of online learning and sequential decision making under uncertainity. First, let’s describe a setup where WMA can be used. Assume that a friend of yours challenges to a True or False quiz where you’re allowed to seek help / advice from $n$ advisors throughout the game. With this challenge at hand, WMA helps you play the game such that the number of your mistakes is upper bounded by roughly twice the number of mistakes made by your best advisor, i.e., the least number of mistakes made among your $n$ advisors. In the next paragraph, we’ll see how you can use WMA in the game. For each advisor $i$ and question (round) $t$, WMA associates a weight $w^t_i$. With $w^1_i=1\;, \forall i=1,\ldots, n.$ Assume the game has $T$ rounds. If advisor $i$ makes a mistake at round $t$, its weight is adjusted by a multiplicative factor $1-\eta$ where $\eta\in(0,1/2)$. That is, $w^{t+1}_i= (1-\eta) w^t_i$ if $i$ makes a mistake. Otherwise, $w^{t+1}_i=w^t_i$. At each round $t$, your decision is governed by the sum of weights of advisors who think the answer is True versus that of those who think the answer should be False. Mathematically, denote your answer at round $t$ by $y^t$, and the advisors’ answers by $x^t_i$. These variables take the values $0$ and $1$ to represent True and False respectively: we have Let’s put WMA in action. Below, I have created $n$ advisors whose decisions at each round are based on tossing coins whose bias is sampled uniformly, $p_i\sim \mathcal{U}(0,1)$. Likewise, the game generates questions whose answers follows a Bernoulli distribution with $p_g\sim\mathcal{U}(0,1)$. We denote the correct answer at round $t$ by $z^t$. Also, let’s try to validate the theoretical bound on its performance. From this, after $\hat{t}$ rounds, That is, Let’s verify the above with the following 100 games with 1000 rounds each. import numpy as np # let's try 100 games each with 1000 rounds, num_runs = 100 T = 1000 diff = np.zeros((num_runs, T)) for run in range(num_runs): n = np.random.randint(1,10) ps = np.random.random(n) ws = np.ones(n) p_g = np.random.random() eta = 0.25 # your mistakes m = 0 # advisors mistakes ms = np.zeros(n, dtype=np.int32) for t in range(T): # game correct answer z = np.random.random() &amp;gt;= p_g # advisors answers xs = np.random.random(n) &amp;gt;= ps # your answer y = np.sum(xs * ws) &amp;gt;= np.sum((1-xs) * ws) # your mistakes m += (y != z) # advistors mistakes ms[xs != z] += 1 # theoretical bound th_ub = 2 * (1+eta) * min(ms) + 2. * np.log(n) / eta diff[run, t] = th_ub - m # update weights ws[xs != z] = (1-eta) * ws[xs != z] assert np.min(diff) &amp;gt;= 0, &quot;Theoretical gap should be non-negative&quot; Our mistakes never exceeded the theoretical upper limit, as can be seen from the non-negative theoretical gap across all the runs and rounds of the game. import matplotlib.pyplot as plt _ = plt.plot(diff.T) plt.xlabel('round') plt.ylabel('Theoretical Gap') plt.show()</summary></entry><entry><title type="html">A Gentle Tour through Sampling Techniques</title><link href="http://ash-aldujaili.github.io/blog/2017/12/26/sampling/" rel="alternate" type="text/html" title="A Gentle Tour through Sampling Techniques" /><published>2017-12-26T00:00:00-08:00</published><updated>2017-12-26T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/12/26/sampling</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/12/26/sampling/">&lt;p&gt;In this post, we’ll take a quick tour into the sampling world and how it might be handy in some applications. Let’s start with some setup code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import numpy.random as rnd
import seaborn as sns
import scipy
from mpl_toolkits.mplot3d import Axes3D
%pylab inline
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Populating the interactive namespace from numpy and matplotlib
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4&gt;Sampling from an aribitrary 2-D distribution&lt;/h4&gt;

&lt;p&gt;Let $x\in \mathbb{R}^2$ be a continuous random variable with from a probability distribution $q(x)$. We are interested in obtaining values that $x$ can take in a way that confirms to its probability distribution $q(x)$. Let’s assume $q(x)$ has the following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x\sim q(x)= \frac{1}{Z} [(x_1 - 0.5)^2 + (x_2 - 0.5)^2]\;,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{Z} = \int^{1}_{0} (x_1 - 0.5)^2 dx_1+ \int^{1}_{0} (x_2 - 0.5)^2 dx_2 = \frac{1}{6}&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s have a look at how does this distribution look like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n = 100
x_1s, x_2s = np.meshgrid(*[np.linspace(0,1, n)]*2)
@np.vectorize
def compute_q(x_1,x_2):
    return q([x_1,x_2])
q_s = compute_q(x_1s, x_2s)
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_trisurf(x_1s.ravel(), x_2s.ravel(), q_s.ravel(), 
                       cmap=plt.cm.BrBG_r, linewidth=0.2)
fig.colorbar(surf, shrink=0.5, aspect=5)
ax.view_init(30, 45)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_5_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;The density accumulates at the boundaries of the distribution’s support. So when we sample, we expect more values around the boundaries rather than the center. The question is how to make our sampling process conforms to the same. In the rest of this post, we will discuss three methods: Rejection Sampling, Gibbs Sampling, Metropolis Hastings.&lt;/p&gt;

&lt;h3&gt;Rejection Sampling:&lt;/h3&gt;

&lt;p&gt;The gist of this method is to find a distribution $p(x)$ from which we know how to sample. We scale $p(x)$ by a factor $C$ such that 
&lt;script type=&quot;math/tex&quot;&gt;q(x) \leq C p(x)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If the above can be obtained, then our sampling procedure is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sample $\bar{x}\sim p(\bar{x})$.&lt;/li&gt;
  &lt;li&gt;Toss a biased coin to accept $\bar{x}$ as a sample coming from $q$. The coin bias is $\frac{q(\bar{x})}{Cp(\bar{x})}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Intuitvely, you can think of this procedure as a correction scheme to account for the mismatch: how accurately $p(x)$ describes $q(x)$. Put it differently, the area under curve $Cp(x)$ is $C$, while under $q(x)$, it is $1$. And since $q(x)\leq Cp(x)$,  if we full the area under $Cp(x)$ with points uniformly, then $1/C$ of these points will be under $q(x)$.&lt;/p&gt;

&lt;p&gt;In the demo below, we choose $p$ to be the uniform distribution:
&lt;script type=&quot;math/tex&quot;&gt;x_1 \sim \mathcal{U}(0,1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;x_2 \sim \mathcal{U}(0,1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;p(x) = 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And $C$ to be $\max_{x} q(x)= 0.0833333$&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;C = q([0.,0.]) #max_x q(x)
p = lambda x: 1
samples = []
num_samples = 10000
while len(samples) &amp;lt; num_samples:
    x = rnd.rand(2)
    if rnd.rand() &amp;lt;= ( q(x) / (C * p(x))):
        samples.append({'x_1':x[0], 'x_2':x[1]})
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples)
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_9_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;We can see how the histogram of the collected samples agrees to the mass distribution of $q$. However, this approach becomes challenging in higher dimensions: higher rejection probability $($which makes the process extremely slow$)$ and less known upper-bounding scaled distributions. For this, Markov Chain Monte Carlo $($MCMC$)$ methods come to the rescue&lt;/p&gt;

&lt;h3&gt;Markov Chain Monte Carlo&lt;/h3&gt;

&lt;p&gt;Based on the notion of stationary distribution,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi(\bar{x})=\sum_{x}\pi(x)T(\bar{x}|x)&lt;/script&gt;
we can generate our samples from a dynamical systems over the states $($values$)$ which the random variable $x$ can take. That is, i. Specify a transition probability in line with the system dynamics $T(\bar{x}|x)$ $($or proposal distribution- to propose the next state $\bar{x}$ given the present state $x$ $)$. ii. Let the system interact, iii. The system will converge to the underlying $($unique$)$ distribution of $\pi$. Note that, if:
&lt;script type=&quot;math/tex&quot;&gt;T(\bar{x}|x)&gt;0\;, \forall x, \bar{x}\;,&lt;/script&gt;
there exists a unique stationary $\pi$&lt;/p&gt;

&lt;p&gt;The challenge in this setup is how to define / sample from the valid transition distribution $($e.g., in higher dimensions$)$. Among other approaches we discuss two here.&lt;/p&gt;

&lt;h4&gt;Gibbs Sampling&lt;/h4&gt;

&lt;p&gt;Given a random $($initialized$)$ state of our systems variables $($here, $x_1$ and $x_2$$)$, one can condition one variable on the other to obtain a 1-D distriubution, which we assume we can sample from $($i.e., rejection sampling, central limit theorem, or other techniques$)$. Iteratively, we will obtain samples. Neverthelss, we need to give it some time till we converge to the stationary distribution of our system. In our example, we will make use of rejection sampling in 1-D to obtain our 1-D sample.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6.
# q(x_1|x_2)
q_1 = lambda x: 6 * q(x)/((x[1]-0.5)**2 + 2. * 0.5**3 / 3.) 
# q(x_2|x_1)
q_2 = lambda x: 6 * q(x)/((x[0]-0.5)**2 + 2. * 0.5**3 / 3.) 
# p (upper bounding), here we are using uniform dist
p = lambda x: 1 

def reject_uniform_sample(q, C):
    while True:
        x_ = rnd.rand()
        if rnd.rand() &amp;lt;= q(x_) /(C * 1.):
            return x_

# aribtrary value, by right we should use max_x q_1, max_x q_2
C_1 = 1.5
C_2 = 1.5
samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}]
num_samples = 10000
while len(samples) &amp;lt; num_samples:
    x_1 = reject_uniform_sample(lambda x_: q_1([x_, samples[-1]['x_2']]), C_1)
    x_2 = reject_uniform_sample(lambda x_: q_2([samples[-1]['x_1'], x_]), C_2)
    samples.append({'x_1':x_1, 'x_2':x_2})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;While burning:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[:50])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_16_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;After discarding some samples away:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[500:])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_18_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;h4&gt;Metropolis-Hastings Sampling&lt;/h4&gt;

&lt;p&gt;In Gibbs sampling, the idea is to generate samples for each of our system’s variables from their posterior distribution with respect to the current values of the rest of the variables. Multi-dimensioanly sampling is reduced into a sequence of 1-D sampling procedures. We saw above, this approach might be a little bit slow and generates highly correlated samples $($again: we are generating samples that are highly probable given the past values of the rest of the variables$)$. Furthermore, what if we can’t compute nor sample from the posterior distribution. Can we propose transitions from one state to another arbitrarily and sitll converge? The idea of MH-sampling is to incorporate rejection sampling with aribtrary proposal distribution such that the transition probability $($which uniquely defines the Markov process$)$ can be constructed.&lt;/p&gt;

&lt;p&gt;Let’s step back and summarise the above:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We would like to sample from our system’s distribution &lt;script type=&quot;math/tex&quot;&gt;q( x )&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We are using Markov Chain $($process$)$ to simulate our system of random variables. That is the Markov chain asymptotically converges to a stationary distribution &lt;script type=&quot;math/tex&quot;&gt;\pi(x)&lt;/script&gt; and which is &lt;script type=&quot;math/tex&quot;&gt;q(x)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;To converge to $ q(x) $, we need to construct the corresponding chain’s transition probability $ T(\bar{x}|x) $
 because it uniquely defines the process and hence its stationary distribution.&lt;/li&gt;
  &lt;li&gt;One way of achieving that is through the detailed balance, sufficient but not a necessary condition:
$ q(x) T(\bar{x}|x) = q(\bar{x}) T(x|\bar{x}) $&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can make use of the above formula to construct a transition probability  that is a product of an arbitrary distribution 
$g(\bar{x}|x)$ 
, which is referred to as the proposal distribution as mentioned above, and the acceptance distribution 
$A(\bar{x}|x)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(\bar{x}|x)= g(\bar{x}|x)A(\bar{x}|x)&lt;/script&gt;

&lt;p&gt;Plugging this in the detailed balance condition, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A(\bar{x}|x)=\min\{1, \frac{q(\bar{x})g(x|\bar{x})}{q(x)g(\bar{x}|x)}\}&lt;/script&gt;

&lt;p&gt;Let’s see how we can realize this in code: Let’s define our&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(\bar{x}|x)\sim \mathcal{N}(x,0.25I)&lt;/script&gt;

&lt;p&gt;and here we are assuming that we can sample efficiently from this 2-D Gaussian distribution $($ otherwise, we may use one of the techniques above as rejection sampling $)$. Notice here, there is nothing preventing us from sampling outside the support of $q(x)$, so we need to explicitly state that. I.e., $q(x)=0$ outside $q$’s support. Also, our choice of proposal distribution might affect the convergence progress.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# target distribution (the one we'd like the MC to converge to)
q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6.
# assuming 2-d Gaussian sampling is doable here (via function call)
sigma = 0.5
propose_x = lambda x_prev: x_prev + sigma * np.dot(rnd.randn(1,2),np.eye(2))[0,:]
# proposal distribution:
from scipy.stats import multivariate_normal
g = lambda x_new, x_prev: multivariate_normal.pdf(x_new, mean=x_prev, cov =sigma * np.eye(2))
# random initialization of the sample which we'll eventually discard
samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}]
num_samples = 10000
while len(samples) &amp;lt; num_samples:
    x_prev = np.array([samples[-1]['x_1'], samples[-1]['x_2']])
    x_new = propose_x(x_prev)
    # to discard samples out of support
    if max(x_new) &amp;gt; 1. or min(x_new) &amp;lt; 0.:
        continue
    if rnd.rand() &amp;lt;= min(1, q(x_new) * g(x_prev,x_new) / (q(x_prev) * g(x_new, x_prev))):
        samples.append({'x_1':x_new[0], 'x_2':x_new[1]})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[:50])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_21_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[1500:])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_22_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we’ll take a quick tour into the sampling world and how it might be handy in some applications. Let’s start with some setup code. import numpy as np import pandas as pd import numpy.random as rnd import seaborn as sns import scipy from mpl_toolkits.mplot3d import Axes3D %pylab inline Populating the interactive namespace from numpy and matplotlib Sampling from an aribitrary 2-D distribution Let $x\in \mathbb{R}^2$ be a continuous random variable with from a probability distribution $q(x)$. We are interested in obtaining values that $x$ can take in a way that confirms to its probability distribution $q(x)$. Let’s assume $q(x)$ has the following form: where q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6. Let’s have a look at how does this distribution look like n = 100 x_1s, x_2s = np.meshgrid(*[np.linspace(0,1, n)]*2) @np.vectorize def compute_q(x_1,x_2): return q([x_1,x_2]) q_s = compute_q(x_1s, x_2s) fig = plt.figure() ax = fig.gca(projection='3d') surf = ax.plot_trisurf(x_1s.ravel(), x_2s.ravel(), q_s.ravel(), cmap=plt.cm.BrBG_r, linewidth=0.2) fig.colorbar(surf, shrink=0.5, aspect=5) ax.view_init(30, 45) plt.show() The density accumulates at the boundaries of the distribution’s support. So when we sample, we expect more values around the boundaries rather than the center. The question is how to make our sampling process conforms to the same. In the rest of this post, we will discuss three methods: Rejection Sampling, Gibbs Sampling, Metropolis Hastings. Rejection Sampling: The gist of this method is to find a distribution $p(x)$ from which we know how to sample. We scale $p(x)$ by a factor $C$ such that If the above can be obtained, then our sampling procedure is as follows: Sample $\bar{x}\sim p(\bar{x})$. Toss a biased coin to accept $\bar{x}$ as a sample coming from $q$. The coin bias is $\frac{q(\bar{x})}{Cp(\bar{x})}$ Intuitvely, you can think of this procedure as a correction scheme to account for the mismatch: how accurately $p(x)$ describes $q(x)$. Put it differently, the area under curve $Cp(x)$ is $C$, while under $q(x)$, it is $1$. And since $q(x)\leq Cp(x)$, if we full the area under $Cp(x)$ with points uniformly, then $1/C$ of these points will be under $q(x)$. In the demo below, we choose $p$ to be the uniform distribution: And $C$ to be $\max_{x} q(x)= 0.0833333$ C = q([0.,0.]) #max_x q(x) p = lambda x: 1 samples = [] num_samples = 10000 while len(samples) &amp;lt; num_samples: x = rnd.rand(2) if rnd.rand() &amp;lt;= ( q(x) / (C * p(x))): samples.append({'x_1':x[0], 'x_2':x[1]}) df = pd.DataFrame(samples) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') We can see how the histogram of the collected samples agrees to the mass distribution of $q$. However, this approach becomes challenging in higher dimensions: higher rejection probability $($which makes the process extremely slow$)$ and less known upper-bounding scaled distributions. For this, Markov Chain Monte Carlo $($MCMC$)$ methods come to the rescue Markov Chain Monte Carlo Based on the notion of stationary distribution, we can generate our samples from a dynamical systems over the states $($values$)$ which the random variable $x$ can take. That is, i. Specify a transition probability in line with the system dynamics $T(\bar{x}|x)$ $($or proposal distribution- to propose the next state $\bar{x}$ given the present state $x$ $)$. ii. Let the system interact, iii. The system will converge to the underlying $($unique$)$ distribution of $\pi$. Note that, if: there exists a unique stationary $\pi$ The challenge in this setup is how to define / sample from the valid transition distribution $($e.g., in higher dimensions$)$. Among other approaches we discuss two here. Gibbs Sampling Given a random $($initialized$)$ state of our systems variables $($here, $x_1$ and $x_2$$)$, one can condition one variable on the other to obtain a 1-D distriubution, which we assume we can sample from $($i.e., rejection sampling, central limit theorem, or other techniques$)$. Iteratively, we will obtain samples. Neverthelss, we need to give it some time till we converge to the stationary distribution of our system. In our example, we will make use of rejection sampling in 1-D to obtain our 1-D sample. q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6. # q(x_1|x_2) q_1 = lambda x: 6 * q(x)/((x[1]-0.5)**2 + 2. * 0.5**3 / 3.) # q(x_2|x_1) q_2 = lambda x: 6 * q(x)/((x[0]-0.5)**2 + 2. * 0.5**3 / 3.) # p (upper bounding), here we are using uniform dist p = lambda x: 1 def reject_uniform_sample(q, C): while True: x_ = rnd.rand() if rnd.rand() &amp;lt;= q(x_) /(C * 1.): return x_ # aribtrary value, by right we should use max_x q_1, max_x q_2 C_1 = 1.5 C_2 = 1.5 samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}] num_samples = 10000 while len(samples) &amp;lt; num_samples: x_1 = reject_uniform_sample(lambda x_: q_1([x_, samples[-1]['x_2']]), C_1) x_2 = reject_uniform_sample(lambda x_: q_2([samples[-1]['x_1'], x_]), C_2) samples.append({'x_1':x_1, 'x_2':x_2}) While burning: df = pd.DataFrame(samples[:50]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') After discarding some samples away: df = pd.DataFrame(samples[500:]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') Metropolis-Hastings Sampling In Gibbs sampling, the idea is to generate samples for each of our system’s variables from their posterior distribution with respect to the current values of the rest of the variables. Multi-dimensioanly sampling is reduced into a sequence of 1-D sampling procedures. We saw above, this approach might be a little bit slow and generates highly correlated samples $($again: we are generating samples that are highly probable given the past values of the rest of the variables$)$. Furthermore, what if we can’t compute nor sample from the posterior distribution. Can we propose transitions from one state to another arbitrarily and sitll converge? The idea of MH-sampling is to incorporate rejection sampling with aribtrary proposal distribution such that the transition probability $($which uniquely defines the Markov process$)$ can be constructed. Let’s step back and summarise the above: We would like to sample from our system’s distribution . We are using Markov Chain $($process$)$ to simulate our system of random variables. That is the Markov chain asymptotically converges to a stationary distribution and which is . To converge to $ q(x) $, we need to construct the corresponding chain’s transition probability $ T(\bar{x}|x) $ because it uniquely defines the process and hence its stationary distribution. One way of achieving that is through the detailed balance, sufficient but not a necessary condition: $ q(x) T(\bar{x}|x) = q(\bar{x}) T(x|\bar{x}) $ We can make use of the above formula to construct a transition probability that is a product of an arbitrary distribution $g(\bar{x}|x)$ , which is referred to as the proposal distribution as mentioned above, and the acceptance distribution $A(\bar{x}|x)$ Plugging this in the detailed balance condition, we have: Let’s see how we can realize this in code: Let’s define our and here we are assuming that we can sample efficiently from this 2-D Gaussian distribution $($ otherwise, we may use one of the techniques above as rejection sampling $)$. Notice here, there is nothing preventing us from sampling outside the support of $q(x)$, so we need to explicitly state that. I.e., $q(x)=0$ outside $q$’s support. Also, our choice of proposal distribution might affect the convergence progress. # target distribution (the one we'd like the MC to converge to) q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6. # assuming 2-d Gaussian sampling is doable here (via function call) sigma = 0.5 propose_x = lambda x_prev: x_prev + sigma * np.dot(rnd.randn(1,2),np.eye(2))[0,:] # proposal distribution: from scipy.stats import multivariate_normal g = lambda x_new, x_prev: multivariate_normal.pdf(x_new, mean=x_prev, cov =sigma * np.eye(2)) # random initialization of the sample which we'll eventually discard samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}] num_samples = 10000 while len(samples) &amp;lt; num_samples: x_prev = np.array([samples[-1]['x_1'], samples[-1]['x_2']]) x_new = propose_x(x_prev) # to discard samples out of support if max(x_new) &amp;gt; 1. or min(x_new) &amp;lt; 0.: continue if rnd.rand() &amp;lt;= min(1, q(x_new) * g(x_prev,x_new) / (q(x_prev) * g(x_new, x_prev))): samples.append({'x_1':x_new[0], 'x_2':x_new[1]}) df = pd.DataFrame(samples[:50]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') df = pd.DataFrame(samples[1500:]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')</summary></entry><entry><title type="html">On the Positive Definitness of Multivariate Gaussian’s Covariance Matrix</title><link href="http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix/" rel="alternate" type="text/html" title="On the Positive Definitness of Multivariate Gaussian's  Covariance Matrix" /><published>2017-09-20T00:00:00-08:00</published><updated>2017-09-20T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix/">&lt;p&gt;$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\covmat}{\Sigma}
$&lt;/p&gt;

&lt;p&gt;In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&amp;gt;0$&lt;/li&gt;
  &lt;li&gt;$\Sigma_{ij} = \Sigma_{ji}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps:&lt;/p&gt;

&lt;p&gt;First, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from
&lt;script type=&quot;math/tex&quot;&gt;{\vx}^T\Sigma \vx=\sum_{ij} y_i y_j \Sigma_{ij}=\sum_{ij}y_i y_j E[(x_i - \bar{x}_i)(x_j - \bar{x}_j)]=E\big[\sum_{ij}y_i y_j (x_i - \bar{x}_i)(x_j - \bar{x}_j)\big]\;,&lt;/script&gt;
with $z_i =x_i - \bar{x}_i$, we have &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\vx^T\Sigma \vx=E[\sum_{ij} y_i z_iy_jz_j]=E[\sum_{i} y^2_iz^2_i + 2 \sum_{i&lt;j} y_iz_i y_j z_j]=E[(\vy^T\vz)^2]\geq 0\;, %]]&gt;&lt;/script&gt; since $(\vy^T\vz)^2\geq 0$.&lt;/p&gt;

&lt;p&gt;Second, for $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i }_{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors 
&lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{v}_i\}_{1\leq i \leq n}\;.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Therefore, &lt;script type=&quot;math/tex&quot;&gt;{\vx}^T \Sigma \vx= \sum_{ij} w_i \mathbf{v}^T_i \lambda_j w_j \mathbf{v}_j= \sum_{i} \lambda_i w^2_i &gt; 0\;.&lt;/script&gt;&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">$ \newcommand{\vx}{\mathbf{x}} \newcommand{\vy}{\mathbf{y}} \newcommand{\vz}{\mathbf{z}} \newcommand{\covmat}{\Sigma} $ In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&amp;gt;0$ $\Sigma_{ij} = \Sigma_{ji}$. The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps: First, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from with $z_i =x_i - \bar{x}_i$, we have since $(\vy^T\vz)^2\geq 0$. Second, for $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i }_{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors Therefore,</summary></entry><entry><title type="html">Quadratic Function Optimization via Linear Algebra</title><link href="http://ash-aldujaili.github.io/blog/2017/08/01/quadratic-form-opt/" rel="alternate" type="text/html" title="Quadratic Function Optimization via Linear Algebra" /><published>2017-08-01T00:00:00-08:00</published><updated>2017-08-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/08/01/quadratic-form-opt</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/08/01/quadratic-form-opt/">&lt;p&gt;In this post, we show how Linear Algebra can be used to solve one of the common optimization problems that arise in a variety of domains. In particular, we are interested in the quadratic optimization problem of the form
$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\lm}{\lambda}
\newcommand{\norm}[1]{||#1||_2}
$&lt;/p&gt;

&lt;p&gt;\begin{align}
\text{max}_{\vx}\hspace{1.15cm} \vx^TA\vx \newline
\hspace{1em}\text{s.t.}\hspace{1em} \norm{\vx}=1
\label{eq:opt-problem}
\end{align}&lt;/p&gt;

&lt;p&gt;where $A\in\mathbb{R}^{n^2}$ is a real symmetric $($square$)$ matrix, $\vx\in\mathbb{R}^n$. An optimal solution of this problem is $\vq_1\in\mathbb{R}^n$, the eigenvector that corresponds to $\lambda_1\in \mathbb{R}$, the largest eigenvalue of $A$.&lt;/p&gt;

&lt;p&gt;Now that we have defined the problem, let’s try to solve it by making use of Linear Algebra, from which we know the following.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If $Q\in\mathbb{R}^{n^2}$ is an orthogonal matrix, then $Q^{T}Q=QQ^T=I_n$ where $I_n\in\mathbb{R}^{n^2}$ is the $n\times n$ identity matrix.&lt;/li&gt;
  &lt;li&gt;Columns (and rows) of an orthogonal matrix have unit norm.&lt;/li&gt;
  &lt;li&gt;Any real symmetric matrix $A$ can be decomposed into $Q\Lambda Q^T$, where $Q\in\mathbb{R}^{n^2}$ is an orthogonal matrix whose columns are the eigenvectors of $A$, while $\Lambda = diag(\mathbf{\lambda})$ is a diagonal matrix whose diagonal entries are the eign values of $A$ such that $\lambda_1\geq \lambda_2 \geq \cdots\geq \lambda_n$ and  $\lambda_i$ is the eigenvalue of the $i^{th}$ column of $Q$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From (3.), one can write $\vx^TA\vx$ as $\vx^T Q\Lambda Q^T \vx \leq \lambda_1 \vx^T QI_n Q^T \vx$. Based on (1.) and the constraint of $\vx$’s unit norm, the last term can further be simplified to have $\vx^TA\vx \leq \lambda_1 \norm{\vx}\leq \lambda_1$.&lt;/p&gt;

&lt;p&gt;On the one hand, we found that $\max_{\vx} \vx^T A \vx = \lambda_1$. On the other hand, $A\vq_1 = \lambda_1\vq_1$, and thus $\vq_1^TA\vq_1 = \lambda_1$. Therefore, $\vq_1 \in \arg\max_{\vx} \vx^T A \vx$.&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we show how Linear Algebra can be used to solve one of the common optimization problems that arise in a variety of domains. In particular, we are interested in the quadratic optimization problem of the form $ \newcommand{\vx}{\mathbf{x}} \newcommand{\vq}{\mathbf{q}} \newcommand{\lm}{\lambda} \newcommand{\norm}[1]{||#1||_2} $ \begin{align} \text{max}_{\vx}\hspace{1.15cm} \vx^TA\vx \newline \hspace{1em}\text{s.t.}\hspace{1em} \norm{\vx}=1 \label{eq:opt-problem} \end{align} where $A\in\mathbb{R}^{n^2}$ is a real symmetric $($square$)$ matrix, $\vx\in\mathbb{R}^n$. An optimal solution of this problem is $\vq_1\in\mathbb{R}^n$, the eigenvector that corresponds to $\lambda_1\in \mathbb{R}$, the largest eigenvalue of $A$. Now that we have defined the problem, let’s try to solve it by making use of Linear Algebra, from which we know the following. If $Q\in\mathbb{R}^{n^2}$ is an orthogonal matrix, then $Q^{T}Q=QQ^T=I_n$ where $I_n\in\mathbb{R}^{n^2}$ is the $n\times n$ identity matrix. Columns (and rows) of an orthogonal matrix have unit norm. Any real symmetric matrix $A$ can be decomposed into $Q\Lambda Q^T$, where $Q\in\mathbb{R}^{n^2}$ is an orthogonal matrix whose columns are the eigenvectors of $A$, while $\Lambda = diag(\mathbf{\lambda})$ is a diagonal matrix whose diagonal entries are the eign values of $A$ such that $\lambda_1\geq \lambda_2 \geq \cdots\geq \lambda_n$ and $\lambda_i$ is the eigenvalue of the $i^{th}$ column of $Q$. From (3.), one can write $\vx^TA\vx$ as $\vx^T Q\Lambda Q^T \vx \leq \lambda_1 \vx^T QI_n Q^T \vx$. Based on (1.) and the constraint of $\vx$’s unit norm, the last term can further be simplified to have $\vx^TA\vx \leq \lambda_1 \norm{\vx}\leq \lambda_1$. On the one hand, we found that $\max_{\vx} \vx^T A \vx = \lambda_1$. On the other hand, $A\vq_1 = \lambda_1\vq_1$, and thus $\vq_1^TA\vq_1 = \lambda_1$. Therefore, $\vq_1 \in \arg\max_{\vx} \vx^T A \vx$.</summary></entry><entry><title type="html">A Practical Summary to Matrix Inversion</title><link href="http://ash-aldujaili.github.io/blog/2017/07/31/matrix-inversion/" rel="alternate" type="text/html" title="A Practical Summary to Matrix Inversion" /><published>2017-07-31T00:00:00-08:00</published><updated>2017-07-31T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/07/31/matrix-inversion</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/07/31/matrix-inversion/">&lt;p&gt;$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\lm}{\lambda}
\newcommand{\norm}[1]{||#1||_2}
\newcommand{\mat}[1]{\left[#1\right]}
$&lt;/p&gt;

&lt;p&gt;Matrix inversion is a handy procedure in solving a linear system of equations based on the notion of identity matrix $I_n\in\mathbb{R}^{n^2}$ whose all entries are zero except the entries along the main diagonal.&lt;/p&gt;

&lt;p&gt;For a linear system $A\vx=\vb$ where $A\in\mathbb{R}^{m\times n}$ is a known matrix, $\vb\in\mathbb{R}^m$ is a known vector, and $\vx\in\mathbb{R}^n$ is a vector of unknown values, one can find a solution if a matrix $B\in\mathbb{R}^{n\times m}$ can be found such that $BA=I_n$ since for all $\vx \in \mathbb{R}^n$, $I_n \vx=\vx$. We refer to $B$ as the inverse of $A$ $($commonly denoted as $A^{-1})$. Mathematically,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A\vx=\vb&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^{-1}A\vx=A^{-1}\vb&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I_n\vx=A^{-1}\vb&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vx=A^{-1}\vb&lt;/script&gt;

&lt;p&gt;Of course, such solution can be found if $A$ is invertible. That is it is possible to find $A^{-1}$. I am writing this post to summarize possible scenarios one may face when inverting a matrix.&lt;/p&gt;

&lt;p&gt;Before delving into the details, I intend to present a geometric view of linear systems of equations. This will help us understand why we could have exactly one, infinitely many, or no solution at all. And subsequently, why some matrices are invertible and others are not.&lt;/p&gt;

&lt;h3&gt;Problem Setup&lt;/h3&gt;
&lt;p&gt;One can think of $\vb$ as a point in the $m$-dimensional space. Furthermore, consider $A\vx$ as a linear combination $A$’s column vectors where the $i^{th}$ column $A_{:,i}$ is scaled by the $i^{th}$ entry of $\vx$, $x_i$. In other words, when we say that $\vb=A\vx$, we mean that $\vb$ is reachable from the origin $(\mathbf{0}\in\mathbb{R}^m)$ by moving $x_1$ units along the vector $A_{:,1}$, $x_2$ units along the vector $A_{:,2}$, …, and $x_n$ units along the vector $A_{:,n}$. For different $\vx\in\mathbb{R}^n$, we may reach different points in the $m$-dimensional space. The set of such points are referred to as the &lt;strong&gt;span&lt;/strong&gt; of $A$ $($or &lt;strong&gt;range&lt;/strong&gt; of $A$, &lt;strong&gt;column space&lt;/strong&gt; of $A)$. That is, the set of points reachable by a linear combination of $A$’s columns. Mathematically, &lt;script type=&quot;math/tex&quot;&gt;span(A\text{'s columns})=\{b\in \mathbb{R}^m|\; b = A\vx,\; \vx\in \mathbb{R}^n\}\;.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Therefore, a solution exists for $A\vx=\vb$ if $\vb\in span(A\text{‘s columns})$. A necessary and sufficient condition for a solution to exist for all possible values of $\vb$ $(\mathbb{R}^m)$ is that $A$’s columns constitute at least one set of $m$ linearly independent vectors. Let’s note that there is no set of $m$-dimensional vectors has more than $m$ linearly indepdent vectors. The following examples provide an illustration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$A=\mat{\begin{array}{cc} 1 &amp;amp; 2 , 0 &amp;amp; 0\end{array}}$,  $\vb =\mat{\begin{array}{c} 1 , 1\end{array}}$. In this example, $A$’s span is $\mathbb{R}^1$ along the vector $\mat{\begin{array}{c}1 , 0\end{array}}$ and no matter how $\vx$ is set, we can not reach $\vb$, because it does not lie at the intersection of its plane $\mathbb{R}^2$ and $A$’s space. With regard to the remark made above, we notice that $m=2$, but we do not have $2$ linearly independent vectors in $A$’s columns.&lt;/li&gt;
  &lt;li&gt;$A=\mat{\begin{array}{cc} 1 &amp;amp; 2 , 0 &amp;amp; 0\end{array}}$,  $\vb =\mat{\begin{array}{c} 1 , 0\end{array}}$. In this example, $A$ is the same as $(1.)$’s but we moved $\vb$ to make it reachable. One may note that the effective dimension of the problem at hand is one (the second entry of all the columns is zero). In other words, we have matrix of $1\times 2$ with $m=1$ and we have $2$ linearly dependent vectors of length 1 that can reach $\vb$ through any solution that satisfies $x_1+ 2x_2=1$, for which there are infinitely many. Such example fulfills the sufficient and necessary condition aforementioned, but here we have more than $m$ columns. Thus, there are infinitely many solutions rather than exactly one.&lt;/li&gt;
  &lt;li&gt;$A=\mat{\begin{array}{cc} 1 &amp;amp; 0 , 0 &amp;amp; 1\end{array}}$,  $\vb =\mat{\begin{array}{c} 1 , 1\end{array}}$. In this example, we made $A$ spans $\mathbb{R}^2$ with exactly $m=2$ linearly indepdendent vectors, and so we can reach $\vb$ with exactly one particular $\vx$. This example fulfils the sufficient and necessary condition with exactly $n=m$ columns.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Based on the above, one can conclude the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When $A$ is a square matrix $(m=n)$ with $m$ mutually independent column vectors, there is exactly one solution for any $\vb\in\mathbb{R}^{m}$. Thus, $A$ is invertible.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $A$ is a square matrix $(m=n)$ with linearly depdenent column vectors, there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible and we say that $A$ is a &lt;strong&gt;singular matrix&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $A$ is a rectangular matrix $(m&amp;gt;n)$, there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible, and we say that the system is &lt;strong&gt;overdetermined&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $A$ is a rectangular matrix $(m&amp;lt; n)$ with $m$ linearly indendent column vectors, there exist infinitely many solutions for any $\vb \in \mathbb{R}^m$. Thus, $A$ is non-invertible, and we say that the system is &lt;strong&gt;underdetermined&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;When $A$ is a rectangular matrix $(m&amp;lt; n)$ with $&amp;lt; m$ linearly indendent column vectors, it is possible that there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible. We still say the system is &lt;strong&gt;underdetermined&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are several procedures to compute $A^{-1}$ for invertible matrices such as the Gauss elimination method $($some others are listed in the figure at the bottom of this page$)$.&lt;/p&gt;

&lt;h3&gt;Non-Invertible Matrices&lt;/h3&gt;

&lt;p&gt;So what do we do when $A$ is non-invertible and we are still interested in finding a solution for a system? Well, we can use a pseudo-inverse matrix with some compromise on the solution’s quality as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If there is &lt;strong&gt;no solution&lt;/strong&gt;. That is we can’t reach $\vb$ and the system is overdetermined. Then, we can choose one with the least squares error. Mathematically,&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\vx} ||A\vx-b||^2_2&lt;/script&gt;

&lt;p&gt;Differentiating w.r.t $\vx$ and setting it to zero,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;0=\nabla_{\vx}{((A\vx-b)^{T}(A\vx-b))}=A^{T}A\vx-A^T\vb&lt;/script&gt;
yields $\vx=(A^{T}A)^{-1}A^T\vb$. Therefore, the pseudo-inverse matrix is $(A^{T}A)^{-1}A^T$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If there are &lt;strong&gt;infinitely many solutions&lt;/strong&gt;. That is the system is underdetermined. Then, one can choose one with the minimum norm. Mathematically,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\min_{\vx} ||\vx||^2_2&lt;/script&gt; 
&lt;script type=&quot;math/tex&quot;&gt;\text{s.t.}\;\;\; A\vx =\vb&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;With the Lagrangian multipliers trick, the above can be solved as follows.&lt;/p&gt;

&lt;p&gt;Differentiating w.r.t $\vx$ and setting it to zero,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0=\nabla_{\vx}{(\vx^{T}\vx - \mathbf{\lambda}^{T}(A\vx-b))}= 2\vx - A^{T}\mathbf{\lambda}&lt;/script&gt;

&lt;p&gt;$A^{T}$ is not invertible, but $AA^{T}$ is.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Therefore, we can solve for $\mathbf{\lambda}$ as follows.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\lambda}= 2(AA^T)^{-1}A\vx=2(AA^T)^{-1}\vb&lt;/script&gt;

&lt;p&gt;Thus, $\vx=A^T(AA^T)^{-1}\vb$&lt;/p&gt;

&lt;p&gt;Interestingly, both of the above pseudo-inverses are equivalent to the &lt;strong&gt;Moore-Penrose&lt;/strong&gt; pseudo-inverse which can be computed from the singular value decomposition $($SVD$)$.&lt;/p&gt;

&lt;h3&gt;Ill-Conditioned Matrices&lt;/h3&gt;

&lt;p&gt;Let’s motivate this issue with an example. Assume you are reading a measurement, which you have to scale by dividing by a very small number. If your reading deviates a little, the absolute error will extremely large. Such large oscillations might be undesirable and we’d like to dampen them. In the matrix world, dividing by a small number corresponds to to multiplying by the inverse of an ill-conditioned matrix. An ill-conditioned matrix is still &lt;strong&gt;invertible&lt;/strong&gt;, but it is numerically unstable. In addition to the measurement error $($here the measurement is captured by $\vb$$)$, inverting an ill-conditioned matrix also suffers from loss of precision in floating point arithmetic.To cope with this unstability, we compromise the quality of the solution and use regularization techniques to have a stable numerical outcome. The bulk of these techniques aim to increase $($or replace$)$ the value of the smallest eigenvalue with respect to the greatest eigenvalue. Common techniques are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Truncated SVD&lt;/li&gt;
  &lt;li&gt;Tikhonov Regularization $($which can also be used for non-invertible matrices$)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is important to consider the regularization effect on the final solution and whether it makes sense to our system or not.&lt;/p&gt;

&lt;h3&gt;Matrix Inversion Summary&lt;/h3&gt;

&lt;p&gt;Below is a summary on matrix inversion.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/matrix-inverse-summary.png&quot; width=&quot;1000&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Summary of Matrix Inverse Techniques.
&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;$AA^T$ is a square matrix. What is left to show is that columns of $AA^T$ are linearly independent, and thus $AA^T$ is invertible. We know that the columns of $A^T$ are linearly independent,  which means its null space is ${\mathbf{0}}$. By contradiction, let’s assume that $\mathbf{v}$ is a non-zero vector that belongs to  $AA^T$’s null space, which means $AA^T \mathbf{v}= \mathbf{0}$. Mutiplying both sides by $\mathbf{v}$ yields $(A^T \mathbf{v})^{T}(A^T \mathbf{v})=0$, i.e., $A^T \mathbf{v}= \mathbf{0}$. This contradicts the fact that $A^T$’s null space is the zero vector. Therefore, $\mathbf{v}$ can not be zero and $AA^T$’s null space has no non-zero vector, and all of its columns are linearly independent.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">$ \newcommand{\vx}{\mathbf{x}} \newcommand{\vq}{\mathbf{q}} \newcommand{\vb}{\mathbf{b}} \newcommand{\lm}{\lambda} \newcommand{\norm}[1]{||#1||_2} \newcommand{\mat}[1]{\left[#1\right]} $ Matrix inversion is a handy procedure in solving a linear system of equations based on the notion of identity matrix $I_n\in\mathbb{R}^{n^2}$ whose all entries are zero except the entries along the main diagonal. For a linear system $A\vx=\vb$ where $A\in\mathbb{R}^{m\times n}$ is a known matrix, $\vb\in\mathbb{R}^m$ is a known vector, and $\vx\in\mathbb{R}^n$ is a vector of unknown values, one can find a solution if a matrix $B\in\mathbb{R}^{n\times m}$ can be found such that $BA=I_n$ since for all $\vx \in \mathbb{R}^n$, $I_n \vx=\vx$. We refer to $B$ as the inverse of $A$ $($commonly denoted as $A^{-1})$. Mathematically, Of course, such solution can be found if $A$ is invertible. That is it is possible to find $A^{-1}$. I am writing this post to summarize possible scenarios one may face when inverting a matrix. Before delving into the details, I intend to present a geometric view of linear systems of equations. This will help us understand why we could have exactly one, infinitely many, or no solution at all. And subsequently, why some matrices are invertible and others are not. Problem Setup One can think of $\vb$ as a point in the $m$-dimensional space. Furthermore, consider $A\vx$ as a linear combination $A$’s column vectors where the $i^{th}$ column $A_{:,i}$ is scaled by the $i^{th}$ entry of $\vx$, $x_i$. In other words, when we say that $\vb=A\vx$, we mean that $\vb$ is reachable from the origin $(\mathbf{0}\in\mathbb{R}^m)$ by moving $x_1$ units along the vector $A_{:,1}$, $x_2$ units along the vector $A_{:,2}$, …, and $x_n$ units along the vector $A_{:,n}$. For different $\vx\in\mathbb{R}^n$, we may reach different points in the $m$-dimensional space. The set of such points are referred to as the span of $A$ $($or range of $A$, column space of $A)$. That is, the set of points reachable by a linear combination of $A$’s columns. Mathematically, Therefore, a solution exists for $A\vx=\vb$ if $\vb\in span(A\text{‘s columns})$. A necessary and sufficient condition for a solution to exist for all possible values of $\vb$ $(\mathbb{R}^m)$ is that $A$’s columns constitute at least one set of $m$ linearly independent vectors. Let’s note that there is no set of $m$-dimensional vectors has more than $m$ linearly indepdent vectors. The following examples provide an illustration. $A=\mat{\begin{array}{cc} 1 &amp;amp; 2 , 0 &amp;amp; 0\end{array}}$, $\vb =\mat{\begin{array}{c} 1 , 1\end{array}}$. In this example, $A$’s span is $\mathbb{R}^1$ along the vector $\mat{\begin{array}{c}1 , 0\end{array}}$ and no matter how $\vx$ is set, we can not reach $\vb$, because it does not lie at the intersection of its plane $\mathbb{R}^2$ and $A$’s space. With regard to the remark made above, we notice that $m=2$, but we do not have $2$ linearly independent vectors in $A$’s columns. $A=\mat{\begin{array}{cc} 1 &amp;amp; 2 , 0 &amp;amp; 0\end{array}}$, $\vb =\mat{\begin{array}{c} 1 , 0\end{array}}$. In this example, $A$ is the same as $(1.)$’s but we moved $\vb$ to make it reachable. One may note that the effective dimension of the problem at hand is one (the second entry of all the columns is zero). In other words, we have matrix of $1\times 2$ with $m=1$ and we have $2$ linearly dependent vectors of length 1 that can reach $\vb$ through any solution that satisfies $x_1+ 2x_2=1$, for which there are infinitely many. Such example fulfills the sufficient and necessary condition aforementioned, but here we have more than $m$ columns. Thus, there are infinitely many solutions rather than exactly one. $A=\mat{\begin{array}{cc} 1 &amp;amp; 0 , 0 &amp;amp; 1\end{array}}$, $\vb =\mat{\begin{array}{c} 1 , 1\end{array}}$. In this example, we made $A$ spans $\mathbb{R}^2$ with exactly $m=2$ linearly indepdendent vectors, and so we can reach $\vb$ with exactly one particular $\vx$. This example fulfils the sufficient and necessary condition with exactly $n=m$ columns. Based on the above, one can conclude the following: When $A$ is a square matrix $(m=n)$ with $m$ mutually independent column vectors, there is exactly one solution for any $\vb\in\mathbb{R}^{m}$. Thus, $A$ is invertible. When $A$ is a square matrix $(m=n)$ with linearly depdenent column vectors, there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible and we say that $A$ is a singular matrix. When $A$ is a rectangular matrix $(m&amp;gt;n)$, there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible, and we say that the system is overdetermined. When $A$ is a rectangular matrix $(m&amp;lt; n)$ with $m$ linearly indendent column vectors, there exist infinitely many solutions for any $\vb \in \mathbb{R}^m$. Thus, $A$ is non-invertible, and we say that the system is underdetermined. When $A$ is a rectangular matrix $(m&amp;lt; n)$ with $&amp;lt; m$ linearly indendent column vectors, it is possible that there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible. We still say the system is underdetermined. There are several procedures to compute $A^{-1}$ for invertible matrices such as the Gauss elimination method $($some others are listed in the figure at the bottom of this page$)$. Non-Invertible Matrices So what do we do when $A$ is non-invertible and we are still interested in finding a solution for a system? Well, we can use a pseudo-inverse matrix with some compromise on the solution’s quality as follows. If there is no solution. That is we can’t reach $\vb$ and the system is overdetermined. Then, we can choose one with the least squares error. Mathematically, Differentiating w.r.t $\vx$ and setting it to zero, yields $\vx=(A^{T}A)^{-1}A^T\vb$. Therefore, the pseudo-inverse matrix is $(A^{T}A)^{-1}A^T$ If there are infinitely many solutions. That is the system is underdetermined. Then, one can choose one with the minimum norm. Mathematically, With the Lagrangian multipliers trick, the above can be solved as follows. Differentiating w.r.t $\vx$ and setting it to zero, $A^{T}$ is not invertible, but $AA^{T}$ is.1 Therefore, we can solve for $\mathbf{\lambda}$ as follows. Thus, $\vx=A^T(AA^T)^{-1}\vb$ Interestingly, both of the above pseudo-inverses are equivalent to the Moore-Penrose pseudo-inverse which can be computed from the singular value decomposition $($SVD$)$. Ill-Conditioned Matrices Let’s motivate this issue with an example. Assume you are reading a measurement, which you have to scale by dividing by a very small number. If your reading deviates a little, the absolute error will extremely large. Such large oscillations might be undesirable and we’d like to dampen them. In the matrix world, dividing by a small number corresponds to to multiplying by the inverse of an ill-conditioned matrix. An ill-conditioned matrix is still invertible, but it is numerically unstable. In addition to the measurement error $($here the measurement is captured by $\vb$$)$, inverting an ill-conditioned matrix also suffers from loss of precision in floating point arithmetic.To cope with this unstability, we compromise the quality of the solution and use regularization techniques to have a stable numerical outcome. The bulk of these techniques aim to increase $($or replace$)$ the value of the smallest eigenvalue with respect to the greatest eigenvalue. Common techniques are: Truncated SVD Tikhonov Regularization $($which can also be used for non-invertible matrices$)$. It is important to consider the regularization effect on the final solution and whether it makes sense to our system or not. Matrix Inversion Summary Below is a summary on matrix inversion. Summary of Matrix Inverse Techniques. $AA^T$ is a square matrix. What is left to show is that columns of $AA^T$ are linearly independent, and thus $AA^T$ is invertible. We know that the columns of $A^T$ are linearly independent, which means its null space is ${\mathbf{0}}$. By contradiction, let’s assume that $\mathbf{v}$ is a non-zero vector that belongs to $AA^T$’s null space, which means $AA^T \mathbf{v}= \mathbf{0}$. Mutiplying both sides by $\mathbf{v}$ yields $(A^T \mathbf{v})^{T}(A^T \mathbf{v})=0$, i.e., $A^T \mathbf{v}= \mathbf{0}$. This contradicts the fact that $A^T$’s null space is the zero vector. Therefore, $\mathbf{v}$ can not be zero and $AA^T$’s null space has no non-zero vector, and all of its columns are linearly independent.&amp;nbsp;&amp;#8617;</summary></entry><entry><title type="html">DACE/BLUP MSE for EGO</title><link href="http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace/" rel="alternate" type="text/html" title="DACE/BLUP MSE for EGO" /><published>2017-06-06T00:00:00-08:00</published><updated>2017-06-06T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace/">&lt;p&gt;$\newcommand{\hy}{\hat{y}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\rR}{\mathbf{r}^TR^{-1}}
\newcommand{\rRr}{\mathbf{r}^TR^{-1}\mathbf{r}}
\newcommand{\oRr}{\mathbf{1}^TR^{-1}\mathbf{r}}
\newcommand{\ymu}{(\mathbf{y}-\mathbf{1}\hmu)}
\newcommand{\hyexp}{\hmu+\rR\ymu}
\newcommand{\oRy}{\mathbf{1}^T\mathbf{R}
^{-1}\mathbf{y}}
\newcommand{\yRo}{\mathbf{y}^T\mathbf{R}
^{-1}\mathbf{1}}
\newcommand{\oRo}{\mathbf{1}^T\mathbf{R}^{-1}\mathbf{1}}
\newcommand{\hmuexp}{\frac{\oRy}{\oRo}}
\newcommand{\st}{\sigma^2}
\newcommand{\mt}{\mu^2}$
This post shows a derivation of the DACE predictor’s MSE discussed in &lt;a href=&quot;http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/&quot;&gt;Stochastic Processes for Expensive Black-Box Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The mean squared error of a predictor at $\mathbf{x}$ based on the stochastic Gaussian process is&lt;/p&gt;

&lt;p&gt;\begin{equation}MSE(\mathbf{x})=E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}\mathbf{r}(\mathbf{x})-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r}(\mathbf{x}))^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]\;,
\label{eq:final-mse}
\end{equation}&lt;/p&gt;

&lt;p&gt;where $\mu$ is process’s mean and $\sigma^2\mathbf{R}$ is its covariance matrix over a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$. For brevity, we will drop $\mathbf{x}$ henceforth. Before proceeding with the proof, let’s recall some terms and their definitions that will be useful in the proof.
\begin{equation}
\hy=\hyexp
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hmu=\hmuexp
\end{equation}&lt;/p&gt;

&lt;p&gt;From the above, we have 
$E[y^2]=\sigma^2 + \mu^2$, $E[\mathbf{y}\mathbf{y}^{T}]=\sigma^2\R+\mu^2\mathbf{1}\mathbf{1}^T$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[\hmu]=\frac{(\oRy)(\yRo)}{(\oRo)^2}=\sigma^2\cdot \frac{(\oRo)}{(\oRo)^2}+ \mu^2\cdot \frac{(\oRo)^2}{(\oRo)^2}=\frac{\st}{\oRo}+\mt&lt;/script&gt;

&lt;p&gt;Thus, we can expand the MSE term as&lt;/p&gt;

&lt;p&gt;\begin{equation}
MSE= \sigma^2 + \mu^2 + E[{\hy}^2] - 2 E[y\hy]\;.
\label{eq:mse}
\end{equation}
Where 
\begin{equation}
E[\hy^2]=\frac{\st}{\oRo}+\mt + \st (\rRr) - \st \frac{(\oRr)^2}{\oRo}
\label{eq:h2}
\end{equation}
and&lt;/p&gt;

&lt;p&gt;\begin{equation}
-2E[y\hy]=-2\st(\rRr)-2\mt-2\st\frac{\oRr}{\oRo}+2\st\frac{(\oRr)^2}{\oRo}\;.
\label{eq:h3}
\end{equation}&lt;/p&gt;

&lt;p&gt;Plugging Equations \ref{eq:h2} and \ref{eq:h3} into Eq. \ref{eq:mse} results in Eq. \ref{eq:final-mse}.&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">$\newcommand{\hy}{\hat{y}} \newcommand{\hmu}{\hat{\mu}} \newcommand{\R}{\mathbf{R}} \newcommand{\hmu}{\hat{\mu}} \newcommand{\rR}{\mathbf{r}^TR^{-1}} \newcommand{\rRr}{\mathbf{r}^TR^{-1}\mathbf{r}} \newcommand{\oRr}{\mathbf{1}^TR^{-1}\mathbf{r}} \newcommand{\ymu}{(\mathbf{y}-\mathbf{1}\hmu)} \newcommand{\hyexp}{\hmu+\rR\ymu} \newcommand{\oRy}{\mathbf{1}^T\mathbf{R} ^{-1}\mathbf{y}} \newcommand{\yRo}{\mathbf{y}^T\mathbf{R} ^{-1}\mathbf{1}} \newcommand{\oRo}{\mathbf{1}^T\mathbf{R}^{-1}\mathbf{1}} \newcommand{\hmuexp}{\frac{\oRy}{\oRo}} \newcommand{\st}{\sigma^2} \newcommand{\mt}{\mu^2}$ This post shows a derivation of the DACE predictor’s MSE discussed in Stochastic Processes for Expensive Black-Box Optimization. The mean squared error of a predictor at $\mathbf{x}$ based on the stochastic Gaussian process is \begin{equation}MSE(\mathbf{x})=E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}\mathbf{r}(\mathbf{x})-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r}(\mathbf{x}))^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]\;, \label{eq:final-mse} \end{equation} where $\mu$ is process’s mean and $\sigma^2\mathbf{R}$ is its covariance matrix over a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$. For brevity, we will drop $\mathbf{x}$ henceforth. Before proceeding with the proof, let’s recall some terms and their definitions that will be useful in the proof. \begin{equation} \hy=\hyexp \end{equation} \begin{equation} \hmu=\hmuexp \end{equation} From the above, we have $E[y^2]=\sigma^2 + \mu^2$, $E[\mathbf{y}\mathbf{y}^{T}]=\sigma^2\R+\mu^2\mathbf{1}\mathbf{1}^T$. Thus, we can expand the MSE term as \begin{equation} MSE= \sigma^2 + \mu^2 + E[{\hy}^2] - 2 E[y\hy]\;. \label{eq:mse} \end{equation} Where \begin{equation} E[\hy^2]=\frac{\st}{\oRo}+\mt + \st (\rRr) - \st \frac{(\oRr)^2}{\oRo} \label{eq:h2} \end{equation} and \begin{equation} -2E[y\hy]=-2\st(\rRr)-2\mt-2\st\frac{\oRr}{\oRo}+2\st\frac{(\oRr)^2}{\oRo}\;. \label{eq:h3} \end{equation} Plugging Equations \ref{eq:h2} and \ref{eq:h3} into Eq. \ref{eq:mse} results in Eq. \ref{eq:final-mse}.</summary></entry><entry><title type="html">Random Projections for Lipschitz Functions</title><link href="http://ash-aldujaili.github.io/blog/2017/05/08/rand-proj-lipschitz/" rel="alternate" type="text/html" title="Random Projections for Lipschitz Functions" /><published>2017-05-08T00:00:00-08:00</published><updated>2017-05-08T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/05/08/rand-proj-lipschitz</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/05/08/rand-proj-lipschitz/">&lt;p&gt;In this post, we’ll shed some light on the behavior of Lipschitz-continuous functions at random projections of the same point in another space. Consider Figure 1; we have a point in the $\mathbf{y}$ in the lower dimensional space $\mathcal{Y}$ that is projected randomly to the higher dimensional space $\mathcal{X}$ twice using two randomly sampled matrices $A_p$ and $A_q$. With this setting at hand, we are interested in the following question: is there a relation between the function values at $A_p\mathbf{y}$ and $A_q\mathbf{y}$. This has been addressed in [1, Theorem I]. Here, we provide a numerical validation of [1]’s result besides reiterating the formal proof. Before we delve into this further, let’s introduce some notation in accordance with [1].&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/illust_rand_lipschitz.png&quot; width=&quot;1000&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 1. Two random projections of $\mathbf{y}$ to $\mathcal{X}$.
&lt;/p&gt;

&lt;h4&gt;Notation&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/notation_rand_lipschitz.png&quot; width=&quot;600&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;

&lt;/p&gt;

&lt;h4&gt;Theorem I [1]&lt;/h4&gt;

&lt;p&gt;It has been shown in [1] that the mean variation in the objective value for a point $\mathbf{y}$ in the low-dimensional
space $\mathcal{Y} \subset \mathbb{R}^d$ projected randomly into the decision space $\mathcal{X}\subset\mathbb{R}^n$ of Lipschitz-continuous
problems is &lt;strong&gt;bounded&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Mathematically, for all $\mathbf{y} \in \mathcal{Y}$, we have
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}[|g_p(\mathbf{y}) − g_q(\mathbf{y})|] \leq \sqrt{8} \cdot L 
\cdot ||\mathbf{y}||\;.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here, we reproduce [1]’s proof for completeness, then we validate the theorem numerically.&lt;/p&gt;

&lt;h4&gt;Proof&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/proof_rand_lipschitz.png&quot; width=&quot;600&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;

&lt;/p&gt;
&lt;h4&gt;Numerical Validation&lt;/h4&gt;

&lt;p&gt;Here, we carry out a numerical validation of [1]’s Theorem I over four popular Lipschitz-like benchmark functions $($namely, Cigar, Sphere, Rastrigin, and Bohachevsky$)$.&lt;/p&gt;

&lt;p&gt;First, we sampled 100 points $\mathbf{y}\in\mathcal{Y}\subset \mathbb{R}^d$ whose norms span the range $[0,1]$. E.g., with $d=1$, $\mathbf{y}\in[-1,1]$.&lt;/p&gt;

&lt;p&gt;Second, each of these 100 points are projected to the function space $\mathcal{X}\subset\mathbb{R}^n$ using 20 random matrices ${A_p}_{1\leq p\leq 20}$.&lt;/p&gt;

&lt;p&gt;Third, the four considered functions are evaluated at these random projections. Note that for each of these functions, $100*20$ function evaluations are performed.&lt;/p&gt;

&lt;p&gt;Fourth, we average over the absolute difference between function values at projections of the same point $\mathbf{y}$. That is, for each of the 100 points, we compute the mean of $\frac{20*20- 20}{2}=190$ values, each of which representing the absolute difference of the function value at two random projections of the corresponding point $\mathbf{y}$.&lt;/p&gt;

&lt;p&gt;Fifth, the computed average values are plotted as a function of the  corresponding point $\mathbf{y}$ in Figure 1. One can see how the trend of the plot follows the curve of $||\mathbf{y}||$ in accordance with [1]’s Theorem I.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/rand_lipschitz.jpg&quot; width=&quot;1000&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 2. Empirical mean of the absolute value difference of four functions evaluated at 20 random projections in $\mathbb{R}^n$ of a point in $\mathbb{R}^d$, where $d=2$ and $n=10^3$.
&lt;/p&gt;
&lt;hr /&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;p&gt;This post is based on the following papers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Al-Dujaili, Abdullah, and S. Suresh.&lt;/strong&gt; &lt;em&gt;“Embedded Bandits for Large-Scale Black-Box Optimization.”&lt;/em&gt; Thirty-First AAAI Conference on Artificial Intelligence. 2017.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we’ll shed some light on the behavior of Lipschitz-continuous functions at random projections of the same point in another space. Consider Figure 1; we have a point in the $\mathbf{y}$ in the lower dimensional space $\mathcal{Y}$ that is projected randomly to the higher dimensional space $\mathcal{X}$ twice using two randomly sampled matrices $A_p$ and $A_q$. With this setting at hand, we are interested in the following question: is there a relation between the function values at $A_p\mathbf{y}$ and $A_q\mathbf{y}$. This has been addressed in [1, Theorem I]. Here, we provide a numerical validation of [1]’s result besides reiterating the formal proof. Before we delve into this further, let’s introduce some notation in accordance with [1]. Figure 1. Two random projections of $\mathbf{y}$ to $\mathcal{X}$. Notation Theorem I [1] It has been shown in [1] that the mean variation in the objective value for a point $\mathbf{y}$ in the low-dimensional space $\mathcal{Y} \subset \mathbb{R}^d$ projected randomly into the decision space $\mathcal{X}\subset\mathbb{R}^n$ of Lipschitz-continuous problems is bounded. Mathematically, for all $\mathbf{y} \in \mathcal{Y}$, we have Here, we reproduce [1]’s proof for completeness, then we validate the theorem numerically. Proof Numerical Validation Here, we carry out a numerical validation of [1]’s Theorem I over four popular Lipschitz-like benchmark functions $($namely, Cigar, Sphere, Rastrigin, and Bohachevsky$)$. First, we sampled 100 points $\mathbf{y}\in\mathcal{Y}\subset \mathbb{R}^d$ whose norms span the range $[0,1]$. E.g., with $d=1$, $\mathbf{y}\in[-1,1]$. Second, each of these 100 points are projected to the function space $\mathcal{X}\subset\mathbb{R}^n$ using 20 random matrices ${A_p}_{1\leq p\leq 20}$. Third, the four considered functions are evaluated at these random projections. Note that for each of these functions, $100*20$ function evaluations are performed. Fourth, we average over the absolute difference between function values at projections of the same point $\mathbf{y}$. That is, for each of the 100 points, we compute the mean of $\frac{20*20- 20}{2}=190$ values, each of which representing the absolute difference of the function value at two random projections of the corresponding point $\mathbf{y}$. Fifth, the computed average values are plotted as a function of the corresponding point $\mathbf{y}$ in Figure 1. One can see how the trend of the plot follows the curve of $||\mathbf{y}||$ in accordance with [1]’s Theorem I. Figure 2. Empirical mean of the absolute value difference of four functions evaluated at 20 random projections in $\mathbb{R}^n$ of a point in $\mathbb{R}^d$, where $d=2$ and $n=10^3$. References This post is based on the following papers: Al-Dujaili, Abdullah, and S. Suresh. “Embedded Bandits for Large-Scale Black-Box Optimization.” Thirty-First AAAI Conference on Artificial Intelligence. 2017.</summary></entry><entry><title type="html">Stochastic Processes for Expensive Black-Box Optimization</title><link href="http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/" rel="alternate" type="text/html" title="Stochastic Processes for Expensive Black-Box Optimization" /><published>2017-03-01T00:00:00-08:00</published><updated>2017-03-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/">&lt;p&gt;In this post, we’ll go through one of the techniques of Black-Box Deterministic Optimization: &lt;strong&gt;Response Surface Methdology&lt;/strong&gt;. In general, this approach models the objective and constraint functions with stochastic processes based on a few evaluations at points in the search space; with the goal of predicting the functions’ behavior as we move away from the evaluated points by different amounts in each coordinate direction.&lt;/p&gt;

&lt;p&gt;The stochastic process approach has been used in several fields under such names as ‘kriging’, ‘Bayesian optimization’, and ‘random function approach’. It comes with two main advantages:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Suitability for Expensive Optimization - as it quickly captures the trends over the search space.&lt;/li&gt;
  &lt;li&gt;Ease of establishing reasonable stopping rules - as it provide statistical confidence intervals.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4&gt;Modelling Alternatives&lt;/h4&gt;

&lt;p&gt;Let’s say we have evaluated a deterministic function,  $~y:\mathbb{R}^k\to \mathbb{R}$, at $n$ points. In other words, we have a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$ 
from the $k$-dimensional space, and we would like to use these observation in guiding our search for the optimal point. Let’s explore possible models that may help us in that.&lt;/p&gt;

&lt;hr /&gt;
&lt;h4&gt;1. &lt;strong&gt;Linear Regression&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;One can use linear regression as a simple way to fit a response surface to $\mathcal{D}$. In linear regression, the observations are assumed to be generated from the following model.&lt;/p&gt;

&lt;p&gt;\begin{equation}
y(\mathbf{x}^{(i)}) = \sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)}) + \epsilon^{(i)}\; (i = 1,\ldots,n).
\label{eq:lin-reg}
\end{equation}&lt;/p&gt;

&lt;p&gt;In the above equation, the $f_h(\mathbf{x})$’s are a set of linear or non-linear functions of $\mathbf{x}$; the $\beta_h$’s are unknown parameters of the model to be estimated; and the $\epsilon^{(i)}$’s are independent error terms with a normal distribution of zero mean and $\sigma^2$ variance.&lt;/p&gt;

&lt;p&gt;Employing Eq. \ref{eq:lin-reg} has two main concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A Conceptual Concern: Our function $f$ is deterministic. Therefore, any improper fit of Eq. \ref{eq:lin-reg} will be a modelling error—i.e., incomplete regression term $\sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)})$—rather than measurement noise or error—i.e., the $\epsilon^{(i)}$ term. In essence $\epsilon^{(i)}=\epsilon(\mathbf{x}^{(i)})$ captures the value of the left-out regression terms at $\mathbf{x}^{(i)}$. And if $y(\mathbf{x})$ is continuous, then $\epsilon(\mathbf{x})$ is continuous as well. Thus, if two points $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$ are close, then the errors $\epsilon(\mathbf{x}^{(i)})$ and $\epsilon(\mathbf{x}^{(j)})$ should be close as well.&lt;/li&gt;
  &lt;li&gt;A Practical Concern: How to define the $f_h$’s functions. Indeed, if we knew the perfect form of Eq. \ref{eq:lin-reg}, our function $f$ would not have been a black box in the first place. One can certainly employ a meta-model $($some referred to it as flexible functional form$)$ approach to decide the most suitable form of Eq. \ref{eq:lin-reg}. However, this approach comes with an additional set of parameters $($hyperparameters$)$ and requires many more function evaluations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In light of the above concerns, it makes no sense to assume that the error terms $\epsilon^{(i)}$’s are independent. Alternatively, we can assume that these terms are associated / related and that the level of such association $($correlation$)$ is low when the corresponding points $\mathbf{x}^{(i)}$’s are far away and high when they are close. The following equation captures this assumption of correlation.&lt;/p&gt;

&lt;p&gt;\begin{equation}
Corr[\epsilon(\mathbf{x}^{(i)}),\epsilon(\mathbf{x}^{(j)})] = \exp\big(- \sum_{h=1}^{k} \theta_h |x^{(j)}_h-x^{(j)}_h|^{p_h}\big)\;, \theta_h \geq 0\;, p_h \in [0,1]\;.
\label{eq:corr-fct}
\end{equation}&lt;/p&gt;

&lt;p&gt;The parameters $\theta_h$’s and $p_h$’s controls how fast the degree of correlation changes between two errors terms as the (weighted) distance between the corresponding points changes as shown in the figure below. In general, $\theta_h$ measures the importance / activity of the $h^{th}$ variable $x_h$ $($the higher $\theta_h$ is, the faster the correlation drops as the corresponding points move away in the coordinate direction $h)$, while $p_h$ captures the function smoothness in the coordinate direction $h$ $(p_h$ = 2 is for smooth functions$)$.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/err_corr.jpg&quot; width=&quot;400&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 1. Correlation Function $($Eq. \ref{eq:corr-fct}$)$ with different parameters.
&lt;/p&gt;

&lt;p&gt;Having modelled the correlation between our observations, fitting the regression term to our observations is less of a concern now. In fact, this forms the basis of the stochastic process model presented next.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;2. &lt;strong&gt;DACE&lt;/strong&gt; $($Design and Analysis of Computer Experiments [1]$)$&lt;/h4&gt;

&lt;p&gt;Based on the correlation model $($Eq. \ref{eq:corr-fct}$)$, one can view the
error term $\epsilon(\mathbf{x})$ as a stochastic process - a set of correlated random variables indexed by the $k$-dimensional space of $\mathbf{x}$. Therefore, the observations can be modeled with the following stochastic process:&lt;/p&gt;

&lt;p&gt;\begin{equation}
y^{(i)} = \mu + \epsilon(\mathbf{x}^{(i)})\; (i=1, \ldots, n).
\label{eq:dace}
\end{equation}&lt;/p&gt;

&lt;p&gt;In comparison to Eq. \ref{eq:lin-reg}, $\mu$ $($the mean of the stochastic process$)$ is the simplest regression term $($ a constant $)$; $\epsilon(\mathbf{x}^{(i)})\sim \mathcal{N}(0, \sigma^2)$ with a correlation given by Eq. \ref{eq:corr-fct}. This model is commonly referred to as &lt;strong&gt;DACE&lt;/strong&gt; - an acronym to the paper that popularized it [1].&lt;/p&gt;

&lt;p&gt;We can estimate DACE’s $2k+2$ parameters by maximizing the likelihood of our sample $\mathcal{D}$. With the correlation between two error terms defined as above, the vector $\mathbf{y}=[y^{(1)},\ldots, y^{(n)}]$ of observed function values is a normal random $n$-vector. That is, $\mathbf{y}\sim\mathcal{N}(\mu, \sigma^2\mathbf{R})$. Thus, the likelihood function of our samples can be written as follows.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 
\begin{equation}
f(\mathbf{y};\theta,p, \mu, \sigma) = 
\frac{1}
{(2\pi)^{n/2}(\sigma^2)^{n/2}|\mathbf{R}|^{(1/2)}}
\exp\big(-\frac{(\mathbf{y}-\mathbf{1}\mu)^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\mu)}{2\sigma^2}\big)
\label{eq:likelihood}
\end{equation}&lt;/p&gt;

&lt;p&gt;Eq. $\ref{eq:likelihood}$ can be optimized with respect to $\mu$ and $\sigma^2$ in closed form by taking the partial derivative of its log:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mu}=\frac{\mathbf{1}^{T}\mathbf{R}^{-1}\mathbf{y}}{\mathbf{1}^{T}\mathbf{R}^{-1}\mathbf{1}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\sigma}^2 = \frac{(\mathbf{y}-\mathbf{1}\hat{\mu})^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\hat{\mu})}{n}&lt;/script&gt;

&lt;p&gt;The above equations can in turn be plugged into the likelihood function to form a &lt;em&gt;concentrated likelihood function&lt;/em&gt;, which can be optimized with resepct to the parameters $\theta_h$’s and $p_h$’s. Subsequently, the corresponding values of $\hat{\mu}$ and $\hat{\sigma}^2$ are computed.&lt;/p&gt;

&lt;p&gt;Having estimated the parameters of our stochastic process model, we can now use it to predict the function value at new points in the $k$-dimensional space. The stochastic process model encodes the left-out terms of the regression model using the correlated errors. In other words, these correlated errors must be employed to adjust the prediction above or below the constant regression term $\hat{\mu}$. To see how this can be done, assume that we are interested in predicting the function value at $x^{(n+1)}$ and let $y^{*}$ be some prediction candidate for $f(x^{(n+1)})$. To measure how well $(x^{(n+1)},y^{*})$ fits the sample $\mathcal{D}$, one can compute the joint $($augmented$)$ likelihood function of $\tilde{\mathbf{y}} = (\mathbf{y}\; y^{*})$ - $f(\tilde{\mathbf{y}};\theta,p, \hat{\mu}, \hat{\sigma})$ from Eq. \ref{eq:likelihood}’s $f(\mathbf{y};\theta,p, \mu, \sigma)$. Taking the log of Eq. \ref{eq:likelihood}, it is easy to see that the only term that depends on $y^{*}$ is $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$ with 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\tilde{\mathbf{R}} = \begin{bmatrix}
    \mathbf{R}       &amp; \mathbf{r} \\
    \mathbf{r}^{T}     &amp; 1 
\end{bmatrix} %]]&gt;&lt;/script&gt;, where $\mathbf{r} = \mathbf{r}(\mathbf{x}^{(n+1)}) = [Corr[\epsilon(\mathbf{x}^{(1)}),\epsilon(\mathbf{x}^{(n+1)})],\ldots,Corr[\epsilon(\mathbf{x}^{(n)}),\epsilon(\mathbf{x}^{(n+1)})]]$.&lt;/p&gt;

&lt;p&gt;Applying the &lt;a href=&quot;https://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion&quot;&gt;blockwise inversion formula&lt;/a&gt; on $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bigg(y^{*}-\text{independent terms}\bigg) + \frac{1}{1-\mathbf{r}^{T}R^{-1}\mathbf{r}}(y^{*}-\hat{\mu})^2 - \frac{2\mathbf{r}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu})}{1-\mathbf{r}^{T}\mathbf{R}^{-1}\mathbf{r}}(y^{*}-\hat{\mu})&lt;/script&gt;

&lt;p&gt;Thus, setting the derivative of the above term with respect to $y^{*}$ to zero gives the best prediction’s formula at $\mathbf{x}^{(n+1)}$ that fits the sample $\mathcal{D}$ as follows.&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hat{y}(\mathbf{x}) = \hat{\mu} + {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu})
\label{eq:prediction}
\end{equation}&lt;/p&gt;

&lt;p&gt;While this value maximizes the joint augmented likelihood of $(x^{(n+1)},y^{*})$ and $\mathcal{D}$, it is interesting to note that it also corresponds to the mean of the normal probablity density function of the function value at $x^{(n+1)}$ conditioned on $\mathcal{D}$.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/stoch_proc_pred.png&quot; width=&quot;400&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 2. Application of Eq. \ref{eq:prediction}
&lt;/p&gt;

&lt;p&gt;Figure 2 shows predicted values of $f$ at different points according to Eq. \ref{eq:prediction}. DACE’s predictor $($Eq. \ref{eq:prediction}$)$ is also an interpolator of $\mathcal{D}$. That is
$\hat{y}(\mathbf{x}^{(i)}) = \hat{\mu} + {\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu}) = y^{(i)}$, since ${\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}={(\mathbf{R}^{-1}\mathbf{R}_i)}^{T}=\mathbf{e}^T_i$. Furthermore, the prediction accuracy can be characterized by the mean squared error, which can be proved to be as follows.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - \mathbf{r}^{T}\mathbf{R}^{-1}\mathbf{r}-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r})^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;TO BE COMPLETED…&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;p&gt;This post is based on the following papers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sacks, Jerome, et al.&lt;/strong&gt; &lt;em&gt;Design and analysis of computer experiments.&lt;/em&gt; Statistical science $($1989$)$: 409-423.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Jones, Donald R.; Schonlau, Matthias; Welch, William J.&lt;/strong&gt; &lt;em&gt;Efficient global optimization of expensive black-box functions&lt;/em&gt;. J. Global Optim. 13 $($1998$)$, no. 4, 455–492.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Make use of: $det(cA)= c^n det(A)$, where A is an $n\times n$-matrix; $Cov[x^{(i)},x^{(j)}] = \sigma^2 \cdot Corr[x^{(i)},x^{(j)}]$; and the multivariate normal probability density function.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;This is proved &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we’ll go through one of the techniques of Black-Box Deterministic Optimization: Response Surface Methdology. In general, this approach models the objective and constraint functions with stochastic processes based on a few evaluations at points in the search space; with the goal of predicting the functions’ behavior as we move away from the evaluated points by different amounts in each coordinate direction. The stochastic process approach has been used in several fields under such names as ‘kriging’, ‘Bayesian optimization’, and ‘random function approach’. It comes with two main advantages: Suitability for Expensive Optimization - as it quickly captures the trends over the search space. Ease of establishing reasonable stopping rules - as it provide statistical confidence intervals. Modelling Alternatives Let’s say we have evaluated a deterministic function, $~y:\mathbb{R}^k\to \mathbb{R}$, at $n$ points. In other words, we have a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$ from the $k$-dimensional space, and we would like to use these observation in guiding our search for the optimal point. Let’s explore possible models that may help us in that. 1. Linear Regression One can use linear regression as a simple way to fit a response surface to $\mathcal{D}$. In linear regression, the observations are assumed to be generated from the following model. \begin{equation} y(\mathbf{x}^{(i)}) = \sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)}) + \epsilon^{(i)}\; (i = 1,\ldots,n). \label{eq:lin-reg} \end{equation} In the above equation, the $f_h(\mathbf{x})$’s are a set of linear or non-linear functions of $\mathbf{x}$; the $\beta_h$’s are unknown parameters of the model to be estimated; and the $\epsilon^{(i)}$’s are independent error terms with a normal distribution of zero mean and $\sigma^2$ variance. Employing Eq. \ref{eq:lin-reg} has two main concerns: A Conceptual Concern: Our function $f$ is deterministic. Therefore, any improper fit of Eq. \ref{eq:lin-reg} will be a modelling error—i.e., incomplete regression term $\sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)})$—rather than measurement noise or error—i.e., the $\epsilon^{(i)}$ term. In essence $\epsilon^{(i)}=\epsilon(\mathbf{x}^{(i)})$ captures the value of the left-out regression terms at $\mathbf{x}^{(i)}$. And if $y(\mathbf{x})$ is continuous, then $\epsilon(\mathbf{x})$ is continuous as well. Thus, if two points $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$ are close, then the errors $\epsilon(\mathbf{x}^{(i)})$ and $\epsilon(\mathbf{x}^{(j)})$ should be close as well. A Practical Concern: How to define the $f_h$’s functions. Indeed, if we knew the perfect form of Eq. \ref{eq:lin-reg}, our function $f$ would not have been a black box in the first place. One can certainly employ a meta-model $($some referred to it as flexible functional form$)$ approach to decide the most suitable form of Eq. \ref{eq:lin-reg}. However, this approach comes with an additional set of parameters $($hyperparameters$)$ and requires many more function evaluations. In light of the above concerns, it makes no sense to assume that the error terms $\epsilon^{(i)}$’s are independent. Alternatively, we can assume that these terms are associated / related and that the level of such association $($correlation$)$ is low when the corresponding points $\mathbf{x}^{(i)}$’s are far away and high when they are close. The following equation captures this assumption of correlation. \begin{equation} Corr[\epsilon(\mathbf{x}^{(i)}),\epsilon(\mathbf{x}^{(j)})] = \exp\big(- \sum_{h=1}^{k} \theta_h |x^{(j)}_h-x^{(j)}_h|^{p_h}\big)\;, \theta_h \geq 0\;, p_h \in [0,1]\;. \label{eq:corr-fct} \end{equation} The parameters $\theta_h$’s and $p_h$’s controls how fast the degree of correlation changes between two errors terms as the (weighted) distance between the corresponding points changes as shown in the figure below. In general, $\theta_h$ measures the importance / activity of the $h^{th}$ variable $x_h$ $($the higher $\theta_h$ is, the faster the correlation drops as the corresponding points move away in the coordinate direction $h)$, while $p_h$ captures the function smoothness in the coordinate direction $h$ $(p_h$ = 2 is for smooth functions$)$. Figure 1. Correlation Function $($Eq. \ref{eq:corr-fct}$)$ with different parameters. Having modelled the correlation between our observations, fitting the regression term to our observations is less of a concern now. In fact, this forms the basis of the stochastic process model presented next. 2. DACE $($Design and Analysis of Computer Experiments [1]$)$ Based on the correlation model $($Eq. \ref{eq:corr-fct}$)$, one can view the error term $\epsilon(\mathbf{x})$ as a stochastic process - a set of correlated random variables indexed by the $k$-dimensional space of $\mathbf{x}$. Therefore, the observations can be modeled with the following stochastic process: \begin{equation} y^{(i)} = \mu + \epsilon(\mathbf{x}^{(i)})\; (i=1, \ldots, n). \label{eq:dace} \end{equation} In comparison to Eq. \ref{eq:lin-reg}, $\mu$ $($the mean of the stochastic process$)$ is the simplest regression term $($ a constant $)$; $\epsilon(\mathbf{x}^{(i)})\sim \mathcal{N}(0, \sigma^2)$ with a correlation given by Eq. \ref{eq:corr-fct}. This model is commonly referred to as DACE - an acronym to the paper that popularized it [1]. We can estimate DACE’s $2k+2$ parameters by maximizing the likelihood of our sample $\mathcal{D}$. With the correlation between two error terms defined as above, the vector $\mathbf{y}=[y^{(1)},\ldots, y^{(n)}]$ of observed function values is a normal random $n$-vector. That is, $\mathbf{y}\sim\mathcal{N}(\mu, \sigma^2\mathbf{R})$. Thus, the likelihood function of our samples can be written as follows.1 \begin{equation} f(\mathbf{y};\theta,p, \mu, \sigma) = \frac{1} {(2\pi)^{n/2}(\sigma^2)^{n/2}|\mathbf{R}|^{(1/2)}} \exp\big(-\frac{(\mathbf{y}-\mathbf{1}\mu)^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\mu)}{2\sigma^2}\big) \label{eq:likelihood} \end{equation} Eq. $\ref{eq:likelihood}$ can be optimized with respect to $\mu$ and $\sigma^2$ in closed form by taking the partial derivative of its log: The above equations can in turn be plugged into the likelihood function to form a concentrated likelihood function, which can be optimized with resepct to the parameters $\theta_h$’s and $p_h$’s. Subsequently, the corresponding values of $\hat{\mu}$ and $\hat{\sigma}^2$ are computed. Having estimated the parameters of our stochastic process model, we can now use it to predict the function value at new points in the $k$-dimensional space. The stochastic process model encodes the left-out terms of the regression model using the correlated errors. In other words, these correlated errors must be employed to adjust the prediction above or below the constant regression term $\hat{\mu}$. To see how this can be done, assume that we are interested in predicting the function value at $x^{(n+1)}$ and let $y^{*}$ be some prediction candidate for $f(x^{(n+1)})$. To measure how well $(x^{(n+1)},y^{*})$ fits the sample $\mathcal{D}$, one can compute the joint $($augmented$)$ likelihood function of $\tilde{\mathbf{y}} = (\mathbf{y}\; y^{*})$ - $f(\tilde{\mathbf{y}};\theta,p, \hat{\mu}, \hat{\sigma})$ from Eq. \ref{eq:likelihood}’s $f(\mathbf{y};\theta,p, \mu, \sigma)$. Taking the log of Eq. \ref{eq:likelihood}, it is easy to see that the only term that depends on $y^{*}$ is $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$ with , where $\mathbf{r} = \mathbf{r}(\mathbf{x}^{(n+1)}) = [Corr[\epsilon(\mathbf{x}^{(1)}),\epsilon(\mathbf{x}^{(n+1)})],\ldots,Corr[\epsilon(\mathbf{x}^{(n)}),\epsilon(\mathbf{x}^{(n+1)})]]$. Applying the blockwise inversion formula on $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$, we have: Thus, setting the derivative of the above term with respect to $y^{*}$ to zero gives the best prediction’s formula at $\mathbf{x}^{(n+1)}$ that fits the sample $\mathcal{D}$ as follows. \begin{equation} \hat{y}(\mathbf{x}) = \hat{\mu} + {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu}) \label{eq:prediction} \end{equation} While this value maximizes the joint augmented likelihood of $(x^{(n+1)},y^{*})$ and $\mathcal{D}$, it is interesting to note that it also corresponds to the mean of the normal probablity density function of the function value at $x^{(n+1)}$ conditioned on $\mathcal{D}$. Figure 2. Application of Eq. \ref{eq:prediction} Figure 2 shows predicted values of $f$ at different points according to Eq. \ref{eq:prediction}. DACE’s predictor $($Eq. \ref{eq:prediction}$)$ is also an interpolator of $\mathcal{D}$. That is $\hat{y}(\mathbf{x}^{(i)}) = \hat{\mu} + {\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu}) = y^{(i)}$, since ${\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}={(\mathbf{R}^{-1}\mathbf{R}_i)}^{T}=\mathbf{e}^T_i$. Furthermore, the prediction accuracy can be characterized by the mean squared error, which can be proved to be as follows.2 TO BE COMPLETED… References This post is based on the following papers: Sacks, Jerome, et al. Design and analysis of computer experiments. Statistical science $($1989$)$: 409-423. Jones, Donald R.; Schonlau, Matthias; Welch, William J. Efficient global optimization of expensive black-box functions. J. Global Optim. 13 $($1998$)$, no. 4, 455–492. Make use of: $det(cA)= c^n det(A)$, where A is an $n\times n$-matrix; $Cov[x^{(i)},x^{(j)}] = \sigma^2 \cdot Corr[x^{(i)},x^{(j)}]$; and the multivariate normal probability density function.&amp;nbsp;&amp;#8617; This is proved here.&amp;nbsp;&amp;#8617;</summary></entry></feed>