<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://ash-aldujaili.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://ash-aldujaili.github.io/blog/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-12-14T19:10:23-08:00</updated><id>http://ash-aldujaili.github.io/blog/</id><title type="html">Hunting Optima</title><subtitle>Write-ups on optimization</subtitle><author><name>Abdullah Al-Dujaili</name></author><entry><title type="html">NeurIPS 2018 - Links to 3-min Videos</title><link href="http://ash-aldujaili.github.io/blog/2018/12/13/nips-videos/" rel="alternate" type="text/html" title="NeurIPS 2018 - Links to 3-min Videos" /><published>2018-12-13T00:00:00-08:00</published><updated>2018-12-13T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/12/13/nips-videos</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/12/13/nips-videos/">&lt;p&gt;Navigating through NeurIPS schedule can be overwhelming, and it is difficult to decide which paper to read/consider. 3-minute videos can be of help in this decision-making process.&lt;/p&gt;

&lt;p&gt;Instead of scrolling through each paper and hunt for its 3-min video (if any), below is a script to generate a csv file of papers that have videos, namely their titles and video links.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/ash-aldujaili/5ea3c76d2b574898d0bb93641534c27e.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Running this with&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python nips_videos.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;will create a file &lt;code class=&quot;highlighter-rouge&quot;&gt;nips_videos.csv&lt;/code&gt;, which lists papers that have videos (namely their titles and video links ) . If you are in a hurry, you can find the file &lt;a href=&quot;https://figshare.com/s/70f0d50b6b5e80617f62&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">Navigating through NeurIPS schedule can be overwhelming, and it is difficult to decide which paper to read/consider. 3-minute videos can be of help in this decision-making process. Instead of scrolling through each paper and hunt for its 3-min video (if any), below is a script to generate a csv file of papers that have videos, namely their titles and video links. Running this with python nips_videos.py will create a file nips_videos.csv, which lists papers that have videos (namely their titles and video links ) . If you are in a hurry, you can find the file here.</summary></entry><entry><title type="html">Deep Learning for Robust Detection of Evasive Malware</title><link href="http://ash-aldujaili.github.io/blog/2018/08/29/evasive-malware/" rel="alternate" type="text/html" title="Deep Learning for Robust Detection of Evasive Malware" /><published>2018-08-29T00:00:00-08:00</published><updated>2018-08-29T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/08/29/evasive-malware</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/08/29/evasive-malware/">&lt;p&gt;I’ve written 3 posts on Medium about our recent work on robust detection of adversarial malware using deep learning. Check it out :)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/alfagroup-csail-mit/robust-detection-of-evasive-malware-part-1-312efc1bccc3&quot;&gt;Part 1&lt;/a&gt;: An intro to Malware Detection, with a demo on evasion attacks&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/alfagroup-csail-mit/robust-detection-of-evasive-malware-part-2-c537719535d8&quot;&gt;Part 2&lt;/a&gt;: A brief description on hardening the detectors against adversarial/evasive malware.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/alfagroup-csail-mit/robust-detection-of-evasive-malware-part-3-6e67d992654f&quot;&gt;Part 3&lt;/a&gt;: A visual inspection of the decision landscape and its association with robust generalization.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">I’ve written 3 posts on Medium about our recent work on robust detection of adversarial malware using deep learning. Check it out :) Part 1: An intro to Malware Detection, with a demo on evasion attacks Part 2: A brief description on hardening the detectors against adversarial/evasive malware. Part 3: A visual inspection of the decision landscape and its association with robust generalization.</summary></entry><entry><title type="html">Benchmarking Open-Source Bayesian Optimization Implementations on the COCO Noiseless Testbed</title><link href="http://ash-aldujaili.github.io/blog/2018/04/01/coco-bayesopt/" rel="alternate" type="text/html" title="Benchmarking Open-Source Bayesian Optimization Implementations on the COCO Noiseless Testbed" /><published>2018-04-01T00:00:00-08:00</published><updated>2018-04-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/04/01/coco-bayesopt</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/04/01/coco-bayesopt/">&lt;p&gt;There have been several open-source implementations of Bayesian Optimization algorithms. In this post, we evaluate a couple of them $($tabulated in the table below$)$ on the COCO benchmark platform.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Implementation&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Source&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SkGP&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;https://github.com/scikit-optimize/scikit-optimize/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SkGBRT&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;https://github.com/scikit-optimize/scikit-optimize/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;GPyOptimize&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;http://sheffieldml.github.io/GPyOpt/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;BO&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;https://github.com/fmfn/BayesianOptimization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PyBO&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;https://github.com/mwhoffman/pybo&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;YelpMOE&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;https://github.com/Yelp/MOE/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HyperOpt&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;http://hyperopt.github.io/hyperopt/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Spearmint&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;https://github.com/HIPS/Spearmint&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Results from experiments according to [6] and [2] on the benchmark functions given in [1, 5] are presented in the figures below. The experiments were performed with COCO [4], version 2.0, with an evaluation budget of 20 times the problem dimensionaliy.&lt;/p&gt;

&lt;p&gt;The average runtime (aRT), used in the figures and tables, depends on a given target function value, $f_t = f_{opt} + \triangle f$ ,
and is computed over all relevant trials as the number of function evaluations executed during each trial while the best function value did not reach $f_t$, summed over all trials and divided by the number of trials that actually reached $f_t$ [3, 7].&lt;/p&gt;

&lt;p&gt;In terms of running time, from slowest to fastest: GPyOptimize, Spearmint, BO, PyBO, SkGP, YelpMOE, SKGBRT, HyperOpt.&lt;/p&gt;

&lt;p&gt;In terms of performance, in low-dimension $(D\leq 10)$, PyBO appears to be outperforming consistently. With higher dimensions, most of the algorithms get extremely slow and algorithms like SkGP and SkGBRT seem to be a good tradeoff.&lt;/p&gt;

&lt;p&gt;Note: The figures about 20-D and 40-D $($ and even 10-D to some extent $)$ should be considered carefully. Not all the algorithms manage to solve the probelms at the time of producing the results. E.g., Spearmint manages to solve 20-D separable functions, while the rest  of the problems are not solved $($ can you see its curve is present in the &lt;code class=&quot;highlighter-rouge&quot;&gt;separable fcts&lt;/code&gt; subplot but not the rest $)$. This in turn, affects its final standing in the &lt;code class=&quot;highlighter-rouge&quot;&gt;all fcts&lt;/code&gt; subplot. The 2-D and 3-D plots are the only ones in which all the algorithms manage to solve all the problems.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/bayescoco/1.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/bayescoco/2.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/bayescoco/3.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/bayescoco/4.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/bayescoco/5.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/bayescoco/6.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;S. Finck, N. Hansen, R. Ros, and A. Auger. 2009. Real-Parameter Black-Box Optimization Benchmarking 2009: Presentation of the Noiseless Functions.
Technical Report 2009/20. Research Center PPE. hp://coco.lri.fr/downloads/download15.03/bbobdocfunctions.pdf Updated February 2010.&lt;/li&gt;
  &lt;li&gt;N. Hansen, A Auger, D. Brockho, D. Tuˇsar, and T. Tuˇsar. 2016. COCO: Performance Assessment. ArXiv e-prints arXiv:1605.03560 (2016).&lt;/li&gt;
  &lt;li&gt;N. Hansen, A. Auger, S. Finck, and R. Ros. 2012. Real-Parameter Black-Box Optimization Benchmarking 2012: Experimental Setup. Technical Report.
INRIA. hp://coco.gforge.inria.fr/bbob2012-downloads&lt;/li&gt;
  &lt;li&gt;N. Hansen, A. Auger, O. Mersmann, T. Tuˇsar, and D. Brockho. 2016. COCO: A Platform for Comparing Continuous Optimizers in a Black-Box
Seing. ArXiv e-prints arXiv:1603.08785 (2016).&lt;/li&gt;
  &lt;li&gt;N. Hansen, S. Finck, R. Ros, and A. Auger. 2009. Real-Parameter Black-Box Optimization Benchmarking 2009: Noiseless Functions Denitions. Technical
Report RR-6829. INRIA. hp://coco.lri.fr/downloads/download15.03/bbobdocfunctions.pdf Updated February 2010.&lt;/li&gt;
  &lt;li&gt;N. Hansen, T. Tuˇsar, O. Mersmann, A. Auger, and D. Brockho. 2016. COCO: e Experimental Procedure. ArXiv e-prints arXiv:1603.08776 (2016).
[7] Kenneth Price. 1997. Dierential evolution vs. the functions of the second ICEO. In Proceedings of the IEEE International Congress on Evolutionary
Computation. IEEE, Piscataway, NJ, USA, 153–157. DOI:hp://dx.doi.org/10.1109/ICEC.1997.592287&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">There have been several open-source implementations of Bayesian Optimization algorithms. In this post, we evaluate a couple of them $($tabulated in the table below$)$ on the COCO benchmark platform. Implementation Source SkGP https://github.com/scikit-optimize/scikit-optimize/ SkGBRT https://github.com/scikit-optimize/scikit-optimize/ GPyOptimize http://sheffieldml.github.io/GPyOpt/ BO https://github.com/fmfn/BayesianOptimization PyBO https://github.com/mwhoffman/pybo YelpMOE https://github.com/Yelp/MOE/ HyperOpt http://hyperopt.github.io/hyperopt/ Spearmint https://github.com/HIPS/Spearmint     Results from experiments according to [6] and [2] on the benchmark functions given in [1, 5] are presented in the figures below. The experiments were performed with COCO [4], version 2.0, with an evaluation budget of 20 times the problem dimensionaliy. The average runtime (aRT), used in the figures and tables, depends on a given target function value, $f_t = f_{opt} + \triangle f$ , and is computed over all relevant trials as the number of function evaluations executed during each trial while the best function value did not reach $f_t$, summed over all trials and divided by the number of trials that actually reached $f_t$ [3, 7]. In terms of running time, from slowest to fastest: GPyOptimize, Spearmint, BO, PyBO, SkGP, YelpMOE, SKGBRT, HyperOpt. In terms of performance, in low-dimension $(D\leq 10)$, PyBO appears to be outperforming consistently. With higher dimensions, most of the algorithms get extremely slow and algorithms like SkGP and SkGBRT seem to be a good tradeoff. Note: The figures about 20-D and 40-D $($ and even 10-D to some extent $)$ should be considered carefully. Not all the algorithms manage to solve the probelms at the time of producing the results. E.g., Spearmint manages to solve 20-D separable functions, while the rest of the problems are not solved $($ can you see its curve is present in the separable fcts subplot but not the rest $)$. This in turn, affects its final standing in the all fcts subplot. The 2-D and 3-D plots are the only ones in which all the algorithms manage to solve all the problems. S. Finck, N. Hansen, R. Ros, and A. Auger. 2009. Real-Parameter Black-Box Optimization Benchmarking 2009: Presentation of the Noiseless Functions. Technical Report 2009/20. Research Center PPE. hp://coco.lri.fr/downloads/download15.03/bbobdocfunctions.pdf Updated February 2010. N. Hansen, A Auger, D. Brockho, D. Tuˇsar, and T. Tuˇsar. 2016. COCO: Performance Assessment. ArXiv e-prints arXiv:1605.03560 (2016). N. Hansen, A. Auger, S. Finck, and R. Ros. 2012. Real-Parameter Black-Box Optimization Benchmarking 2012: Experimental Setup. Technical Report. INRIA. hp://coco.gforge.inria.fr/bbob2012-downloads N. Hansen, A. Auger, O. Mersmann, T. Tuˇsar, and D. Brockho. 2016. COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Seing. ArXiv e-prints arXiv:1603.08785 (2016). N. Hansen, S. Finck, R. Ros, and A. Auger. 2009. Real-Parameter Black-Box Optimization Benchmarking 2009: Noiseless Functions Denitions. Technical Report RR-6829. INRIA. hp://coco.lri.fr/downloads/download15.03/bbobdocfunctions.pdf Updated February 2010. N. Hansen, T. Tuˇsar, O. Mersmann, A. Auger, and D. Brockho. 2016. COCO: e Experimental Procedure. ArXiv e-prints arXiv:1603.08776 (2016). [7] Kenneth Price. 1997. Dierential evolution vs. the functions of the second ICEO. In Proceedings of the IEEE International Congress on Evolutionary Computation. IEEE, Piscataway, NJ, USA, 153–157. DOI:hp://dx.doi.org/10.1109/ICEC.1997.592287</summary></entry><entry><title type="html">Computing the Optimal Discriminator between Mixture of Gaussians</title><link href="http://ash-aldujaili.github.io/blog/2018/03/20/opt-discr/" rel="alternate" type="text/html" title="Computing the Optimal Discriminator between Mixture of Gaussians" /><published>2018-03-20T00:00:00-08:00</published><updated>2018-03-20T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/03/20/opt-discr</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/03/20/opt-discr/">&lt;p&gt;Let $p$ and $q$ be two mixtures of $k$ univariate Gaussians as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)=\sum^{k}_{i=1} \pi^{(i)}_p \mathcal{N}(\mu^{(i)}_p, \sigma^{(i)}_p)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(x)=\sum^{k}_{j=1} \pi^{(j)}_q \mathcal{N}(\mu^{(j)}_q, \sigma^{(j)}_q)&lt;/script&gt;

&lt;p&gt;We are interested in a discriminator $D$ that distinguishes between the above distributions. Let’s assess $D$’s performance with the WGAN objective&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(D) = \mathbb{E}_{x\sim p}[D(x)] + \mathbb{E}_{x\sim q}[1-D(x)]&lt;/script&gt;

&lt;p&gt;That is, the greater $L$ is, the better our discriminator $D$ is. Let the output range of $D$ be $[0,1]$. $L(D)$ can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(D)=\int_{\mathcal{X}}D(x) (p(x)-q(x)) dx+ 1&lt;/script&gt;

&lt;p&gt;Since $D(x)\geq 0$, $L(D)$ is maximized with $D(x)=0$ for $p(x)&amp;lt;q(x)$ and $D(x)=1$ for $p(x)\geq q(x)$. With this observation at hand, one can think of the discriminator as an indicator function, with the optimal one being $D^*(x)=\mathbb{1}{p(x)\geq q(x)}$.&lt;/p&gt;

&lt;p&gt;For univariate Gaussians, $D^*(x)=1$ on interavls where $p(x)\geq q(x)$ and $0$ otherwise. To find these intervals, one can look for the zero-crossings of the function $p(x)-q(x)$. From &lt;a href=&quot;https://arxiv.org/pdf/1706.09884.pdf&quot;&gt;Theorem A.2&lt;/a&gt;, the linear combination $p(x)-q(x)$ has at most $2k-1$ zero-crossings.&lt;/p&gt;

&lt;p&gt;With $k=2$, we have 3 zero-crossings, which we can find with a root-finding solver given good initial guesses. Subsequently, $p(x)\geq q(x)$ over at most $2k-2=2$ disjoint intervals. What’s left now is computing the lower and upper bounds of these intervals for an optimal discriminator. We will demonstrate this in the following snippets.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import root
from scipy.stats import norm


def f(_x, *args):
	&quot;&quot;&quot;
	evaluate p(x) - q(x)
	It is assumed that sigma = 1 for all the Gaussians
	&quot;&quot;&quot;
    _params = args[0]
    p_x = 0.5 * (norm.pdf(_x, loc=_params['p_mu_1']) + norm.pdf(_x, loc=_params['p_mu_2']))
    q_x = 0.5 * (norm.pdf(_x, loc=_params['q_mu_1']) + norm.pdf(_x, loc=_params['q_mu_2']))
    return p_x - q_x


def solve_fx(_f, x0, _params):
	&quot;&quot;&quot;
		Find the zero crossing
	&quot;&quot;&quot;
    _res = root(_f, x0, _params)
    return float(_res.x)

def get_optimal_bounds(params):
	&quot;&quot;&quot;
		computes the optimal bounds
	&quot;&quot;&quot;
    sorted_param_names = sorted(params, key=params.get)

    x = np.linspace(params[sorted_param_names[0]], params[sorted_param_names[-1]], 1000)
    y = f(x, params)

    crosses = np.where(np.diff(np.sign(y[y != 0])))[0]
    positives = np.where(y &amp;gt;= 0)[0]

    x0s = x[crosses]
    res = root(f, x0s, params)

    vals = list(res.x)

    if len(vals) == 1:
        if y[crosses[0]] &amp;gt; 0:
            l_1 = - np.float(&quot;inf&quot;)
            r_1 = vals[0]
            l_2 = r_1
            r_2 = l_2
        else:
            l_1 = vals[0]
            r_1 = vals[0]
            l_2 = vals[0]
            r_2 = np.float(&quot;inf&quot;)
    elif len(vals) == 2:
        if y[crosses[0]] &amp;gt; 0:
            l_1 = - np.float(&quot;inf&quot;)
            r_1 = vals[0]
            l_2 = vals[1]
            r_2 = np.float(&quot;inf&quot;)
        else:
            l_1 = vals[0]
            r_1 = 0.5 * (vals[0] + vals[1])
            l_2 = r_1
            r_2 = vals[1]
    elif len(vals) == 3:
        if y[crosses[0]] &amp;gt; 0:
            l_1 = - np.float(&quot;inf&quot;)
            r_1 = vals[0]
            l_2 = vals[1]
            r_2 = vals[2]
        else:
            l_1 = vals[0]
            r_1 = vals[1]
            l_2 = vals[2]
            r_2 = np.float(&quot;inf&quot;)
    elif len(vals) == 0:
        # any arbitrary value
        l_1, r_1, l_2, r_2 = -10., 0., 0., 10.
    else:
        raise Exception(&quot;There should be at most 3 crossings!&quot;)

    return l_1, r_1, l_2, r_2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s visulaize an arbitrary set of intervals, note here we have two mixture of 2 Gaussian, that is there exist at most 3 zero crossings.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# viz intervals for D(x) =  1{-3&amp;lt;=x&amp;lt;= -1} | 1{1&amp;lt;=x&amp;lt;=3}
plot_bounds([-3, -1, 1, 3],
            {'p_mu_1': -2, 'p_mu_2': 2, 'q_mu_1': 1, 'q_mu_2': 5})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/optdiscr/discr.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;The bounds are covering regions where $p(x) &amp;lt; q(x)$, this should not be the case for an optimal discriminator $D^*(x)$ as shown below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# viz opt disc bounds D^*(x)
params = {'p_mu_1': -2, 'p_mu_2': 2, 'q_mu_1': 1, 'q_mu_2': 5}
plot_opt_disc(params)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/optdiscr/optdiscr.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">Let $p$ and $q$ be two mixtures of $k$ univariate Gaussians as follows: We are interested in a discriminator $D$ that distinguishes between the above distributions. Let’s assess $D$’s performance with the WGAN objective That is, the greater $L$ is, the better our discriminator $D$ is. Let the output range of $D$ be $[0,1]$. $L(D)$ can be written as Since $D(x)\geq 0$, $L(D)$ is maximized with $D(x)=0$ for $p(x)&amp;lt;q(x)$ and $D(x)=1$ for $p(x)\geq q(x)$. With this observation at hand, one can think of the discriminator as an indicator function, with the optimal one being $D^*(x)=\mathbb{1}{p(x)\geq q(x)}$. For univariate Gaussians, $D^*(x)=1$ on interavls where $p(x)\geq q(x)$ and $0$ otherwise. To find these intervals, one can look for the zero-crossings of the function $p(x)-q(x)$. From Theorem A.2, the linear combination $p(x)-q(x)$ has at most $2k-1$ zero-crossings. With $k=2$, we have 3 zero-crossings, which we can find with a root-finding solver given good initial guesses. Subsequently, $p(x)\geq q(x)$ over at most $2k-2=2$ disjoint intervals. What’s left now is computing the lower and upper bounds of these intervals for an optimal discriminator. We will demonstrate this in the following snippets. import numpy as np import matplotlib.pyplot as plt from scipy.optimize import root from scipy.stats import norm def f(_x, *args): &quot;&quot;&quot; evaluate p(x) - q(x) It is assumed that sigma = 1 for all the Gaussians &quot;&quot;&quot; _params = args[0] p_x = 0.5 * (norm.pdf(_x, loc=_params['p_mu_1']) + norm.pdf(_x, loc=_params['p_mu_2'])) q_x = 0.5 * (norm.pdf(_x, loc=_params['q_mu_1']) + norm.pdf(_x, loc=_params['q_mu_2'])) return p_x - q_x def solve_fx(_f, x0, _params): &quot;&quot;&quot; Find the zero crossing &quot;&quot;&quot; _res = root(_f, x0, _params) return float(_res.x) def get_optimal_bounds(params): &quot;&quot;&quot; computes the optimal bounds &quot;&quot;&quot; sorted_param_names = sorted(params, key=params.get) x = np.linspace(params[sorted_param_names[0]], params[sorted_param_names[-1]], 1000) y = f(x, params) crosses = np.where(np.diff(np.sign(y[y != 0])))[0] positives = np.where(y &amp;gt;= 0)[0] x0s = x[crosses] res = root(f, x0s, params) vals = list(res.x) if len(vals) == 1: if y[crosses[0]] &amp;gt; 0: l_1 = - np.float(&quot;inf&quot;) r_1 = vals[0] l_2 = r_1 r_2 = l_2 else: l_1 = vals[0] r_1 = vals[0] l_2 = vals[0] r_2 = np.float(&quot;inf&quot;) elif len(vals) == 2: if y[crosses[0]] &amp;gt; 0: l_1 = - np.float(&quot;inf&quot;) r_1 = vals[0] l_2 = vals[1] r_2 = np.float(&quot;inf&quot;) else: l_1 = vals[0] r_1 = 0.5 * (vals[0] + vals[1]) l_2 = r_1 r_2 = vals[1] elif len(vals) == 3: if y[crosses[0]] &amp;gt; 0: l_1 = - np.float(&quot;inf&quot;) r_1 = vals[0] l_2 = vals[1] r_2 = vals[2] else: l_1 = vals[0] r_1 = vals[1] l_2 = vals[2] r_2 = np.float(&quot;inf&quot;) elif len(vals) == 0: # any arbitrary value l_1, r_1, l_2, r_2 = -10., 0., 0., 10. else: raise Exception(&quot;There should be at most 3 crossings!&quot;) return l_1, r_1, l_2, r_2 Let’s visulaize an arbitrary set of intervals, note here we have two mixture of 2 Gaussian, that is there exist at most 3 zero crossings. # viz intervals for D(x) = 1{-3&amp;lt;=x&amp;lt;= -1} | 1{1&amp;lt;=x&amp;lt;=3} plot_bounds([-3, -1, 1, 3], {'p_mu_1': -2, 'p_mu_2': 2, 'q_mu_1': 1, 'q_mu_2': 5}) The bounds are covering regions where $p(x) &amp;lt; q(x)$, this should not be the case for an optimal discriminator $D^*(x)$ as shown below. # viz opt disc bounds D^*(x) params = {'p_mu_1': -2, 'p_mu_2': 2, 'q_mu_1': 1, 'q_mu_2': 5} plot_opt_disc(params)</summary></entry><entry><title type="html">On DeepMind’s Population-Based Training</title><link href="http://ash-aldujaili.github.io/blog/2018/03/15/pbt/" rel="alternate" type="text/html" title="On DeepMind's Population-Based Training" /><published>2018-03-15T00:00:00-08:00</published><updated>2018-03-15T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/03/15/pbt</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/03/15/pbt/">&lt;p&gt;DeepMind has recently released a &lt;a href=&quot;https://arxiv.org/abs/1711.09846&quot;&gt;paper&lt;/a&gt; about population-based training of neural nets. The proposition is a simple asynchronous optimization algorithm, which jointly optimizes a population of models and their hyperparameters. In this post, I aim to demonstrate a simple implementation of the algorithm by reproducing the paper’s Figure 2.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/pbt/pbt-paper-demo.png&quot; width=&quot;750&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;The objective is to maximize the function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\theta) = 1.2 - (\theta^2_0 + \theta^2_1)&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def q(theta):
    return 1.2 - np.sum(np.array(theta)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Think of the above function as the validation set performance whose closed-form expression is unknown. However, we can still query its value $($ evaluate $)$ at specific $\theta$. While our goal is to maximize it and achieve a model with good generalization capabilities, we optimize our neural model $($ tune its parameters $)$ with respect to the training set performance. That is, our model is optimized w.r.t. to a surrogate objective $\hat{Q}$ function rather than the objective function we are after.&lt;/p&gt;

&lt;p&gt;In an ideal setup, one would like $\hat{Q}=Q$ $($ or at least optimizing $\hat{Q}$ should correspond to optimizing $Q$ as well$)$, but this is not the case in general. The quality of this approximation $($or coupled optimization $)$  is often dependent on the model at hand and how we are training it: the hyperparameters. In other words, when we optimize $\hat{Q}$ w.r.t. $\theta$, we are doing so given $($ conditioned on $)$ a set of hyperparameters, $\hat{Q}(\theta\mid h)$.&lt;/p&gt;

&lt;p&gt;In the considered toy example, let the mpirical loss / training set performance by parameterized by $h$ as follows.
&lt;script type=&quot;math/tex&quot;&gt;\hat{Q}(\theta|h) = 1.2 - (h_0\theta^2_0 + h_1\theta^2_1)&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def q_hat(theta, h):
    # this is used by `step` function to perform
    # gradient updates 
    return 1.2 - np.sum(np.dot(np.array(h), np.array(theta)**2))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The task now boils down to using $\hat{Q}$ to tune $\theta$ while searching for $h$ to make the tuning process as efficient as possible in $($ indirectly $)$ optimizing $Q$. PBT is one solution to do so. Here is the pesudocode, and below is an implementation of the same.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/pbt/pbt-alg.png&quot; width=&quot;750&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;Let’s start with modeling the population members:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Member(object):
    def __init__(self, theta=[0., 0.], h=[0., 0.], _id=1, _eta=0.01, _sigma=1e-1):
        self.theta = theta
        self.h = h
        self.id = _id
        self.num_steps = 0
        self.p = q(self.theta)
        self._eta = _eta
        self._sigma = _sigma
        ## for visualization
        self.trace = []
        self.ps = [self.p]

    def eval(self):
        self.p = q(self.theta)

    def ready(self):
        return self.num_steps &amp;gt; 10

    def step(self):
        for i in range(2):
            self.theta[i] +=  self._eta * (-2. * self.h[i] * self.theta[i])
        self.num_steps += 1

    def explore(self):
        for i in range(2):
            self.h[i] = np.clip(self.h[i] + self._sigma * np.random.randn(), 0, 1)

    def exploit(self, other):
        if self.p &amp;lt;= other.p:
            self.theta = list(other.theta)
            self.p = other.p
            self.num_steps = 0
            return True
        else:
            return False

    def log(self):
        self.trace.append(list(self.theta))
        self.ps.append(self.p)

    def __str__(self):
        return 'theta:' + '%.2f' % self.theta[0] + ',%.2f' % self.theta[1] + \
               '| h:' + '%.2f' % self.h[0] + ',%.2f' % self.h[1] + '| id' + str(self.id) + \
               '| p:' + '%.2f' % self.p + '| steps:' + str(self.num_steps)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here’s PBT implementation. I’ve put flags to show PBT in explore, exploit, grid search modes.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def pbt(grid=False, explore_only=False, exploit_only=False):
    # a check to ensure only one mode is selected for PBT
    assert grid + exploit_only + explore_only &amp;lt;= 1, &quot;at most one flag can be set for PBT modes&quot;
    # init population
    population = [
        Member(theta=[0.9, 0.9], h=[0,1], _id=0),
        Member(theta=[0.9, 0.9], h=[1,0], _id=1)
        ]
    member_ids = np.arange(len(population))
    # begin training
    for _ in range(75):
        np.random.shuffle(member_ids)
        for mem_id in member_ids:
            member = population[mem_id]
            member.step()
            member.eval()
            if member.ready() and not grid:
                if explore_only:
                    member.explore()
                else:
                    member.exploit(population[(mem_id + 1) % 2])
                    if not exploit_only:
                        member.explore()
                    member.eval()
            member.log()

    traces = [population[i].trace for i in range(len(population))]
    ps = [population[i].ps for i in range(len(population))]
    return traces, ps
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s plot the four modes&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;f, axes = plt.subplots(2, 4)#, sharex=True)

traces, ps = pbt(grid=True)
plot_traces(axes[0,0], traces, title='Grid Search')
plot_curves(axes[0,2], ps, title='Grid Serach')

traces, ps = pbt(exploit_only=True)
plot_traces(axes[0,1], traces, title='Exploit Only')
plot_curves(axes[0,3], ps, title='Exploit Only')

traces, ps = pbt(explore_only=True)
plot_traces(axes[1,0], traces, title='Explore Only')
plot_curves(axes[1,2], ps, title='Explore Only')

traces, ps = pbt()
plot_traces(axes[1,1], traces, title='PBT')
plot_curves(axes[1,3], ps, title='PBT')

plt.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which results in the following:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/pbt/pbt-demo.png&quot; width=&quot;850&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">DeepMind has recently released a paper about population-based training of neural nets. The proposition is a simple asynchronous optimization algorithm, which jointly optimizes a population of models and their hyperparameters. In this post, I aim to demonstrate a simple implementation of the algorithm by reproducing the paper’s Figure 2. The objective is to maximize the function def q(theta): return 1.2 - np.sum(np.array(theta)**2) Think of the above function as the validation set performance whose closed-form expression is unknown. However, we can still query its value $($ evaluate $)$ at specific $\theta$. While our goal is to maximize it and achieve a model with good generalization capabilities, we optimize our neural model $($ tune its parameters $)$ with respect to the training set performance. That is, our model is optimized w.r.t. to a surrogate objective $\hat{Q}$ function rather than the objective function we are after. In an ideal setup, one would like $\hat{Q}=Q$ $($ or at least optimizing $\hat{Q}$ should correspond to optimizing $Q$ as well$)$, but this is not the case in general. The quality of this approximation $($or coupled optimization $)$ is often dependent on the model at hand and how we are training it: the hyperparameters. In other words, when we optimize $\hat{Q}$ w.r.t. $\theta$, we are doing so given $($ conditioned on $)$ a set of hyperparameters, $\hat{Q}(\theta\mid h)$. In the considered toy example, let the mpirical loss / training set performance by parameterized by $h$ as follows. def q_hat(theta, h): # this is used by `step` function to perform # gradient updates return 1.2 - np.sum(np.dot(np.array(h), np.array(theta)**2)) The task now boils down to using $\hat{Q}$ to tune $\theta$ while searching for $h$ to make the tuning process as efficient as possible in $($ indirectly $)$ optimizing $Q$. PBT is one solution to do so. Here is the pesudocode, and below is an implementation of the same. Let’s start with modeling the population members: class Member(object): def __init__(self, theta=[0., 0.], h=[0., 0.], _id=1, _eta=0.01, _sigma=1e-1): self.theta = theta self.h = h self.id = _id self.num_steps = 0 self.p = q(self.theta) self._eta = _eta self._sigma = _sigma ## for visualization self.trace = [] self.ps = [self.p] def eval(self): self.p = q(self.theta) def ready(self): return self.num_steps &amp;gt; 10 def step(self): for i in range(2): self.theta[i] += self._eta * (-2. * self.h[i] * self.theta[i]) self.num_steps += 1 def explore(self): for i in range(2): self.h[i] = np.clip(self.h[i] + self._sigma * np.random.randn(), 0, 1) def exploit(self, other): if self.p &amp;lt;= other.p: self.theta = list(other.theta) self.p = other.p self.num_steps = 0 return True else: return False def log(self): self.trace.append(list(self.theta)) self.ps.append(self.p) def __str__(self): return 'theta:' + '%.2f' % self.theta[0] + ',%.2f' % self.theta[1] + \ '| h:' + '%.2f' % self.h[0] + ',%.2f' % self.h[1] + '| id' + str(self.id) + \ '| p:' + '%.2f' % self.p + '| steps:' + str(self.num_steps) Here’s PBT implementation. I’ve put flags to show PBT in explore, exploit, grid search modes. def pbt(grid=False, explore_only=False, exploit_only=False): # a check to ensure only one mode is selected for PBT assert grid + exploit_only + explore_only &amp;lt;= 1, &quot;at most one flag can be set for PBT modes&quot; # init population population = [ Member(theta=[0.9, 0.9], h=[0,1], _id=0), Member(theta=[0.9, 0.9], h=[1,0], _id=1) ] member_ids = np.arange(len(population)) # begin training for _ in range(75): np.random.shuffle(member_ids) for mem_id in member_ids: member = population[mem_id] member.step() member.eval() if member.ready() and not grid: if explore_only: member.explore() else: member.exploit(population[(mem_id + 1) % 2]) if not exploit_only: member.explore() member.eval() member.log() traces = [population[i].trace for i in range(len(population))] ps = [population[i].ps for i in range(len(population))] return traces, ps Let’s plot the four modes f, axes = plt.subplots(2, 4)#, sharex=True) traces, ps = pbt(grid=True) plot_traces(axes[0,0], traces, title='Grid Search') plot_curves(axes[0,2], ps, title='Grid Serach') traces, ps = pbt(exploit_only=True) plot_traces(axes[0,1], traces, title='Exploit Only') plot_curves(axes[0,3], ps, title='Exploit Only') traces, ps = pbt(explore_only=True) plot_traces(axes[1,0], traces, title='Explore Only') plot_curves(axes[1,2], ps, title='Explore Only') traces, ps = pbt() plot_traces(axes[1,1], traces, title='PBT') plot_curves(axes[1,3], ps, title='PBT') plt.tight_layout() plt.show() which results in the following:</summary></entry><entry><title type="html">Expected Improvement for Bayesian Optimization: A Derivation</title><link href="http://ash-aldujaili.github.io/blog/2018/02/01/ei/" rel="alternate" type="text/html" title="Expected Improvement for Bayesian Optimization: A Derivation" /><published>2018-02-01T00:00:00-08:00</published><updated>2018-02-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/02/01/ei</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/02/01/ei/">&lt;p&gt;In this post, we derive the closed-form expression of the Expected Improvement $($EI$)$ criterion commonly used in Bayesian Optimization.&lt;/p&gt;

&lt;p&gt;Modelled with a Gaussian Process, the function value at a given point $x$ can be considered as a normal random variable with mean $\mu$ and variance $\sigma^2$. Given the best $($minimum in a minimization setup$)$ function value obtained so far-let’s denote it by 
$f^*$:&lt;/p&gt;

&lt;p&gt;we are interested in quantifying the improvement over 
$f^*$ 
we will have 
if we sample a point 
$x$ 
. Mathematically, the improvement at 
$x$ 
can be expressed as follows&lt;/p&gt;

&lt;p&gt;$I(x) = \max(f^* - Y,0)$&lt;/p&gt;

&lt;p&gt;where $Y$ is the random variable $\sim \mathcal{N}(\mu, \sigma^2)$ that corresponds to the function value at $x$. Since $I$ is a random variable, one can consider the average $($expected$)$ improvement $($EI$)$ to assess $x$:&lt;/p&gt;

&lt;p&gt;$EI(x) =  E_{Y\sim \mathcal{N}(\mu, \sigma^2)}[I(x)]$&lt;/p&gt;

&lt;p&gt;With the reparameterization trick, $Y=\mu + \sigma \epsilon$ where 
$\epsilon\sim\mathcal{N}(0,1)$, we have:&lt;/p&gt;

&lt;p&gt;$EI(x) =  E_{\epsilon\sim \mathcal{N}(0,1)}[I(x)]$&lt;/p&gt;

&lt;p&gt;which can be written as $($from linearity of integral, and the definition of  $\frac{d}{d\epsilon}e^{-\epsilon^2 / 2}$ derivative $)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x) = \int_{-\infty}^{\infty} I(x) \phi(\epsilon) d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x) =  \int_{-\infty}^{(f^*-\mu)/\sigma} (f^* - \mu - \sigma \epsilon) \phi(\epsilon) d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)= (f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) - \sigma \int_{-\infty}^{(f^*-\mu)/\sigma} \epsilon \phi(\epsilon) d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \frac{\sigma}{
\sqrt{2\pi}} \int_{-\infty}^{(f^*-\mu)/\sigma} (-\epsilon) e^{-\epsilon^2/2} d\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \frac{\sigma}{
\sqrt{2\pi}}  e^{-\epsilon^2/2}|_{-\infty}^{(f^*-\mu)/\sigma}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \sigma
  \big(\phi(\frac{f^*-\mu}{\sigma}) - 0\big)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EI(x)=(f^* - \mu)\Phi(\frac{f^*-\mu}{\sigma}) + \sigma
  \phi(\frac{f^*-\mu}{\sigma})&lt;/script&gt;

&lt;p&gt;where $\phi, \Phi$ are the PDF, CDF of standard normal distribution, respectively.&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we derive the closed-form expression of the Expected Improvement $($EI$)$ criterion commonly used in Bayesian Optimization. Modelled with a Gaussian Process, the function value at a given point $x$ can be considered as a normal random variable with mean $\mu$ and variance $\sigma^2$. Given the best $($minimum in a minimization setup$)$ function value obtained so far-let’s denote it by $f^*$: we are interested in quantifying the improvement over $f^*$ we will have if we sample a point $x$ . Mathematically, the improvement at $x$ can be expressed as follows $I(x) = \max(f^* - Y,0)$ where $Y$ is the random variable $\sim \mathcal{N}(\mu, \sigma^2)$ that corresponds to the function value at $x$. Since $I$ is a random variable, one can consider the average $($expected$)$ improvement $($EI$)$ to assess $x$: $EI(x) = E_{Y\sim \mathcal{N}(\mu, \sigma^2)}[I(x)]$ With the reparameterization trick, $Y=\mu + \sigma \epsilon$ where $\epsilon\sim\mathcal{N}(0,1)$, we have: $EI(x) = E_{\epsilon\sim \mathcal{N}(0,1)}[I(x)]$ which can be written as $($from linearity of integral, and the definition of $\frac{d}{d\epsilon}e^{-\epsilon^2 / 2}$ derivative $)$ where $\phi, \Phi$ are the PDF, CDF of standard normal distribution, respectively.</summary></entry><entry><title type="html">Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts</title><link href="http://ash-aldujaili.github.io/blog/2018/01/08/wma/" rel="alternate" type="text/html" title="Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts" /><published>2018-01-08T00:00:00-08:00</published><updated>2018-01-08T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/01/08/wma</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/01/08/wma/">&lt;p&gt;In this post, we demonstrate the theoretical gurantee on the performance of the &lt;strong&gt;Weighted Majority Alogorithm&lt;/strong&gt; (&lt;em&gt;WMA&lt;/em&gt;) with empirical validation. WMA is one of the beautiful algorithms in the framework of online learning and sequential decision making under uncertainity.&lt;/p&gt;

&lt;p&gt;First, let’s describe a setup where WMA can be used. Assume that a friend of yours challenges to a &lt;strong&gt;True or False&lt;/strong&gt; quiz where you’re allowed to seek help / advice from $n$ advisors throughout the game.&lt;/p&gt;

&lt;p&gt;With this challenge at hand, WMA helps you play the game such that the number of your mistakes is upper bounded by roughly twice the number of mistakes made by your best advisor, i.e., the least number of mistakes made among your $n$ advisors. In the next paragraph, we’ll see how you can use WMA in the game.&lt;/p&gt;

&lt;p&gt;For each advisor $i$ and question (round) $t$, WMA associates a weight $w^t_i$. With $w^1_i=1\;, \forall i=1,\ldots, n.$ Assume the game has $T$ rounds. If advisor $i$ makes a mistake at round $t$, its weight is adjusted by a multiplicative factor $1-\eta$ where $\eta\in(0,1/2)$. That is, $w^{t+1}_i= (1-\eta) w^t_i$ if $i$ makes a mistake. Otherwise, $w^{t+1}_i=w^t_i$. At each round $t$, your decision is governed by the sum of weights of advisors who think the answer is &lt;strong&gt;True&lt;/strong&gt; versus that of those who think the answer should be &lt;strong&gt;False&lt;/strong&gt;. Mathematically, denote your answer at round $t$ by $y^t$, and the advisors’ answers by $x^t_i$. These variables take the values $0$ and $1$ to represent &lt;em&gt;True&lt;/em&gt; and &lt;em&gt;False&lt;/em&gt; respectively: we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^t = \mathbf{1}\big\{\sum_{i}x^t_i * w^t_i \geq \sum_{i} (1-x^t_i) * w^t_i \big\}&lt;/script&gt;

&lt;p&gt;Let’s put WMA in action. Below, I have created $n$ advisors whose decisions at each round are based on tossing coins whose bias is sampled uniformly, $p_i\sim \mathcal{U}(0,1)$. Likewise, the game generates questions whose answers follows a Bernoulli distribution with $p_g\sim\mathcal{U}(0,1)$. We denote the correct answer at round $t$ by $z^t$. Also, let’s try to validate the theoretical bound on its performance. From &lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall13/cos521/lecnotes/lec8.pdf&quot;&gt;this&lt;/a&gt;, after $\hat{t}$ rounds,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|\text{your mistakes}| \leq 2 (1+ \eta)\;|\text{best_advisors_mistakes}|\; + \frac{2 \ln n}{\eta}&lt;/script&gt;

&lt;p&gt;That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2(1+\eta) \min(\sum_{t=0}^{\hat{t}}\mathbf{1}\{x^t_1 \neq z^t\}, \ldots, \sum_{t=0}^{\hat{t}}\mathbf{1}\{x^t_n \neq z^t\}) + \frac{2\ln n}{\eta}-  \sum_{t=0}^{\hat{t}} \mathbf{1}\{y^t\neq z^t\} \geq 0\;, \hat{t}= 1,\ldots, T&lt;/script&gt;

&lt;p&gt;Let’s verify the above with the following 100 games with 1000 rounds each.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np

# let's try 100 games each with 1000 rounds,  
num_runs = 100
T = 1000

diff = np.zeros((num_runs, T))
for run in range(num_runs):
    n = np.random.randint(1,10)
    ps = np.random.random(n)
    ws = np.ones(n)
    p_g = np.random.random()
    eta = 0.25

    # your mistakes
    m = 0
    # advisors mistakes
    ms = np.zeros(n, dtype=np.int32)

    for t in range(T):
        # game correct answer
        z = np.random.random() &amp;gt;= p_g
        # advisors answers
        xs = np.random.random(n) &amp;gt;= ps
        # your answer
        y = np.sum(xs * ws) &amp;gt;= np.sum((1-xs) * ws)

        # your mistakes
        m += (y != z)
        # advistors mistakes
        ms[xs != z] += 1

        # theoretical bound
        th_ub = 2 * (1+eta) * min(ms) + 2. * np.log(n) / eta
        diff[run, t] = th_ub - m
        
        # update weights
        ws[xs != z] = (1-eta) * ws[xs != z]


assert np.min(diff) &amp;gt;= 0, &quot;Theoretical gap should be non-negative&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Our mistakes never exceeded the theoretical upper limit, as can be seen from the non-negative theoretical gap across all the runs and rounds of the game.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import matplotlib.pyplot as plt

_ = plt.plot(diff.T)
plt.xlabel('round')
plt.ylabel('Theoretical Gap')
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/wma/wma.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we demonstrate the theoretical gurantee on the performance of the Weighted Majority Alogorithm (WMA) with empirical validation. WMA is one of the beautiful algorithms in the framework of online learning and sequential decision making under uncertainity. First, let’s describe a setup where WMA can be used. Assume that a friend of yours challenges to a True or False quiz where you’re allowed to seek help / advice from $n$ advisors throughout the game. With this challenge at hand, WMA helps you play the game such that the number of your mistakes is upper bounded by roughly twice the number of mistakes made by your best advisor, i.e., the least number of mistakes made among your $n$ advisors. In the next paragraph, we’ll see how you can use WMA in the game. For each advisor $i$ and question (round) $t$, WMA associates a weight $w^t_i$. With $w^1_i=1\;, \forall i=1,\ldots, n.$ Assume the game has $T$ rounds. If advisor $i$ makes a mistake at round $t$, its weight is adjusted by a multiplicative factor $1-\eta$ where $\eta\in(0,1/2)$. That is, $w^{t+1}_i= (1-\eta) w^t_i$ if $i$ makes a mistake. Otherwise, $w^{t+1}_i=w^t_i$. At each round $t$, your decision is governed by the sum of weights of advisors who think the answer is True versus that of those who think the answer should be False. Mathematically, denote your answer at round $t$ by $y^t$, and the advisors’ answers by $x^t_i$. These variables take the values $0$ and $1$ to represent True and False respectively: we have Let’s put WMA in action. Below, I have created $n$ advisors whose decisions at each round are based on tossing coins whose bias is sampled uniformly, $p_i\sim \mathcal{U}(0,1)$. Likewise, the game generates questions whose answers follows a Bernoulli distribution with $p_g\sim\mathcal{U}(0,1)$. We denote the correct answer at round $t$ by $z^t$. Also, let’s try to validate the theoretical bound on its performance. From this, after $\hat{t}$ rounds, That is, Let’s verify the above with the following 100 games with 1000 rounds each. import numpy as np # let's try 100 games each with 1000 rounds, num_runs = 100 T = 1000 diff = np.zeros((num_runs, T)) for run in range(num_runs): n = np.random.randint(1,10) ps = np.random.random(n) ws = np.ones(n) p_g = np.random.random() eta = 0.25 # your mistakes m = 0 # advisors mistakes ms = np.zeros(n, dtype=np.int32) for t in range(T): # game correct answer z = np.random.random() &amp;gt;= p_g # advisors answers xs = np.random.random(n) &amp;gt;= ps # your answer y = np.sum(xs * ws) &amp;gt;= np.sum((1-xs) * ws) # your mistakes m += (y != z) # advistors mistakes ms[xs != z] += 1 # theoretical bound th_ub = 2 * (1+eta) * min(ms) + 2. * np.log(n) / eta diff[run, t] = th_ub - m # update weights ws[xs != z] = (1-eta) * ws[xs != z] assert np.min(diff) &amp;gt;= 0, &quot;Theoretical gap should be non-negative&quot; Our mistakes never exceeded the theoretical upper limit, as can be seen from the non-negative theoretical gap across all the runs and rounds of the game. import matplotlib.pyplot as plt _ = plt.plot(diff.T) plt.xlabel('round') plt.ylabel('Theoretical Gap') plt.show()</summary></entry><entry><title type="html">Non-Smooth Analysis for Convergence of Derivative-Free Optimization</title><link href="http://ash-aldujaili.github.io/blog/2018/01/01/non-smooth-analysis/" rel="alternate" type="text/html" title="Non-Smooth Analysis for Convergence of Derivative-Free Optimization" /><published>2018-01-01T00:00:00-08:00</published><updated>2018-01-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2018/01/01/non-smooth-analysis</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2018/01/01/non-smooth-analysis/">&lt;p&gt;This post presents a concise and short tutorial about non-smooth
analysis that can be applied to study the convergence of derivative-free
optimization algorithms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: This post has not gone through a rigorous review. It
is mainly a documentation of observations about this tool, i.e.,
non-smooth analysis.&lt;/p&gt;

&lt;iframe src=&quot;https://drive.google.com/file/d/1PvRODT6BVcAZE14-viufc8PkOpZzECyZ/preview&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;/iframe&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">This post presents a concise and short tutorial about non-smooth analysis that can be applied to study the convergence of derivative-free optimization algorithms. Disclaimer: This post has not gone through a rigorous review. It is mainly a documentation of observations about this tool, i.e., non-smooth analysis.</summary></entry><entry><title type="html">A Gentle Tour through Sampling Techniques</title><link href="http://ash-aldujaili.github.io/blog/2017/12/26/sampling/" rel="alternate" type="text/html" title="A Gentle Tour through Sampling Techniques" /><published>2017-12-26T00:00:00-08:00</published><updated>2017-12-26T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/12/26/sampling</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/12/26/sampling/">&lt;p&gt;In this post, we’ll take a quick tour into the sampling world and how it might be handy in some applications. Let’s start with some setup code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import numpy.random as rnd
import seaborn as sns
import scipy
from mpl_toolkits.mplot3d import Axes3D
%pylab inline
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Populating the interactive namespace from numpy and matplotlib
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4&gt;Sampling from an aribitrary 2-D distribution&lt;/h4&gt;

&lt;p&gt;Let $x\in \mathbb{R}^2$ be a continuous random variable with from a probability distribution $q(x)$. We are interested in obtaining values that $x$ can take in a way that confirms to its probability distribution $q(x)$. Let’s assume $q(x)$ has the following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x\sim q(x)= \frac{1}{Z} [(x_1 - 0.5)^2 + (x_2 - 0.5)^2]\;,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{Z} = \int^{1}_{0} (x_1 - 0.5)^2 dx_1+ \int^{1}_{0} (x_2 - 0.5)^2 dx_2 = \frac{1}{6}&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s have a look at how does this distribution look like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n = 100
x_1s, x_2s = np.meshgrid(*[np.linspace(0,1, n)]*2)
@np.vectorize
def compute_q(x_1,x_2):
    return q([x_1,x_2])
q_s = compute_q(x_1s, x_2s)
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_trisurf(x_1s.ravel(), x_2s.ravel(), q_s.ravel(), 
                       cmap=plt.cm.BrBG_r, linewidth=0.2)
fig.colorbar(surf, shrink=0.5, aspect=5)
ax.view_init(30, 45)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_5_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;The density accumulates at the boundaries of the distribution’s support. So when we sample, we expect more values around the boundaries rather than the center. The question is how to make our sampling process conforms to the same. In the rest of this post, we will discuss three methods: Rejection Sampling, Gibbs Sampling, Metropolis Hastings.&lt;/p&gt;

&lt;h3&gt;Rejection Sampling:&lt;/h3&gt;

&lt;p&gt;The gist of this method is to find a distribution $p(x)$ from which we know how to sample. We scale $p(x)$ by a factor $C$ such that 
&lt;script type=&quot;math/tex&quot;&gt;q(x) \leq C p(x)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If the above can be obtained, then our sampling procedure is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sample $\bar{x}\sim p(\bar{x})$.&lt;/li&gt;
  &lt;li&gt;Toss a biased coin to accept $\bar{x}$ as a sample coming from $q$. The coin bias is $\frac{q(\bar{x})}{Cp(\bar{x})}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Intuitvely, you can think of this procedure as a correction scheme to account for the mismatch: how accurately $p(x)$ describes $q(x)$. Put it differently, the area under curve $Cp(x)$ is $C$, while under $q(x)$, it is $1$. And since $q(x)\leq Cp(x)$,  if we full the area under $Cp(x)$ with points uniformly, then $1/C$ of these points will be under $q(x)$.&lt;/p&gt;

&lt;p&gt;In the demo below, we choose $p$ to be the uniform distribution:
&lt;script type=&quot;math/tex&quot;&gt;x_1 \sim \mathcal{U}(0,1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;x_2 \sim \mathcal{U}(0,1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;p(x) = 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And $C$ to be $\max_{x} q(x)= 0.0833333$&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;C = q([0.,0.]) #max_x q(x)
p = lambda x: 1
samples = []
num_samples = 10000
while len(samples) &amp;lt; num_samples:
    x = rnd.rand(2)
    if rnd.rand() &amp;lt;= ( q(x) / (C * p(x))):
        samples.append({'x_1':x[0], 'x_2':x[1]})
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples)
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_9_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;We can see how the histogram of the collected samples agrees to the mass distribution of $q$. However, this approach becomes challenging in higher dimensions: higher rejection probability $($which makes the process extremely slow$)$ and less known upper-bounding scaled distributions. For this, Markov Chain Monte Carlo $($MCMC$)$ methods come to the rescue&lt;/p&gt;

&lt;h3&gt;Markov Chain Monte Carlo&lt;/h3&gt;

&lt;p&gt;Based on the notion of stationary distribution,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi(\bar{x})=\sum_{x}\pi(x)T(\bar{x}|x)&lt;/script&gt;
we can generate our samples from a dynamical systems over the states $($values$)$ which the random variable $x$ can take. That is, i. Specify a transition probability in line with the system dynamics $T(\bar{x}|x)$ $($or proposal distribution- to propose the next state $\bar{x}$ given the present state $x$ $)$. ii. Let the system interact, iii. The system will converge to the underlying $($unique$)$ distribution of $\pi$. Note that, if:
&lt;script type=&quot;math/tex&quot;&gt;T(\bar{x}|x)&gt;0\;, \forall x, \bar{x}\;,&lt;/script&gt;
there exists a unique stationary $\pi$&lt;/p&gt;

&lt;p&gt;The challenge in this setup is how to define / sample from the valid transition distribution $($e.g., in higher dimensions$)$. Among other approaches we discuss two here.&lt;/p&gt;

&lt;h4&gt;Gibbs Sampling&lt;/h4&gt;

&lt;p&gt;Given a random $($initialized$)$ state of our systems variables $($here, $x_1$ and $x_2$$)$, one can condition one variable on the other to obtain a 1-D distriubution, which we assume we can sample from $($i.e., rejection sampling, central limit theorem, or other techniques$)$. Iteratively, we will obtain samples. Neverthelss, we need to give it some time till we converge to the stationary distribution of our system. In our example, we will make use of rejection sampling in 1-D to obtain our 1-D sample.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6.
# q(x_1|x_2)
q_1 = lambda x: 6 * q(x)/((x[1]-0.5)**2 + 2. * 0.5**3 / 3.) 
# q(x_2|x_1)
q_2 = lambda x: 6 * q(x)/((x[0]-0.5)**2 + 2. * 0.5**3 / 3.) 
# p (upper bounding), here we are using uniform dist
p = lambda x: 1 

def reject_uniform_sample(q, C):
    while True:
        x_ = rnd.rand()
        if rnd.rand() &amp;lt;= q(x_) /(C * 1.):
            return x_

# aribtrary value, by right we should use max_x q_1, max_x q_2
C_1 = 1.5
C_2 = 1.5
samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}]
num_samples = 10000
while len(samples) &amp;lt; num_samples:
    x_1 = reject_uniform_sample(lambda x_: q_1([x_, samples[-1]['x_2']]), C_1)
    x_2 = reject_uniform_sample(lambda x_: q_2([samples[-1]['x_1'], x_]), C_2)
    samples.append({'x_1':x_1, 'x_2':x_2})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;While burning:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[:50])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_16_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;After discarding some samples away:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[500:])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_18_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;h4&gt;Metropolis-Hastings Sampling&lt;/h4&gt;

&lt;p&gt;In Gibbs sampling, the idea is to generate samples for each of our system’s variables from their posterior distribution with respect to the current values of the rest of the variables. Multi-dimensioanly sampling is reduced into a sequence of 1-D sampling procedures. We saw above, this approach might be a little bit slow and generates highly correlated samples $($again: we are generating samples that are highly probable given the past values of the rest of the variables$)$. Furthermore, what if we can’t compute nor sample from the posterior distribution. Can we propose transitions from one state to another arbitrarily and sitll converge? The idea of MH-sampling is to incorporate rejection sampling with aribtrary proposal distribution such that the transition probability $($which uniquely defines the Markov process$)$ can be constructed.&lt;/p&gt;

&lt;p&gt;Let’s step back and summarise the above:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We would like to sample from our system’s distribution &lt;script type=&quot;math/tex&quot;&gt;q( x )&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We are using Markov Chain $($process$)$ to simulate our system of random variables. That is the Markov chain asymptotically converges to a stationary distribution &lt;script type=&quot;math/tex&quot;&gt;\pi(x)&lt;/script&gt; and which is &lt;script type=&quot;math/tex&quot;&gt;q(x)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;To converge to $ q(x) $, we need to construct the corresponding chain’s transition probability $ T(\bar{x}|x) $
 because it uniquely defines the process and hence its stationary distribution.&lt;/li&gt;
  &lt;li&gt;One way of achieving that is through the detailed balance, sufficient but not a necessary condition:
$ q(x) T(\bar{x}|x) = q(\bar{x}) T(x|\bar{x}) $&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can make use of the above formula to construct a transition probability  that is a product of an arbitrary distribution 
$g(\bar{x}|x)$ 
, which is referred to as the proposal distribution as mentioned above, and the acceptance distribution 
$A(\bar{x}|x)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(\bar{x}|x)= g(\bar{x}|x)A(\bar{x}|x)&lt;/script&gt;

&lt;p&gt;Plugging this in the detailed balance condition, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A(\bar{x}|x)=\min\{1, \frac{q(\bar{x})g(x|\bar{x})}{q(x)g(\bar{x}|x)}\}&lt;/script&gt;

&lt;p&gt;Let’s see how we can realize this in code: Let’s define our&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(\bar{x}|x)\sim \mathcal{N}(x,0.25I)&lt;/script&gt;

&lt;p&gt;and here we are assuming that we can sample efficiently from this 2-D Gaussian distribution $($ otherwise, we may use one of the techniques above as rejection sampling $)$. Notice here, there is nothing preventing us from sampling outside the support of $q(x)$, so we need to explicitly state that. I.e., $q(x)=0$ outside $q$’s support. Also, our choice of proposal distribution might affect the convergence progress.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# target distribution (the one we'd like the MC to converge to)
q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6.
# assuming 2-d Gaussian sampling is doable here (via function call)
sigma = 0.5
propose_x = lambda x_prev: x_prev + sigma * np.dot(rnd.randn(1,2),np.eye(2))[0,:]
# proposal distribution:
from scipy.stats import multivariate_normal
g = lambda x_new, x_prev: multivariate_normal.pdf(x_new, mean=x_prev, cov =sigma * np.eye(2))
# random initialization of the sample which we'll eventually discard
samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}]
num_samples = 10000
while len(samples) &amp;lt; num_samples:
    x_prev = np.array([samples[-1]['x_1'], samples[-1]['x_2']])
    x_new = propose_x(x_prev)
    # to discard samples out of support
    if max(x_new) &amp;gt; 1. or min(x_new) &amp;lt; 0.:
        continue
    if rnd.rand() &amp;lt;= min(1, q(x_new) * g(x_prev,x_new) / (q(x_prev) * g(x_new, x_prev))):
        samples.append({'x_1':x_new[0], 'x_2':x_new[1]})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[:50])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_21_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.DataFrame(samples[1500:])
_ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/sampling/output_22_0.png&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we’ll take a quick tour into the sampling world and how it might be handy in some applications. Let’s start with some setup code. import numpy as np import pandas as pd import numpy.random as rnd import seaborn as sns import scipy from mpl_toolkits.mplot3d import Axes3D %pylab inline Populating the interactive namespace from numpy and matplotlib Sampling from an aribitrary 2-D distribution Let $x\in \mathbb{R}^2$ be a continuous random variable with from a probability distribution $q(x)$. We are interested in obtaining values that $x$ can take in a way that confirms to its probability distribution $q(x)$. Let’s assume $q(x)$ has the following form: where q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6. Let’s have a look at how does this distribution look like n = 100 x_1s, x_2s = np.meshgrid(*[np.linspace(0,1, n)]*2) @np.vectorize def compute_q(x_1,x_2): return q([x_1,x_2]) q_s = compute_q(x_1s, x_2s) fig = plt.figure() ax = fig.gca(projection='3d') surf = ax.plot_trisurf(x_1s.ravel(), x_2s.ravel(), q_s.ravel(), cmap=plt.cm.BrBG_r, linewidth=0.2) fig.colorbar(surf, shrink=0.5, aspect=5) ax.view_init(30, 45) plt.show() The density accumulates at the boundaries of the distribution’s support. So when we sample, we expect more values around the boundaries rather than the center. The question is how to make our sampling process conforms to the same. In the rest of this post, we will discuss three methods: Rejection Sampling, Gibbs Sampling, Metropolis Hastings. Rejection Sampling: The gist of this method is to find a distribution $p(x)$ from which we know how to sample. We scale $p(x)$ by a factor $C$ such that If the above can be obtained, then our sampling procedure is as follows: Sample $\bar{x}\sim p(\bar{x})$. Toss a biased coin to accept $\bar{x}$ as a sample coming from $q$. The coin bias is $\frac{q(\bar{x})}{Cp(\bar{x})}$ Intuitvely, you can think of this procedure as a correction scheme to account for the mismatch: how accurately $p(x)$ describes $q(x)$. Put it differently, the area under curve $Cp(x)$ is $C$, while under $q(x)$, it is $1$. And since $q(x)\leq Cp(x)$, if we full the area under $Cp(x)$ with points uniformly, then $1/C$ of these points will be under $q(x)$. In the demo below, we choose $p$ to be the uniform distribution: And $C$ to be $\max_{x} q(x)= 0.0833333$ C = q([0.,0.]) #max_x q(x) p = lambda x: 1 samples = [] num_samples = 10000 while len(samples) &amp;lt; num_samples: x = rnd.rand(2) if rnd.rand() &amp;lt;= ( q(x) / (C * p(x))): samples.append({'x_1':x[0], 'x_2':x[1]}) df = pd.DataFrame(samples) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') We can see how the histogram of the collected samples agrees to the mass distribution of $q$. However, this approach becomes challenging in higher dimensions: higher rejection probability $($which makes the process extremely slow$)$ and less known upper-bounding scaled distributions. For this, Markov Chain Monte Carlo $($MCMC$)$ methods come to the rescue Markov Chain Monte Carlo Based on the notion of stationary distribution, we can generate our samples from a dynamical systems over the states $($values$)$ which the random variable $x$ can take. That is, i. Specify a transition probability in line with the system dynamics $T(\bar{x}|x)$ $($or proposal distribution- to propose the next state $\bar{x}$ given the present state $x$ $)$. ii. Let the system interact, iii. The system will converge to the underlying $($unique$)$ distribution of $\pi$. Note that, if: there exists a unique stationary $\pi$ The challenge in this setup is how to define / sample from the valid transition distribution $($e.g., in higher dimensions$)$. Among other approaches we discuss two here. Gibbs Sampling Given a random $($initialized$)$ state of our systems variables $($here, $x_1$ and $x_2$$)$, one can condition one variable on the other to obtain a 1-D distriubution, which we assume we can sample from $($i.e., rejection sampling, central limit theorem, or other techniques$)$. Iteratively, we will obtain samples. Neverthelss, we need to give it some time till we converge to the stationary distribution of our system. In our example, we will make use of rejection sampling in 1-D to obtain our 1-D sample. q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6. # q(x_1|x_2) q_1 = lambda x: 6 * q(x)/((x[1]-0.5)**2 + 2. * 0.5**3 / 3.) # q(x_2|x_1) q_2 = lambda x: 6 * q(x)/((x[0]-0.5)**2 + 2. * 0.5**3 / 3.) # p (upper bounding), here we are using uniform dist p = lambda x: 1 def reject_uniform_sample(q, C): while True: x_ = rnd.rand() if rnd.rand() &amp;lt;= q(x_) /(C * 1.): return x_ # aribtrary value, by right we should use max_x q_1, max_x q_2 C_1 = 1.5 C_2 = 1.5 samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}] num_samples = 10000 while len(samples) &amp;lt; num_samples: x_1 = reject_uniform_sample(lambda x_: q_1([x_, samples[-1]['x_2']]), C_1) x_2 = reject_uniform_sample(lambda x_: q_2([samples[-1]['x_1'], x_]), C_2) samples.append({'x_1':x_1, 'x_2':x_2}) While burning: df = pd.DataFrame(samples[:50]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') After discarding some samples away: df = pd.DataFrame(samples[500:]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') Metropolis-Hastings Sampling In Gibbs sampling, the idea is to generate samples for each of our system’s variables from their posterior distribution with respect to the current values of the rest of the variables. Multi-dimensioanly sampling is reduced into a sequence of 1-D sampling procedures. We saw above, this approach might be a little bit slow and generates highly correlated samples $($again: we are generating samples that are highly probable given the past values of the rest of the variables$)$. Furthermore, what if we can’t compute nor sample from the posterior distribution. Can we propose transitions from one state to another arbitrarily and sitll converge? The idea of MH-sampling is to incorporate rejection sampling with aribtrary proposal distribution such that the transition probability $($which uniquely defines the Markov process$)$ can be constructed. Let’s step back and summarise the above: We would like to sample from our system’s distribution . We are using Markov Chain $($process$)$ to simulate our system of random variables. That is the Markov chain asymptotically converges to a stationary distribution and which is . To converge to $ q(x) $, we need to construct the corresponding chain’s transition probability $ T(\bar{x}|x) $ because it uniquely defines the process and hence its stationary distribution. One way of achieving that is through the detailed balance, sufficient but not a necessary condition: $ q(x) T(\bar{x}|x) = q(\bar{x}) T(x|\bar{x}) $ We can make use of the above formula to construct a transition probability that is a product of an arbitrary distribution $g(\bar{x}|x)$ , which is referred to as the proposal distribution as mentioned above, and the acceptance distribution $A(\bar{x}|x)$ Plugging this in the detailed balance condition, we have: Let’s see how we can realize this in code: Let’s define our and here we are assuming that we can sample efficiently from this 2-D Gaussian distribution $($ otherwise, we may use one of the techniques above as rejection sampling $)$. Notice here, there is nothing preventing us from sampling outside the support of $q(x)$, so we need to explicitly state that. I.e., $q(x)=0$ outside $q$’s support. Also, our choice of proposal distribution might affect the convergence progress. # target distribution (the one we'd like the MC to converge to) q = lambda x: ((x[0] - 0.5)**2 + (x[1] - 0.5)**2) / 6. # assuming 2-d Gaussian sampling is doable here (via function call) sigma = 0.5 propose_x = lambda x_prev: x_prev + sigma * np.dot(rnd.randn(1,2),np.eye(2))[0,:] # proposal distribution: from scipy.stats import multivariate_normal g = lambda x_new, x_prev: multivariate_normal.pdf(x_new, mean=x_prev, cov =sigma * np.eye(2)) # random initialization of the sample which we'll eventually discard samples = [{'x_1':rnd.rand(), 'x_2': rnd.rand()}] num_samples = 10000 while len(samples) &amp;lt; num_samples: x_prev = np.array([samples[-1]['x_1'], samples[-1]['x_2']]) x_new = propose_x(x_prev) # to discard samples out of support if max(x_new) &amp;gt; 1. or min(x_new) &amp;lt; 0.: continue if rnd.rand() &amp;lt;= min(1, q(x_new) * g(x_prev,x_new) / (q(x_prev) * g(x_new, x_prev))): samples.append({'x_1':x_new[0], 'x_2':x_new[1]}) df = pd.DataFrame(samples[:50]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde') df = pd.DataFrame(samples[1500:]) _ = sns.jointplot(x=df.x_1, y=df.x_2, kind='kde')</summary></entry><entry><title type="html">On the Positive Definitness of Multivariate Gaussian’s Covariance Matrix</title><link href="http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix/" rel="alternate" type="text/html" title="On the Positive Definitness of Multivariate Gaussian's  Covariance Matrix" /><published>2017-09-20T00:00:00-08:00</published><updated>2017-09-20T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix/">&lt;p&gt;$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\covmat}{\Sigma}
$&lt;/p&gt;

&lt;p&gt;In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&amp;gt;0$&lt;/li&gt;
  &lt;li&gt;$\Sigma_{ij} = \Sigma_{ji}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps:&lt;/p&gt;

&lt;p&gt;First, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from
&lt;script type=&quot;math/tex&quot;&gt;{\vx}^T\Sigma \vx=\sum_{ij} y_i y_j \Sigma_{ij}=\sum_{ij}y_i y_j E[(x_i - \bar{x}_i)(x_j - \bar{x}_j)]=E\big[\sum_{ij}y_i y_j (x_i - \bar{x}_i)(x_j - \bar{x}_j)\big]\;,&lt;/script&gt;
with $z_i =x_i - \bar{x}_i$, we have &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\vx^T\Sigma \vx=E[\sum_{ij} y_i z_iy_jz_j]=E[\sum_{i} y^2_iz^2_i + 2 \sum_{i&lt;j} y_iz_i y_j z_j]=E[(\vy^T\vz)^2]\geq 0\;, %]]&gt;&lt;/script&gt; since $(\vy^T\vz)^2\geq 0$.&lt;/p&gt;

&lt;p&gt;Second, for $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i }_{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors 
&lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{v}_i\}_{1\leq i \leq n}\;.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Therefore, &lt;script type=&quot;math/tex&quot;&gt;{\vx}^T \Sigma \vx= \sum_{ij} w_i \mathbf{v}^T_i \lambda_j w_j \mathbf{v}_j= \sum_{i} \lambda_i w^2_i &gt; 0\;.&lt;/script&gt;&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">$ \newcommand{\vx}{\mathbf{x}} \newcommand{\vy}{\mathbf{y}} \newcommand{\vz}{\mathbf{z}} \newcommand{\covmat}{\Sigma} $ In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&amp;gt;0$ $\Sigma_{ij} = \Sigma_{ji}$. The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps: First, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from with $z_i =x_i - \bar{x}_i$, we have since $(\vy^T\vz)^2\geq 0$. Second, for $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i }_{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors Therefore,</summary></entry></feed>