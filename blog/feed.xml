<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://ash-aldujaili.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://ash-aldujaili.github.io/blog/" rel="alternate" type="text/html" hreflang="en" /><updated>2017-06-21T09:31:23-08:00</updated><id>http://ash-aldujaili.github.io/blog/</id><title type="html">Hunting Optima</title><subtitle>Write-ups on optimization</subtitle><author><name>Abdullah Al-Dujaili</name></author><entry><title type="html">DACE/BLUP MSE for EGO</title><link href="http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace/" rel="alternate" type="text/html" title="DACE/BLUP MSE for EGO" /><published>2017-06-06T00:00:00-08:00</published><updated>2017-06-06T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace/">&lt;p&gt;$\newcommand{\hy}{\hat{y}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\rR}{\mathbf{r}^TR^{-1}}
\newcommand{\rRr}{\mathbf{r}^TR^{-1}\mathbf{r}}
\newcommand{\oRr}{\mathbf{1}^TR^{-1}\mathbf{r}}
\newcommand{\ymu}{(\mathbf{y}-\mathbf{1}\hmu)}
\newcommand{\hyexp}{\hmu+\rR\ymu}
\newcommand{\oRy}{\mathbf{1}^T\mathbf{R}
^{-1}\mathbf{y}}
\newcommand{\yRo}{\mathbf{y}^T\mathbf{R}
^{-1}\mathbf{1}}
\newcommand{\oRo}{\mathbf{1}^T\mathbf{R}^{-1}\mathbf{1}}
\newcommand{\hmuexp}{\frac{\oRy}{\oRo}}
\newcommand{\st}{\sigma^2}
\newcommand{\mt}{\mu^2}$
This post shows a derivation of the DACE predictor’s MSE discussed in &lt;a href=&quot;http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/&quot;&gt;Stochastic Processes for Expensive Black-Box Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The mean squared error of a predictor at $\mathbf{x}$ based on the stochastic Gaussian process is&lt;/p&gt;

&lt;p&gt;\begin{equation}MSE(\mathbf{x})=E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}\mathbf{r}(\mathbf{x})-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r}(\mathbf{x}))^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]\;,
\label{eq:final-mse}
\end{equation}&lt;/p&gt;

&lt;p&gt;where $\mu$ is process’s mean and $\sigma^2\mathbf{R}$ is its covariance matrix over a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$. For brevity, we will drop $\mathbf{x}$ henceforth. Before proceeding with the proof, let’s recall some terms and their definitions that will be useful in the proof.
\begin{equation}
\hy=\hyexp
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hmu=\hmuexp
\end{equation}&lt;/p&gt;

&lt;p&gt;From the above, we have 
$E[y^2]=\sigma^2 + \mu^2$, $E[\mathbf{y}\mathbf{y}^{T}]=\sigma^2\R+\mu^2\mathbf{1}\mathbf{1}^T$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[\hmu]=\frac{(\oRy)(\yRo)}{(\oRo)^2}=\sigma^2\cdot \frac{(\oRo)}{(\oRo)^2}+ \mu^2\cdot \frac{(\oRo)^2}{(\oRo)^2}=\frac{\st}{\oRo}+\mt&lt;/script&gt;

&lt;p&gt;Thus, we can expand the MSE term as&lt;/p&gt;

&lt;p&gt;\begin{equation}
MSE= \sigma^2 + \mu^2 + E[{\hy}^2] - 2 E[y\hy]\;.
\label{eq:mse}
\end{equation}
Where 
\begin{equation}
E[\hy^2]=\frac{\st}{\oRo}+\mt + \st (\rRr) - \st \frac{(\oRr)^2}{\oRo}
\label{eq:h2}
\end{equation}
and&lt;/p&gt;

&lt;p&gt;\begin{equation}
-2E[y\hy]=-2\st(\rRr)-2\mt-2\st\frac{\oRr}{\oRo}+2\st\frac{(\oRr)^2}{\oRo}\;.
\label{eq:h3}
\end{equation}&lt;/p&gt;

&lt;p&gt;Plugging Equations \ref{eq:h2} and \ref{eq:h3} into Eq. \ref{eq:mse} results in Eq. \ref{eq:final-mse}.&lt;/p&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">$\newcommand{\hy}{\hat{y}} \newcommand{\hmu}{\hat{\mu}} \newcommand{\R}{\mathbf{R}} \newcommand{\hmu}{\hat{\mu}} \newcommand{\rR}{\mathbf{r}^TR^{-1}} \newcommand{\rRr}{\mathbf{r}^TR^{-1}\mathbf{r}} \newcommand{\oRr}{\mathbf{1}^TR^{-1}\mathbf{r}} \newcommand{\ymu}{(\mathbf{y}-\mathbf{1}\hmu)} \newcommand{\hyexp}{\hmu+\rR\ymu} \newcommand{\oRy}{\mathbf{1}^T\mathbf{R} ^{-1}\mathbf{y}} \newcommand{\yRo}{\mathbf{y}^T\mathbf{R} ^{-1}\mathbf{1}} \newcommand{\oRo}{\mathbf{1}^T\mathbf{R}^{-1}\mathbf{1}} \newcommand{\hmuexp}{\frac{\oRy}{\oRo}} \newcommand{\st}{\sigma^2} \newcommand{\mt}{\mu^2}$ This post shows a derivation of the DACE predictor’s MSE discussed in Stochastic Processes for Expensive Black-Box Optimization. The mean squared error of a predictor at $\mathbf{x}$ based on the stochastic Gaussian process is \begin{equation}MSE(\mathbf{x})=E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}\mathbf{r}(\mathbf{x})-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r}(\mathbf{x}))^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]\;, \label{eq:final-mse} \end{equation} where $\mu$ is process’s mean and $\sigma^2\mathbf{R}$ is its covariance matrix over a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$. For brevity, we will drop $\mathbf{x}$ henceforth. Before proceeding with the proof, let’s recall some terms and their definitions that will be useful in the proof. \begin{equation} \hy=\hyexp \end{equation} \begin{equation} \hmu=\hmuexp \end{equation} From the above, we have $E[y^2]=\sigma^2 + \mu^2$, $E[\mathbf{y}\mathbf{y}^{T}]=\sigma^2\R+\mu^2\mathbf{1}\mathbf{1}^T$. Thus, we can expand the MSE term as \begin{equation} MSE= \sigma^2 + \mu^2 + E[{\hy}^2] - 2 E[y\hy]\;. \label{eq:mse} \end{equation} Where \begin{equation} E[\hy^2]=\frac{\st}{\oRo}+\mt + \st (\rRr) - \st \frac{(\oRr)^2}{\oRo} \label{eq:h2} \end{equation} and \begin{equation} -2E[y\hy]=-2\st(\rRr)-2\mt-2\st\frac{\oRr}{\oRo}+2\st\frac{(\oRr)^2}{\oRo}\;. \label{eq:h3} \end{equation} Plugging Equations \ref{eq:h2} and \ref{eq:h3} into Eq. \ref{eq:mse} results in Eq. \ref{eq:final-mse}.</summary></entry><entry><title type="html">Random Projections for Lipschitz Functions</title><link href="http://ash-aldujaili.github.io/blog/2017/05/08/rand-proj-lipschitz/" rel="alternate" type="text/html" title="Random Projections for Lipschitz Functions" /><published>2017-05-08T00:00:00-08:00</published><updated>2017-05-08T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/05/08/rand-proj-lipschitz</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/05/08/rand-proj-lipschitz/">&lt;p&gt;In this post, we’ll shed some light on the behavior of Lipschitz-continuous functions at random projections of the same point in another space. Consider Figure 1; we have a point in the $\mathbf{y}$ in the lower dimensional space $\mathcal{Y}$ that is projected randomly to the higher dimensional space $\mathcal{X}$ twice using two randomly sampled matrices $A_p$ and $A_q$. With this setting at hand, we are interested in the following question: is there a relation between the function values at $A_p\mathbf{y}$ and $A_q\mathbf{y}$. This has been addressed in [1, Theorem I]. Here, we provide a numerical validation of [1]’s result besides reiterating the formal proof. Before we delve into this further, let’s introduce some notation in accordance with [1].&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/illust_rand_lipschitz.png&quot; width=&quot;1000&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 1. Two random projections of $\mathbf{y}$ to $\mathcal{X}$.
&lt;/p&gt;

&lt;h4&gt;Notation&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/notation_rand_lipschitz.png&quot; width=&quot;600&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;

&lt;/p&gt;

&lt;h4&gt;Theorem I [1]&lt;/h4&gt;

&lt;p&gt;It has been shown in [1] that the mean variation in the objective value for a point $\mathbf{y}$ in the low-dimensional
space $\mathcal{Y} \subset \mathbb{R}^d$ projected randomly into the decision space $\mathcal{X}\subset\mathbb{R}^n$ of Lipschitz-continuous
problems is &lt;strong&gt;bounded&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Mathematically, for all $\mathbf{y} \in \mathcal{Y}$, we have
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}[|g_p(\mathbf{y}) − g_q(\mathbf{y})|] \leq \sqrt{8} \cdot L 
\cdot ||\mathbf{y}||\;.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here, we reproduce [1]’s proof for completeness, then we validate the theorem numerically.&lt;/p&gt;

&lt;h4&gt;Proof&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/proof_rand_lipschitz.png&quot; width=&quot;600&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;

&lt;/p&gt;
&lt;h4&gt;Numerical Validation&lt;/h4&gt;

&lt;p&gt;Here, we carry out a numerical validation of [1]’s Theorem I over four popular Lipschitz-like benchmark functions $($namely, Cigar, Sphere, Rastrigin, and Bohachevsky$)$.&lt;/p&gt;

&lt;p&gt;First, we sampled 100 points $\mathbf{y}\in\mathcal{Y}\subset \mathbb{R}^d$ whose norms span the range $[0,1]$. E.g., with $d=1$, $\mathbf{y}\in[-1,1]$.&lt;/p&gt;

&lt;p&gt;Second, each of these 100 points are projected to the function space $\mathcal{X}\subset\mathbb{R}^n$ using 20 random matrices ${A_p}_{1\leq p\leq 20}$.&lt;/p&gt;

&lt;p&gt;Third, the four considered functions are evaluated at these random projections. Note that for each of these functions, $100*20$ function evaluations are performed.&lt;/p&gt;

&lt;p&gt;Fourth, we average over the absolute difference between function values at projections of the same point $\mathbf{y}$. That is, for each of the 100 points, we compute the mean of $\frac{20*20- 20}{2}=190$ values, each of which representing the absolute difference of the function value at two random projections of the corresponding point $\mathbf{y}$.&lt;/p&gt;

&lt;p&gt;Fifth, the computed average values are plotted as a function of the  corresponding point $\mathbf{y}$ in Figure 1. One can see how the trend of the plot follows the curve of $||\mathbf{y}||$ in accordance with [1]’s Theorem I.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/rand_lipschitz.jpg&quot; width=&quot;1000&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 2. Empirical mean of the absolute value difference of four functions evaluated at 20 random projections in $\mathbb{R}^n$ of a point in $\mathbb{R}^d$, where $d=2$ and $n=10^3$.
&lt;/p&gt;
&lt;hr /&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;p&gt;This post is based on the following papers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Al-Dujaili, Abdullah, and S. Suresh.&lt;/strong&gt; &lt;em&gt;“Embedded Bandits for Large-Scale Black-Box Optimization.”&lt;/em&gt; Thirty-First AAAI Conference on Artificial Intelligence. 2017.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we’ll shed some light on the behavior of Lipschitz-continuous functions at random projections of the same point in another space. Consider Figure 1; we have a point in the $\mathbf{y}$ in the lower dimensional space $\mathcal{Y}$ that is projected randomly to the higher dimensional space $\mathcal{X}$ twice using two randomly sampled matrices $A_p$ and $A_q$. With this setting at hand, we are interested in the following question: is there a relation between the function values at $A_p\mathbf{y}$ and $A_q\mathbf{y}$. This has been addressed in [1, Theorem I]. Here, we provide a numerical validation of [1]’s result besides reiterating the formal proof. Before we delve into this further, let’s introduce some notation in accordance with [1]. Figure 1. Two random projections of $\mathbf{y}$ to $\mathcal{X}$. Notation Theorem I [1] It has been shown in [1] that the mean variation in the objective value for a point $\mathbf{y}$ in the low-dimensional space $\mathcal{Y} \subset \mathbb{R}^d$ projected randomly into the decision space $\mathcal{X}\subset\mathbb{R}^n$ of Lipschitz-continuous problems is bounded. Mathematically, for all $\mathbf{y} \in \mathcal{Y}$, we have Here, we reproduce [1]’s proof for completeness, then we validate the theorem numerically. Proof Numerical Validation Here, we carry out a numerical validation of [1]’s Theorem I over four popular Lipschitz-like benchmark functions $($namely, Cigar, Sphere, Rastrigin, and Bohachevsky$)$. First, we sampled 100 points $\mathbf{y}\in\mathcal{Y}\subset \mathbb{R}^d$ whose norms span the range $[0,1]$. E.g., with $d=1$, $\mathbf{y}\in[-1,1]$. Second, each of these 100 points are projected to the function space $\mathcal{X}\subset\mathbb{R}^n$ using 20 random matrices ${A_p}_{1\leq p\leq 20}$. Third, the four considered functions are evaluated at these random projections. Note that for each of these functions, $100*20$ function evaluations are performed. Fourth, we average over the absolute difference between function values at projections of the same point $\mathbf{y}$. That is, for each of the 100 points, we compute the mean of $\frac{20*20- 20}{2}=190$ values, each of which representing the absolute difference of the function value at two random projections of the corresponding point $\mathbf{y}$. Fifth, the computed average values are plotted as a function of the corresponding point $\mathbf{y}$ in Figure 1. One can see how the trend of the plot follows the curve of $||\mathbf{y}||$ in accordance with [1]’s Theorem I. Figure 2. Empirical mean of the absolute value difference of four functions evaluated at 20 random projections in $\mathbb{R}^n$ of a point in $\mathbb{R}^d$, where $d=2$ and $n=10^3$. References This post is based on the following papers: Al-Dujaili, Abdullah, and S. Suresh. “Embedded Bandits for Large-Scale Black-Box Optimization.” Thirty-First AAAI Conference on Artificial Intelligence. 2017.</summary></entry><entry><title type="html">Stochastic Processes for Expensive Black-Box Optimization</title><link href="http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/" rel="alternate" type="text/html" title="Stochastic Processes for Expensive Black-Box Optimization" /><published>2017-03-01T00:00:00-08:00</published><updated>2017-03-01T00:00:00-08:00</updated><id>http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt</id><content type="html" xml:base="http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/">&lt;p&gt;In this post, we’ll go through one of the techniques of Black-Box Deterministic Optimization: &lt;strong&gt;Response Surface Methdology&lt;/strong&gt;. In general, this approach models the objective and constraint functions with stochastic processes based on a few evaluations at points in the search space; with the goal of predicting the functions’ behavior as we move away from the evaluated points by different amounts in each coordinate direction.&lt;/p&gt;

&lt;p&gt;The stochastic process approach has been used in several fields under such names as ‘kriging’, ‘Bayesian optimization’, and ‘random function approach’. It comes with two main advantages:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Suitability for Expensive Optimization - as it quickly captures the trends over the search space.&lt;/li&gt;
  &lt;li&gt;Ease of establishing reasonable stopping rules - as it provide statistical confidence intervals.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4&gt;Modelling Alternatives&lt;/h4&gt;

&lt;p&gt;Let’s say we have evaluated a deterministic function,  $~y:\mathbb{R}^k\to \mathbb{R}$, at $n$ points. In other words, we have a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$ 
from the $k$-dimensional space, and we would like to use these observation in guiding our search for the optimal point. Let’s explore possible models that may help us in that.&lt;/p&gt;

&lt;hr /&gt;
&lt;h4&gt;1. &lt;strong&gt;Linear Regression&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;One can use linear regression as a simple way to fit a response surface to $\mathcal{D}$. In linear regression, the observations are assumed to be generated from the following model.&lt;/p&gt;

&lt;p&gt;\begin{equation}
y(\mathbf{x}^{(i)}) = \sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)}) + \epsilon^{(i)}\; (i = 1,\ldots,n).
\label{eq:lin-reg}
\end{equation}&lt;/p&gt;

&lt;p&gt;In the above equation, the $f_h(\mathbf{x})$’s are a set of linear or non-linear functions of $\mathbf{x}$; the $\beta_h$’s are unknown parameters of the model to be estimated; and the $\epsilon^{(i)}$’s are independent error terms with a normal distribution of zero mean and $\sigma^2$ variance.&lt;/p&gt;

&lt;p&gt;Employing Eq. \ref{eq:lin-reg} has two main concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A Conceptual Concern: Our function $f$ is deterministic. Therefore, any improper fit of Eq. \ref{eq:lin-reg} will be a modelling error—i.e., incomplete regression term $\sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)})$—rather than measurement noise or error—i.e., the $\epsilon^{(i)}$ term. In essence $\epsilon^{(i)}=\epsilon(\mathbf{x}^{(i)})$ captures the value of the left-out regression terms at $\mathbf{x}^{(i)}$. And if $y(\mathbf{x})$ is continuous, then $\epsilon(\mathbf{x})$ is continuous as well. Thus, if two points $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$ are close, then the errors $\epsilon(\mathbf{x}^{(i)})$ and $\epsilon(\mathbf{x}^{(j)})$ should be close as well.&lt;/li&gt;
  &lt;li&gt;A Practical Concern: How to define the $f_h$’s functions. Indeed, if we knew the perfect form of Eq. \ref{eq:lin-reg}, our function $f$ would not have been a black box in the first place. One can certainly employ a meta-model $($some referred to it as flexible functional form$)$ approach to decide the most suitable form of Eq. \ref{eq:lin-reg}. However, this approach comes with an additional set of parameters $($hyperparameters$)$ and requires many more function evaluations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In light of the above concerns, it makes no sense to assume that the error terms $\epsilon^{(i)}$’s are independent. Alternatively, we can assume that these terms are associated / related and that the level of such association $($correlation$)$ is low when the corresponding points $\mathbf{x}^{(i)}$’s are far away and high when they are close. The following equation captures this assumption of correlation.&lt;/p&gt;

&lt;p&gt;\begin{equation}
Corr[\epsilon(\mathbf{x}^{(i)}),\epsilon(\mathbf{x}^{(j)})] = \exp\big(- \sum_{h=1}^{k} \theta_h |x^{(j)}_h-x^{(j)}_h|^{p_h}\big)\;, \theta_h \geq 0\;, p_h \in [0,1]\;.
\label{eq:corr-fct}
\end{equation}&lt;/p&gt;

&lt;p&gt;The parameters $\theta_h$’s and $p_h$’s controls how fast the degree of correlation changes between two errors terms as the (weighted) distance between the corresponding points changes as shown in the figure below. In general, $\theta_h$ measures the importance / activity of the $h^{th}$ variable $x_h$ $($the higher $\theta_h$ is, the faster the correlation drops as the corresponding points move away in the coordinate direction $h)$, while $p_h$ captures the function smoothness in the coordinate direction $h$ $(p_h$ = 2 is for smooth functions$)$.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/err_corr.jpg&quot; width=&quot;400&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 1. Correlation Function $($Eq. \ref{eq:corr-fct}$)$ with different parameters.
&lt;/p&gt;

&lt;p&gt;Having modelled the correlation between our observations, fitting the regression term to our observations is less of a concern now. In fact, this forms the basis of the stochastic process model presented next.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;2. &lt;strong&gt;DACE&lt;/strong&gt; $($Design and Analysis of Computer Experiments [1]$)$&lt;/h4&gt;

&lt;p&gt;Based on the correlation model $($Eq. \ref{eq:corr-fct}$)$, one can view the
error term $\epsilon(\mathbf{x})$ as a stochastic process - a set of correlated random variables indexed by the $k$-dimensional space of $\mathbf{x}$. Therefore, the observations can be modeled with the following stochastic process:&lt;/p&gt;

&lt;p&gt;\begin{equation}
y^{(i)} = \mu + \epsilon(\mathbf{x}^{(i)})\; (i=1, \ldots, n).
\label{eq:dace}
\end{equation}&lt;/p&gt;

&lt;p&gt;In comparison to Eq. \ref{eq:lin-reg}, $\mu$ $($the mean of the stochastic process$)$ is the simplest regression term $($ a constant $)$; $\epsilon(\mathbf{x}^{(i)})\sim \mathcal{N}(0, \sigma^2)$ with a correlation given by Eq. \ref{eq:corr-fct}. This model is commonly referred to as &lt;strong&gt;DACE&lt;/strong&gt; - an acronym to the paper that popularized it [1].&lt;/p&gt;

&lt;p&gt;We can estimate DACE’s $2k+2$ parameters by maximizing the likelihood of our sample $\mathcal{D}$. With the correlation between two error terms defined as above, the vector $\mathbf{y}=[y^{(1)},\ldots, y^{(n)}]$ of observed function values is a normal random $n$-vector. That is, $\mathbf{y}\sim\mathcal{N}(\mu, \sigma^2\mathbf{R})$. Thus, the likelihood function of our samples can be written as follows.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 
\begin{equation}
f(\mathbf{y};\theta,p, \mu, \sigma) = 
\frac{1}
{(2\pi)^{n/2}(\sigma^2)^{n/2}|\mathbf{R}|^{(1/2)}}
\exp\big(-\frac{(\mathbf{y}-\mathbf{1}\mu)^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\mu)}{2\sigma^2}\big)
\label{eq:likelihood}
\end{equation}&lt;/p&gt;

&lt;p&gt;Eq. $\ref{eq:likelihood}$ can be optimized with respect to $\mu$ and $\sigma^2$ in closed form by taking the partial derivative of its log:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mu}=\frac{\mathbf{1}^{T}\mathbf{R}^{-1}\mathbf{y}}{\mathbf{1}^{T}\mathbf{R}^{-1}\mathbf{1}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\sigma}^2 = \frac{(\mathbf{y}-\mathbf{1}\hat{\mu})^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\hat{\mu})}{n}&lt;/script&gt;

&lt;p&gt;The above equations can in turn be plugged into the likelihood function to form a &lt;em&gt;concentrated likelihood function&lt;/em&gt;, which can be optimized with resepct to the parameters $\theta_h$’s and $p_h$’s. Subsequently, the corresponding values of $\hat{\mu}$ and $\hat{\sigma}^2$ are computed.&lt;/p&gt;

&lt;p&gt;Having estimated the parameters of our stochastic process model, we can now use it to predict the function value at new points in the $k$-dimensional space. The stochastic process model encodes the left-out terms of the regression model using the correlated errors. In other words, these correlated errors must be employed to adjust the prediction above or below the constant regression term $\hat{\mu}$. To see how this can be done, assume that we are interested in predicting the function value at $x^{(n+1)}$ and let $y^{*}$ be some prediction candidate for $f(x^{(n+1)})$. To measure how well $(x^{(n+1)},y^{*})$ fits the sample $\mathcal{D}$, one can compute the joint $($augmented$)$ likelihood function of $\tilde{\mathbf{y}} = (\mathbf{y}\; y^{*})$ - $f(\tilde{\mathbf{y}};\theta,p, \hat{\mu}, \hat{\sigma})$ from Eq. \ref{eq:likelihood}’s $f(\mathbf{y};\theta,p, \mu, \sigma)$. Taking the log of Eq. \ref{eq:likelihood}, it is easy to see that the only term that depends on $y^{*}$ is $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$ with 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\tilde{\mathbf{R}} = \begin{bmatrix}
    \mathbf{R}       &amp; \mathbf{r} \\
    \mathbf{r}^{T}     &amp; 1 
\end{bmatrix} %]]&gt;&lt;/script&gt;, where $\mathbf{r} = \mathbf{r}(\mathbf{x}^{(n+1)}) = [Corr[\epsilon(\mathbf{x}^{(1)}),\epsilon(\mathbf{x}^{(n+1)})],\ldots,Corr[\epsilon(\mathbf{x}^{(n)}),\epsilon(\mathbf{x}^{(n+1)})]]$.&lt;/p&gt;

&lt;p&gt;Applying the &lt;a href=&quot;https://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion&quot;&gt;blockwise inversion formula&lt;/a&gt; on $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bigg(y^{*}-\text{independent terms}\bigg) + \frac{1}{1-\mathbf{r}^{T}R^{-1}\mathbf{r}}(y^{*}-\hat{\mu})^2 - \frac{2\mathbf{r}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu})}{1-\mathbf{r}^{T}\mathbf{R}^{-1}\mathbf{r}}(y^{*}-\hat{\mu})&lt;/script&gt;

&lt;p&gt;Thus, setting the derivative of the above term with respect to $y^{*}$ to zero gives the best prediction’s formula at $\mathbf{x}^{(n+1)}$ that fits the sample $\mathcal{D}$ as follows.&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hat{y}(\mathbf{x}) = \hat{\mu} + {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu})
\label{eq:prediction}
\end{equation}&lt;/p&gt;

&lt;p&gt;While this value maximizes the joint augmented likelihood of $(x^{(n+1)},y^{*})$ and $\mathcal{D}$, it is interesting to note that it also corresponds to the mean of the normal probablity density function of the function value at $x^{(n+1)}$ conditioned on $\mathcal{D}$.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;http://ash-aldujaili.github.io/blog/assets/stoch_proc_pred.png&quot; width=&quot;400&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
  Figure 2. Application of Eq. \ref{eq:prediction}
&lt;/p&gt;

&lt;p&gt;Figure 2 shows predicted values of $f$ at different points according to Eq. \ref{eq:prediction}. DACE’s predictor $($Eq. \ref{eq:prediction}$)$ is also an interpolator of $\mathcal{D}$. That is
$\hat{y}(\mathbf{x}^{(i)}) = \hat{\mu} + {\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu}) = y^{(i)}$, since ${\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}={(\mathbf{R}^{-1}\mathbf{R}_i)}^{T}=\mathbf{e}^T_i$. Furthermore, the prediction accuracy can be characterized by the mean squared error, which can be proved to be as follows.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - \mathbf{r}^{T}\mathbf{R}^{-1}\mathbf{r}-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r})^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;TO BE COMPLETED…&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;p&gt;This post is based on the following papers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sacks, Jerome, et al.&lt;/strong&gt; &lt;em&gt;Design and analysis of computer experiments.&lt;/em&gt; Statistical science $($1989$)$: 409-423.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Jones, Donald R.; Schonlau, Matthias; Welch, William J.&lt;/strong&gt; &lt;em&gt;Efficient global optimization of expensive black-box functions&lt;/em&gt;. J. Global Optim. 13 $($1998$)$, no. 4, 455–492.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Make use of: $det(cA)= c^n det(A)$, where A is an $n\times n$-matrix; $Cov[x^{(i)},x^{(j)}] = \sigma^2 \cdot Corr[x^{(i)},x^{(j)}]$; and the multivariate normal probability density function.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;This is proved &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abdullah Al-Dujaili</name></author><summary type="html">In this post, we’ll go through one of the techniques of Black-Box Deterministic Optimization: Response Surface Methdology. In general, this approach models the objective and constraint functions with stochastic processes based on a few evaluations at points in the search space; with the goal of predicting the functions’ behavior as we move away from the evaluated points by different amounts in each coordinate direction. The stochastic process approach has been used in several fields under such names as ‘kriging’, ‘Bayesian optimization’, and ‘random function approach’. It comes with two main advantages: Suitability for Expensive Optimization - as it quickly captures the trends over the search space. Ease of establishing reasonable stopping rules - as it provide statistical confidence intervals. Modelling Alternatives Let’s say we have evaluated a deterministic function, $~y:\mathbb{R}^k\to \mathbb{R}$, at $n$ points. In other words, we have a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$ from the $k$-dimensional space, and we would like to use these observation in guiding our search for the optimal point. Let’s explore possible models that may help us in that. 1. Linear Regression One can use linear regression as a simple way to fit a response surface to $\mathcal{D}$. In linear regression, the observations are assumed to be generated from the following model. \begin{equation} y(\mathbf{x}^{(i)}) = \sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)}) + \epsilon^{(i)}\; (i = 1,\ldots,n). \label{eq:lin-reg} \end{equation} In the above equation, the $f_h(\mathbf{x})$’s are a set of linear or non-linear functions of $\mathbf{x}$; the $\beta_h$’s are unknown parameters of the model to be estimated; and the $\epsilon^{(i)}$’s are independent error terms with a normal distribution of zero mean and $\sigma^2$ variance. Employing Eq. \ref{eq:lin-reg} has two main concerns: A Conceptual Concern: Our function $f$ is deterministic. Therefore, any improper fit of Eq. \ref{eq:lin-reg} will be a modelling error—i.e., incomplete regression term $\sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)})$—rather than measurement noise or error—i.e., the $\epsilon^{(i)}$ term. In essence $\epsilon^{(i)}=\epsilon(\mathbf{x}^{(i)})$ captures the value of the left-out regression terms at $\mathbf{x}^{(i)}$. And if $y(\mathbf{x})$ is continuous, then $\epsilon(\mathbf{x})$ is continuous as well. Thus, if two points $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$ are close, then the errors $\epsilon(\mathbf{x}^{(i)})$ and $\epsilon(\mathbf{x}^{(j)})$ should be close as well. A Practical Concern: How to define the $f_h$’s functions. Indeed, if we knew the perfect form of Eq. \ref{eq:lin-reg}, our function $f$ would not have been a black box in the first place. One can certainly employ a meta-model $($some referred to it as flexible functional form$)$ approach to decide the most suitable form of Eq. \ref{eq:lin-reg}. However, this approach comes with an additional set of parameters $($hyperparameters$)$ and requires many more function evaluations. In light of the above concerns, it makes no sense to assume that the error terms $\epsilon^{(i)}$’s are independent. Alternatively, we can assume that these terms are associated / related and that the level of such association $($correlation$)$ is low when the corresponding points $\mathbf{x}^{(i)}$’s are far away and high when they are close. The following equation captures this assumption of correlation. \begin{equation} Corr[\epsilon(\mathbf{x}^{(i)}),\epsilon(\mathbf{x}^{(j)})] = \exp\big(- \sum_{h=1}^{k} \theta_h |x^{(j)}_h-x^{(j)}_h|^{p_h}\big)\;, \theta_h \geq 0\;, p_h \in [0,1]\;. \label{eq:corr-fct} \end{equation} The parameters $\theta_h$’s and $p_h$’s controls how fast the degree of correlation changes between two errors terms as the (weighted) distance between the corresponding points changes as shown in the figure below. In general, $\theta_h$ measures the importance / activity of the $h^{th}$ variable $x_h$ $($the higher $\theta_h$ is, the faster the correlation drops as the corresponding points move away in the coordinate direction $h)$, while $p_h$ captures the function smoothness in the coordinate direction $h$ $(p_h$ = 2 is for smooth functions$)$. Figure 1. Correlation Function $($Eq. \ref{eq:corr-fct}$)$ with different parameters. Having modelled the correlation between our observations, fitting the regression term to our observations is less of a concern now. In fact, this forms the basis of the stochastic process model presented next. 2. DACE $($Design and Analysis of Computer Experiments [1]$)$ Based on the correlation model $($Eq. \ref{eq:corr-fct}$)$, one can view the error term $\epsilon(\mathbf{x})$ as a stochastic process - a set of correlated random variables indexed by the $k$-dimensional space of $\mathbf{x}$. Therefore, the observations can be modeled with the following stochastic process: \begin{equation} y^{(i)} = \mu + \epsilon(\mathbf{x}^{(i)})\; (i=1, \ldots, n). \label{eq:dace} \end{equation} In comparison to Eq. \ref{eq:lin-reg}, $\mu$ $($the mean of the stochastic process$)$ is the simplest regression term $($ a constant $)$; $\epsilon(\mathbf{x}^{(i)})\sim \mathcal{N}(0, \sigma^2)$ with a correlation given by Eq. \ref{eq:corr-fct}. This model is commonly referred to as DACE - an acronym to the paper that popularized it [1]. We can estimate DACE’s $2k+2$ parameters by maximizing the likelihood of our sample $\mathcal{D}$. With the correlation between two error terms defined as above, the vector $\mathbf{y}=[y^{(1)},\ldots, y^{(n)}]$ of observed function values is a normal random $n$-vector. That is, $\mathbf{y}\sim\mathcal{N}(\mu, \sigma^2\mathbf{R})$. Thus, the likelihood function of our samples can be written as follows.1 \begin{equation} f(\mathbf{y};\theta,p, \mu, \sigma) = \frac{1} {(2\pi)^{n/2}(\sigma^2)^{n/2}|\mathbf{R}|^{(1/2)}} \exp\big(-\frac{(\mathbf{y}-\mathbf{1}\mu)^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\mu)}{2\sigma^2}\big) \label{eq:likelihood} \end{equation} Eq. $\ref{eq:likelihood}$ can be optimized with respect to $\mu$ and $\sigma^2$ in closed form by taking the partial derivative of its log: The above equations can in turn be plugged into the likelihood function to form a concentrated likelihood function, which can be optimized with resepct to the parameters $\theta_h$’s and $p_h$’s. Subsequently, the corresponding values of $\hat{\mu}$ and $\hat{\sigma}^2$ are computed. Having estimated the parameters of our stochastic process model, we can now use it to predict the function value at new points in the $k$-dimensional space. The stochastic process model encodes the left-out terms of the regression model using the correlated errors. In other words, these correlated errors must be employed to adjust the prediction above or below the constant regression term $\hat{\mu}$. To see how this can be done, assume that we are interested in predicting the function value at $x^{(n+1)}$ and let $y^{*}$ be some prediction candidate for $f(x^{(n+1)})$. To measure how well $(x^{(n+1)},y^{*})$ fits the sample $\mathcal{D}$, one can compute the joint $($augmented$)$ likelihood function of $\tilde{\mathbf{y}} = (\mathbf{y}\; y^{*})$ - $f(\tilde{\mathbf{y}};\theta,p, \hat{\mu}, \hat{\sigma})$ from Eq. \ref{eq:likelihood}’s $f(\mathbf{y};\theta,p, \mu, \sigma)$. Taking the log of Eq. \ref{eq:likelihood}, it is easy to see that the only term that depends on $y^{*}$ is $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$ with , where $\mathbf{r} = \mathbf{r}(\mathbf{x}^{(n+1)}) = [Corr[\epsilon(\mathbf{x}^{(1)}),\epsilon(\mathbf{x}^{(n+1)})],\ldots,Corr[\epsilon(\mathbf{x}^{(n)}),\epsilon(\mathbf{x}^{(n+1)})]]$. Applying the blockwise inversion formula on $(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})^{T}{\tilde{\mathbf{R}}}^{-1}(\tilde{\mathbf{y}}- \mathbf{1}\hat{\mu})$, we have: Thus, setting the derivative of the above term with respect to $y^{*}$ to zero gives the best prediction’s formula at $\mathbf{x}^{(n+1)}$ that fits the sample $\mathcal{D}$ as follows. \begin{equation} \hat{y}(\mathbf{x}) = \hat{\mu} + {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu}) \label{eq:prediction} \end{equation} While this value maximizes the joint augmented likelihood of $(x^{(n+1)},y^{*})$ and $\mathcal{D}$, it is interesting to note that it also corresponds to the mean of the normal probablity density function of the function value at $x^{(n+1)}$ conditioned on $\mathcal{D}$. Figure 2. Application of Eq. \ref{eq:prediction} Figure 2 shows predicted values of $f$ at different points according to Eq. \ref{eq:prediction}. DACE’s predictor $($Eq. \ref{eq:prediction}$)$ is also an interpolator of $\mathcal{D}$. That is $\hat{y}(\mathbf{x}^{(i)}) = \hat{\mu} + {\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{1}\hat{\mu}) = y^{(i)}$, since ${\mathbf{r}(\mathbf{x}^{(i)})}^{T}\mathbf{R}^{-1}={(\mathbf{R}^{-1}\mathbf{R}_i)}^{T}=\mathbf{e}^T_i$. Furthermore, the prediction accuracy can be characterized by the mean squared error, which can be proved to be as follows.2 TO BE COMPLETED… References This post is based on the following papers: Sacks, Jerome, et al. Design and analysis of computer experiments. Statistical science $($1989$)$: 409-423. Jones, Donald R.; Schonlau, Matthias; Welch, William J. Efficient global optimization of expensive black-box functions. J. Global Optim. 13 $($1998$)$, no. 4, 455–492. Make use of: $det(cA)= c^n det(A)$, where A is an $n\times n$-matrix; $Cov[x^{(i)},x^{(j)}] = \sigma^2 \cdot Corr[x^{(i)},x^{(j)}]$; and the multivariate normal probability density function.&amp;nbsp;&amp;#8617; This is proved here.&amp;nbsp;&amp;#8617;</summary></entry></feed>