<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Hunting Optima</title>
  <meta name="description" content="Write-ups on optimization">
  

  <link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="canonical" href="http://ash-aldujaili.github.io/blog/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Hunting Optima" href="http://ash-aldujaili.github.io/blog/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Hunting Optima">
  <meta name="twitter:description" content="Write-ups on optimization">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68645681-6', 'auto');
    ga('send', 'pageview');

  </script>


</head>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/blog/">Hunting Optima</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="http://ash-aldujaili.github.io/">Home</a>
      
        
        <a class="page-link" href="/blog/about/">About my blog</a>
      
        
        <a class="page-link" href="/blog/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/ash-aldujaili">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/09/20/covariance-matrix/">On the Positive Definitness of Multivariate Gaussian&#39;s  Covariance Matrix</a>
          </h1>

          <p class="post-meta">Sep 20, 2017</p>
        </header>

        <div class="post-content">
          <p>$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\covmat}{\Sigma}
$</p>

<p>In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is,</p>

<ol>
  <li>for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&gt;0$</li>
  <li>$\Sigma_{ij} = \Sigma_{ji}$.</li>
</ol>

<p>The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps:</p>

<ol>
  <li>
    <p>For all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from
<script type="math/tex">\vx^T\Sigma \vx=\sum_{ij} y_i y_j \Sigma_{ij}=\sum_{ij}y_i y_j E[(x_i - \bar{x}_i)(x_j - \bar{x}_j)]=E\big[\sum_{ij}y_i y_j (x_i - \bar{x}_i)(x_j - \bar{x}_j)\big]\;,</script>
with $z_i =x_i - \bar{x}_i$, we have <script type="math/tex">% <![CDATA[
\vx^T\Sigma \vx=E[\sum_{ij} y_i z_iy_jz_j]=E[\sum_{i} y^2_iz^2_i + 2 \sum_{i<j} y_iz_i y_j z_j]=E[(\vy^T\vz)^2]\geq 0\;, %]]></script> since $(\vy^T\vz)^2\geq 0$.</p>
  </li>
  <li>
    <p>For $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i}<em>{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors ${\mathbf{v}_i}</em>{1\leq i \leq n}$. Therefore, $\vx^T\Sigma \vx= \sum_{ij} w_i \mathbf{v}^T_i \lambda_j w_j \mathbf{v}<em>j= \sum</em>{i} \lambda_i w^2_i &gt; 0$.</p>
  </li>
</ol>


        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/08/01/quadratic-form-opt/">Quadratic Function Optimization via Linear Algebra</a>
          </h1>

          <p class="post-meta">Aug 1, 2017</p>
        </header>

        <div class="post-content">
          <p>In this post, we show how Linear Algebra can be used to solve one of the common optimization problems that arise in a variety of domains. In particular, we are interested in the quadratic optimization problem of the form
$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\lm}{\lambda}
\newcommand{\norm}[1]{||#1||_2}
$</p>

<p>\begin{align}
\text{max}_{\vx}\hspace{1.15cm} \vx^TA\vx \newline
\hspace{1em}\text{s.t.}\hspace{1em} \norm{\vx}=1
\label{eq:opt-problem}
\end{align}</p>

<p>where $A\in\mathbb{R}^{n^2}$ is a real symmetric $($square$)$ matrix, $\vx\in\mathbb{R}^n$. An optimal solution of this problem is $\vq_1\in\mathbb{R}^n$, the eigenvector that corresponds to $\lambda_1\in \mathbb{R}$, the largest eigenvalue of $A$.</p>

<p>Now that we have defined the problem, let’s try to solve it by making use of Linear Algebra, from which we know the following.</p>

<ol>
  <li>If $Q\in\mathbb{R}^{n^2}$ is an orthogonal matrix, then $Q^{T}Q=QQ^T=I_n$ where $I_n\in\mathbb{R}^{n^2}$ is the $n\times n$ identity matrix.</li>
  <li>Columns (and rows) of an orthogonal matrix have unit norm.</li>
  <li>Any real symmetric matrix $A$ can be decomposed into $Q\Lambda Q^T$, where $Q\in\mathbb{R}^{n^2}$ is an orthogonal matrix whose columns are the eigenvectors of $A$, while $\Lambda = diag(\mathbf{\lambda})$ is a diagonal matrix whose diagonal entries are the eign values of $A$ such that $\lambda_1\geq \lambda_2 \geq \cdots\geq \lambda_n$ and  $\lambda_i$ is the eigenvalue of the $i^{th}$ column of $Q$.</li>
</ol>

<p>From (3.), one can write $\vx^TA\vx$ as $\vx^T Q\Lambda Q^T \vx \leq \lambda_1 \vx^T QI_n Q^T \vx$. Based on (1.) and the constraint of $\vx$’s unit norm, the last term can further be simplified to have $\vx^TA\vx \leq \lambda_1 \norm{\vx}\leq \lambda_1$.</p>

<p>On the one hand, we found that $\max_{\vx} \vx^T A \vx = \lambda_1$. On the other hand, $A\vq_1 = \lambda_1\vq_1$, and thus $\vq_1^TA\vq_1 = \lambda_1$. Therefore, $\vq_1 \in \arg\max_{\vx} \vx^T A \vx$.</p>


        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/07/31/matrix-inversion/">A Practical Summary to Matrix Inversion</a>
          </h1>

          <p class="post-meta">Jul 31, 2017</p>
        </header>

        <div class="post-content">
          <p>$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\lm}{\lambda}
\newcommand{\norm}[1]{||#1||_2}
\newcommand{\mat}[1]{\left[#1\right]}
$</p>

<p>Matrix inversion is a handy procedure in solving a linear system of equations based on the notion of identity matrix $I_n\in\mathbb{R}^{n^2}$ whose all entries are zero except the entries along the main diagonal.</p>

<p>For a linear system $A\vx=\vb$ where $A\in\mathbb{R}^{m\times n}$ is a known matrix, $\vb\in\mathbb{R}^m$ is a known vector, and $\vx\in\mathbb{R}^n$ is a vector of unknown values, one can find a solution if a matrix $B\in\mathbb{R}^{n\times m}$ can be found such that $BA=I_n$ since for all $\vx \in \mathbb{R}^n$, $I_n \vx=\vx$. We refer to $B$ as the inverse of $A$ $($commonly denoted as $A^{-1})$. Mathematically,</p>

<script type="math/tex; mode=display">A\vx=\vb</script>

<script type="math/tex; mode=display">A^{-1}A\vx=A^{-1}\vb</script>

<script type="math/tex; mode=display">I_n\vx=A^{-1}\vb</script>

<script type="math/tex; mode=display">\vx=A^{-1}\vb</script>

<p>Of course, such solution can be found if $A$ is invertible. That is it is possible to find $A^{-1}$. I am writing this post to summarize possible scenarios one may face when inverting a matrix.</p>

<p>Before delving into the details, I intend to present a geometric view of linear systems of equations. This will help us understand why we could have exactly one, infinitely many, or no solution at all. And subsequently, why some matrices are invertible and others are not.</p>

<h3>Problem Setup</h3>
<p>One can think of $\vb$ as a point in the $m$-dimensional space. Furthermore, consider $A\vx$ as a linear combination $A$’s column vectors where the $i^{th}$ column $A_{:,i}$ is scaled by the $i^{th}$ entry of $\vx$, $x_i$. In other words, when we say that $\vb=A\vx$, we mean that $\vb$ is reachable from the origin $(\mathbf{0}\in\mathbb{R}^m)$ by moving $x_1$ units along the vector $A_{:,1}$, $x_2$ units along the vector $A_{:,2}$, …, and $x_n$ units along the vector $A_{:,n}$. For different $\vx\in\mathbb{R}^n$, we may reach different points in the $m$-dimensional space. The set of such points are referred to as the <strong>span</strong> of $A$ $($or <strong>range</strong> of $A$, <strong>column space</strong> of $A)$. That is, the set of points reachable by a linear combination of $A$’s columns. Mathematically, <script type="math/tex">span(A\text{'s columns})=\{b\in \mathbb{R}^m|\; b = A\vx,\; \vx\in \mathbb{R}^n\}\;.</script></p>

<p>Therefore, a solution exists for $A\vx=\vb$ if $\vb\in span(A\text{‘s columns})$. A necessary and sufficient condition for a solution to exist for all possible values of $\vb$ $(\mathbb{R}^m)$ is that $A$’s columns constitute at least one set of $m$ linearly independent vectors. Let’s note that there is no set of $m$-dimensional vectors has more than $m$ linearly indepdent vectors. The following examples provide an illustration.</p>

<ol>
  <li>$A=\mat{\begin{array}{cc} 1 &amp; 2 , 0 &amp; 0\end{array}}$,  $\vb =\mat{\begin{array}{c} 1 , 1\end{array}}$. In this example, $A$’s span is $\mathbb{R}^1$ along the vector $\mat{\begin{array}{c}1 , 0\end{array}}$ and no matter how $\vx$ is set, we can not reach $\vb$, because it does not lie at the intersection of its plane $\mathbb{R}^2$ and $A$’s space. With regard to the remark made above, we notice that $m=2$, but we do not have $2$ linearly independent vectors in $A$’s columns.</li>
  <li>$A=\mat{\begin{array}{cc} 1 &amp; 2 , 0 &amp; 0\end{array}}$,  $\vb =\mat{\begin{array}{c} 1 , 0\end{array}}$. In this example, $A$ is the same as $(1.)$’s but we moved $\vb$ to make it reachable. One may note that the effective dimension of the problem at hand is one (the second entry of all the columns is zero). In other words, we have matrix of $1\times 2$ with $m=1$ and we have $2$ linearly dependent vectors of length 1 that can reach $\vb$ through any solution that satisfies $x_1+ 2x_2=1$, for which there are infinitely many. Such example fulfills the sufficient and necessary condition aforementioned, but here we have more than $m$ columns. Thus, there are infinitely many solutions rather than exactly one.</li>
  <li>$A=\mat{\begin{array}{cc} 1 &amp; 0 , 0 &amp; 1\end{array}}$,  $\vb =\mat{\begin{array}{c} 1 , 1\end{array}}$. In this example, we made $A$ spans $\mathbb{R}^2$ with exactly $m=2$ linearly indepdendent vectors, and so we can reach $\vb$ with exactly one particular $\vx$. This example fulfils the sufficient and necessary condition with exactly $n=m$ columns.</li>
</ol>

<p>Based on the above, one can conclude the following:</p>

<ol>
  <li>When $A$ is a square matrix $(m=n)$ with $m$ mutually independent column vectors, there is exactly one solution for any $\vb\in\mathbb{R}^{m}$. Thus, $A$ is invertible.</li>
  <li>
    <p>When $A$ is a square matrix $(m=n)$ with linearly depdenent column vectors, there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible and we say that $A$ is a <strong>singular matrix</strong>.</p>
  </li>
  <li>
    <p>When $A$ is a rectangular matrix $(m&gt;n)$, there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible, and we say that the system is <strong>overdetermined</strong>.</p>
  </li>
  <li>
    <p>When $A$ is a rectangular matrix $(m&lt; n)$ with $m$ linearly indendent column vectors, there exist infinitely many solutions for any $\vb \in \mathbb{R}^m$. Thus, $A$ is non-invertible, and we say that the system is <strong>underdetermined</strong>.</p>
  </li>
  <li>When $A$ is a rectangular matrix $(m&lt; n)$ with $&lt; m$ linearly indendent column vectors, it is possible that there exists some $\vb$ for which no solution exists. Thus, $A$ is non-invertible. We still say the system is <strong>underdetermined</strong>.</li>
</ol>

<p>There are several procedures to compute $A^{-1}$ for invertible matrices such as the Gauss elimination method $($some others are listed in the figure at the bottom of this page$)$.</p>

<h3>Non-Invertible Matrices</h3>

<p>So what do we do when $A$ is non-invertible and we are still interested in finding a solution for a system? Well, we can use a pseudo-inverse matrix with some compromise on the solution’s quality as follows.</p>

<ul>
  <li>If there is <strong>no solution</strong>. That is we can’t reach $\vb$ and the system is overdetermined. Then, we can choose one with the least squares error. Mathematically,</li>
</ul>

<script type="math/tex; mode=display">\min_{\vx} ||A\vx-b||^2_2</script>

<p>Differentiating w.r.t $\vx$ and setting it to zero,</p>

<p><script type="math/tex">0=\nabla_{\vx}{((A\vx-b)^{T}(A\vx-b))}=A^{T}A\vx-A^T\vb</script>
yields $\vx=(A^{T}A)^{-1}A^T\vb$. Therefore, the pseudo-inverse matrix is $(A^{T}A)^{-1}A^T$</p>

<ul>
  <li>If there are <strong>infinitely many solutions</strong>. That is the system is underdetermined. Then, one can choose one with the minimum norm. Mathematically,</li>
</ul>

<p><script type="math/tex">\min_{\vx} ||\vx||^2_2</script> 
<script type="math/tex">\text{s.t.}\;\;\; A\vx =\vb</script></p>

<p>With the Lagrangian multipliers trick, the above can be solved as follows.</p>

<p>Differentiating w.r.t $\vx$ and setting it to zero,</p>

<script type="math/tex; mode=display">0=\nabla_{\vx}{(\vx^{T}\vx - \mathbf{\lambda}^{T}(A\vx-b))}= 2\vx - A^{T}\mathbf{\lambda}</script>

<p>$A^{T}$ is not invertible, but $AA^{T}$ is.<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> Therefore, we can solve for $\mathbf{\lambda}$ as follows.</p>

<script type="math/tex; mode=display">\mathbf{\lambda}= 2(AA^T)^{-1}A\vx=2(AA^T)^{-1}\vb</script>

<p>Thus, $\vx=A^T(AA^T)^{-1}\vb$</p>

<p>Interestingly, both of the above pseudo-inverses are equivalent to the <strong>Moore-Penrose</strong> pseudo-inverse which can be computed from the singular value decomposition $($SVD$)$.</p>

<h3>Ill-Conditioned Matrices</h3>

<p>Let’s motivate this issue with an example. Assume you are reading a measurement, which you have to scale by dividing by a very small number. If your reading deviates a little, the absolute error will extremely large. Such large oscillations might be undesirable and we’d like to dampen them. In the matrix world, dividing by a small number corresponds to to multiplying by the inverse of an ill-conditioned matrix. An ill-conditioned matrix is still <strong>invertible</strong>, but it is numerically unstable. In addition to the measurement error $($here the measurement is captured by $\vb$$)$, inverting an ill-conditioned matrix also suffers from loss of precision in floating point arithmetic.To cope with this unstability, we compromise the quality of the solution and use regularization techniques to have a stable numerical outcome. The bulk of these techniques aim to increase $($or replace$)$ the value of the smallest eigenvalue with respect to the greatest eigenvalue. Common techniques are:</p>

<ol>
  <li>Truncated SVD</li>
  <li>Tikhonov Regularization $($which can also be used for non-invertible matrices$)$.</li>
</ol>

<p>It is important to consider the regularization effect on the final solution and whether it makes sense to our system or not.</p>

<h3>Matrix Inversion Summary</h3>

<p>Below is a summary on matrix inversion.</p>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/matrix-inverse-summary.png" width="1000" />
<br /><br />
  Summary of Matrix Inverse Techniques.
</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>$AA^T$ is a square matrix. What is left to show is that columns of $AA^T$ are linearly independent, and thus $AA^T$ is invertible. We know that the columns of $A^T$ are linearly independent,  which means its null space is ${\mathbf{0}}$. By contradiction, let’s assume that $\mathbf{v}$ is a non-zero vector that belongs to  $AA^T$’s null space, which means $AA^T \mathbf{v}= \mathbf{0}$. Mutiplying both sides by $\mathbf{v}$ yields $(A^T \mathbf{v})^{T}(A^T \mathbf{v})=0$, i.e., $A^T \mathbf{v}= \mathbf{0}$. This contradicts the fact that $A^T$’s null space is the zero vector. Therefore, $\mathbf{v}$ can not be zero and $AA^T$’s null space has no non-zero vector, and all of its columns are linearly independent.&nbsp;<a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/06/06/mse-ego-dace/">DACE/BLUP MSE for EGO</a>
          </h1>

          <p class="post-meta">Jun 6, 2017</p>
        </header>

        <div class="post-content">
          <p>$\newcommand{\hy}{\hat{y}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\rR}{\mathbf{r}^TR^{-1}}
\newcommand{\rRr}{\mathbf{r}^TR^{-1}\mathbf{r}}
\newcommand{\oRr}{\mathbf{1}^TR^{-1}\mathbf{r}}
\newcommand{\ymu}{(\mathbf{y}-\mathbf{1}\hmu)}
\newcommand{\hyexp}{\hmu+\rR\ymu}
\newcommand{\oRy}{\mathbf{1}^T\mathbf{R}
^{-1}\mathbf{y}}
\newcommand{\yRo}{\mathbf{y}^T\mathbf{R}
^{-1}\mathbf{1}}
\newcommand{\oRo}{\mathbf{1}^T\mathbf{R}^{-1}\mathbf{1}}
\newcommand{\hmuexp}{\frac{\oRy}{\oRo}}
\newcommand{\st}{\sigma^2}
\newcommand{\mt}{\mu^2}$
This post shows a derivation of the DACE predictor’s MSE discussed in <a href="http://ash-aldujaili.github.io/blog/2017/03/01/exp-opt/">Stochastic Processes for Expensive Black-Box Optimization</a>.</p>

<p>The mean squared error of a predictor at $\mathbf{x}$ based on the stochastic Gaussian process is</p>

<p>\begin{equation}MSE(\mathbf{x})=E[(\hat{y}(\mathbf{x})-y(\mathbf{x}))^2] =\sigma \big[1 - {\mathbf{r}(\mathbf{x})}^{T}\mathbf{R}^{-1}\mathbf{r}(\mathbf{x})-\frac{(1-\mathbf{1}^T\mathbf{R}^{-1}\mathbf{r}(\mathbf{x}))^2}{(\mathbf{1}^T\mathbf{R}\mathbf{1})}\big]\;,
\label{eq:final-mse}
\end{equation}</p>

<p>where $\mu$ is process’s mean and $\sigma^2\mathbf{R}$ is its covariance matrix over a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$. For brevity, we will drop $\mathbf{x}$ henceforth. Before proceeding with the proof, let’s recall some terms and their definitions that will be useful in the proof.
\begin{equation}
\hy=\hyexp
\end{equation}</p>

<p>\begin{equation}
\hmu=\hmuexp
\end{equation}</p>

<p>From the above, we have 
$E[y^2]=\sigma^2 + \mu^2$, $E[\mathbf{y}\mathbf{y}^{T}]=\sigma^2\R+\mu^2\mathbf{1}\mathbf{1}^T$.</p>

<script type="math/tex; mode=display">E[\hmu]=\frac{(\oRy)(\yRo)}{(\oRo)^2}=\sigma^2\cdot \frac{(\oRo)}{(\oRo)^2}+ \mu^2\cdot \frac{(\oRo)^2}{(\oRo)^2}=\frac{\st}{\oRo}+\mt</script>

<p>Thus, we can expand the MSE term as</p>

<p>\begin{equation}
MSE= \sigma^2 + \mu^2 + E[{\hy}^2] - 2 E[y\hy]\;.
\label{eq:mse}
\end{equation}
Where 
\begin{equation}
E[\hy^2]=\frac{\st}{\oRo}+\mt + \st (\rRr) - \st \frac{(\oRr)^2}{\oRo}
\label{eq:h2}
\end{equation}
and</p>

<p>\begin{equation}
-2E[y\hy]=-2\st(\rRr)-2\mt-2\st\frac{\oRr}{\oRo}+2\st\frac{(\oRr)^2}{\oRo}\;.
\label{eq:h3}
\end{equation}</p>

<p>Plugging Equations \ref{eq:h2} and \ref{eq:h3} into Eq. \ref{eq:mse} results in Eq. \ref{eq:final-mse}.</p>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/05/08/rand-proj-lipschitz/">Random Projections for Lipschitz Functions</a>
          </h1>

          <p class="post-meta">May 8, 2017</p>
        </header>

        <div class="post-content">
          <p>In this post, we’ll shed some light on the behavior of Lipschitz-continuous functions at random projections of the same point in another space. Consider Figure 1; we have a point in the $\mathbf{y}$ in the lower dimensional space $\mathcal{Y}$ that is projected randomly to the higher dimensional space $\mathcal{X}$ twice using two randomly sampled matrices $A_p$ and $A_q$. With this setting at hand, we are interested in the following question: is there a relation between the function values at $A_p\mathbf{y}$ and $A_q\mathbf{y}$. This has been addressed in [1, Theorem I]. Here, we provide a numerical validation of [1]’s result besides reiterating the formal proof. Before we delve into this further, let’s introduce some notation in accordance with [1].</p>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/illust_rand_lipschitz.png" width="1000" />
<br /><br />
  Figure 1. Two random projections of $\mathbf{y}$ to $\mathcal{X}$.
</p>

<h4>Notation</h4>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/notation_rand_lipschitz.png" width="600" />
<br /><br />

</p>

<h4>Theorem I [1]</h4>

<p>It has been shown in [1] that the mean variation in the objective value for a point $\mathbf{y}$ in the low-dimensional
space $\mathcal{Y} \subset \mathbb{R}^d$ projected randomly into the decision space $\mathcal{X}\subset\mathbb{R}^n$ of Lipschitz-continuous
problems is <strong>bounded</strong>.</p>

<p>Mathematically, for all $\mathbf{y} \in \mathcal{Y}$, we have
<script type="math/tex">\mathbb{E}[|g_p(\mathbf{y}) − g_q(\mathbf{y})|] \leq \sqrt{8} \cdot L 
\cdot ||\mathbf{y}||\;.</script></p>

<p>Here, we reproduce [1]’s proof for completeness, then we validate the theorem numerically.</p>

<h4>Proof</h4>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/proof_rand_lipschitz.png" width="600" />
<br /><br />

</p>
<h4>Numerical Validation</h4>

<p>Here, we carry out a numerical validation of [1]’s Theorem I over four popular Lipschitz-like benchmark functions $($namely, Cigar, Sphere, Rastrigin, and Bohachevsky$)$.</p>

<p>First, we sampled 100 points $\mathbf{y}\in\mathcal{Y}\subset \mathbb{R}^d$ whose norms span the range $[0,1]$. E.g., with $d=1$, $\mathbf{y}\in[-1,1]$.</p>

<p>Second, each of these 100 points are projected to the function space $\mathcal{X}\subset\mathbb{R}^n$ using 20 random matrices ${A_p}_{1\leq p\leq 20}$.</p>

<p>Third, the four considered functions are evaluated at these random projections. Note that for each of these functions, $100*20$ function evaluations are performed.</p>

<p>Fourth, we average over the absolute difference between function values at projections of the same point $\mathbf{y}$. That is, for each of the 100 points, we compute the mean of $\frac{20*20- 20}{2}=190$ values, each of which representing the absolute difference of the function value at two random projections of the corresponding point $\mathbf{y}$.</p>

<p>Fifth, the computed average values are plotted as a function of the  corresponding point $\mathbf{y}$ in Figure 1. One can see how the trend of the plot follows the curve of $||\mathbf{y}||$ in accordance with [1]’s Theorem I.</p>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/rand_lipschitz.jpg" width="1000" />
<br /><br />
  Figure 2. Empirical mean of the absolute value difference of four functions evaluated at 20 random projections in $\mathbb{R}^n$ of a point in $\mathbb{R}^d$, where $d=2$ and $n=10^3$.
</p>
<hr />

<h4>References</h4>

<p>This post is based on the following papers:</p>

<ol>
  <li><strong>Al-Dujaili, Abdullah, and S. Suresh.</strong> <em>“Embedded Bandits for Large-Scale Black-Box Optimization.”</em> Thirty-First AAAI Conference on Artificial Intelligence. 2017.</li>
</ol>


        </div>
        
      </li>
    
  </ul>

  
  <div class="pagination">
    
      <a class="previous" href="/blog/posts/2/">&laquo; Older</a>
    

    
  </div>



</div>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Abdullah Al-Dujaili - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://ash-aldujaili.github.io/blog/feed.xml">RSS</a>

    </p>

  </div>

</footer>


    
  </body>

</html>
