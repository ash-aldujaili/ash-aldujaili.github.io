<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Hunting Optima</title>
  <meta name="description" content="Write-ups on optimization">
  

  <link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="canonical" href="http://ash-aldujaili.github.io//blog/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Hunting Optima" href="http://ash-aldujaili.github.io//blog/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Hunting Optima">
  <meta name="twitter:description" content="Write-ups on optimization">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68645681-6', 'auto');
    ga('send', 'pageview');

  </script>


</head>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/blog/">Hunting Optima</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="http://ash-aldujaili.github.io/">Home</a>
      
        
        <a class="page-link" href="/blog/about/">About my blog</a>
      
        
        <a class="page-link" href="/blog/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/ash-aldujaili">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/05/08/rand-proj-lipschitz/">Random Projections for Lipschitz Functions</a>
          </h1>

          <p class="post-meta">May 8, 2017</p>
        </header>

        <div class="post-content">
          <p>In this post, we’ll shed some light on the behavior of Lipschitz-continuous functions at random projections of the same point in another space. Consider the figure below. We have a point in the $\mathbf{y}$ in the lower dimensional space $\mathcal{Y}$ that is projected randomly to the higher dimensional space $\mathcal{X}$ twice using two randomly sampled matrices $A_p$ and $A_q$. With this setting at hand, we are interested in the following question: is there a relation between the function values at $A_p\mathbf{y}$ and $A_q\mathbf{y}$. This has been addressed in [1, Theorem I]. Here, we provide a numerical validation of [1]’s result besides reiterating the formal proof. Before we delve into this further, let’s introduce some notation in accordance with [1].</p>

<h4>Notation</h4>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/notation_rand_lipschitz.jpg" width="400" />
<br /><br />

</p>

<h4>Theorem I [1]</h4>

<p>It has been shown in [1] that the mean variation in the objective value for a point $\mathbf{y}$ in the low-dimensional
space $\mathcal{Y} \subset \mathbb{R}^d$ projected randomly into the decision space $\mathcal{X}\subset\mathbb{R}^n$ of Lipschitz-continuous
problems is <strong>bounded</strong>.</p>

<p>Mathematically, for all $\mathbf{y} \in \mathcal{Y}$, we have
<script type="math/tex">\mathbb{E}[|g_p(\mathbf{y}) − g_q(\mathbf{y})|] \leq \sqrt{8} \cdot L 
\cdot ||\mathbf{y}||\;.</script></p>

<p>Here, we reproduce [1]’s proof for completeness, then we validate the theorem numerically.</p>

<h4>Proof</h4>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/proof_rand_lipschitz.jpg" width="400" />
<br /><br />

</p>
<h4>Numerical Validation</h4>

<p>Figure 1 shows a numerical validation of [1]’s Theorem I. First, we sampled 100 points $\mathbf{y}\in\mathcal{Y}\subset \mathbb{R}^d$ whose norms span the range $[0,1]$. E.g., with $d=1$, $\mathbf{y}\in[-1,1]$.</p>

<p>Second, each of these 100 points are projected to the function space $\mathcal{X}\subset\mathbb{R}^n$ using 20 random matrices ${A_p}_{1\leq p\leq 20}$.</p>

<p>Third, four functions (namely, Cigar, Sphere, Rastrigin, and Bohachevsky) are evaluated at these random projections. Note that for each of these functions, $100*20$ function evaluations are performed.</p>

<p>Fourth, we average over the absolute difference between function values at projections of the same point $\mathbf{y}$. That is, for each of the 100 points, we compute the mean of $\frac{20*20- 20}{2}=190$ values, each of which representing the absolute difference of the function value at two random projections of the corresponding point $\mathbf{y}$.</p>

<p>Fifth, the computed average values are plotted as a function of the  corresponding point $\mathbf{y}$. One can see how the trend of the plot follows the curve of $||\mathbf{y}||$ in accordance with [1]’s Theorem I.</p>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/rand_lipschitz.jpg" width="400" />
<br /><br />
  Figure 1. Empirical mean of the absolute value difference of four functions evaluated at 20 random projections in $\mathbb{R}^n$ of a point in $\mathbb{R}^d$, where $d&lt;n$.
</p>
<hr />

<h4>References</h4>

<p>This post is based on the following papers:</p>

<ol>
  <li><strong>Al-Dujaili, Abdullah, and S. Suresh.</strong> <em>“Embedded Bandits for Large-Scale Black-Box Optimization.”</em> Thirty-First AAAI Conference on Artificial Intelligence. 2017.</li>
</ol>


        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/blog/2017/03/01/exp-opt/">Stochastic Processes for Expensive Black-Box Optimization</a>
          </h1>

          <p class="post-meta">Mar 1, 2017</p>
        </header>

        <div class="post-content">
          <p>In this post, we’ll go through one of the techniques of Black-Box Deterministic Optimization: <strong>Response Surface Methdology</strong>. In general, this approach models the objective and constraint functions with stochastic processes based on a few evaluations at points in the search space; with the goal of predicting the functions’ behavior as we move away from the evaluated points by different amounts in each coordinate direction.</p>

<p>The stochastic process approach has been used in several fields under such names as ‘kriging’, ‘Bayesian optimization’, and ‘random function approach’. It comes with two main advantages:</p>
<ol>
  <li>Suitability for Expensive Optimization - as it quickly captures the trends over the search space.</li>
  <li>Ease of establishing reasonable stopping rules - as it provide statistical confidence intervals.</li>
</ol>

<hr />

<h4>Modelling Alternatives</h4>

<p>Let’s say we have evaluated a deterministic function,  $~y:\mathbb{R}^k\to \mathbb{R}$, at $n$ points. In other words, we have a sample $\mathcal{D}={(x^{(i)},y^{(i)})}_{1\leq i \leq n}$ 
from the $k$-dimensional space, and we would like to use these observation in guiding our search for the optimal point. Let’s explore possible models that may help us in that.</p>

<hr />
<h4>1. <strong>Linear Regression</strong></h4>

<p>One can use linear regression as a simple way to fit a response surface to $\mathcal{D}$. In linear regression, the observations are assumed to be generated from the following model.</p>

<p>\begin{equation}
y(\mathbf{x}^{(i)}) = \sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)}) + \epsilon^{(i)}\; (i = 1,\ldots,n).
\label{eq:lin-reg}
\end{equation}</p>

<p>In the above equation, the $f_h(\mathbf{x})$’s are a set of linear or non-linear functions of $\mathbf{x}$; the $\beta_h$’s are unknown parameters of the model to be estimated; and the $\epsilon^{(i)}$’s are independent error terms with a normal distribution of zero mean and $\sigma^2$ variance.</p>

<p>Employing Eq. \ref{eq:lin-reg} has two main concerns:</p>

<ol>
  <li>A Conceptual Concern: Our function $f$ is deterministic. Therefore, any improper fit of Eq. \ref{eq:lin-reg} will be a modelling error—i.e., incomplete regression term $\sum_{h} \beta_{h}f_h(\mathbf{x}^{(i)})$—rather than measurement noise or error—i.e., the $\epsilon^{(i)}$ term. In essence $\epsilon^{(i)}=\epsilon(\mathbf{x}^{(i)})$ captures the value of the left-out regression terms at $\mathbf{x}^{(i)}$. And if $y(\mathbf{x})$ is continuous, then $\epsilon(\mathbf{x})$ is continuous as well. Thus, if two points $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$ are close, then the errors $\epsilon(\mathbf{x}^{(i)})$ and $\epsilon(\mathbf{x}^{(j)})$ should be close as well.</li>
  <li>A Practical Concern: How to define the $f_h$’s functions. Indeed, if we knew the perfect form of Eq. \ref{eq:lin-reg}, our function $f$ would not have been a black box in the first place. One can certainly employ a meta-model $($some referred to it as flexible functional form$)$ approach to decide the most suitable form of Eq. \ref{eq:lin-reg}. However, this approach comes with an additional set of parameters $($hyperparameters$)$ and requires many more function evaluations.</li>
</ol>

<p>In light of the above concerns, it makes no sense to assume that the error terms $\epsilon^{(i)}$’s are independent. Alternatively, we can assume that these terms are associated / related and that the level of such association $($correlation$)$ is low when the corresponding points $\mathbf{x}^{(i)}$’s are far away and high when they are close. The following equation captures this assumption of correlation.</p>

<p>\begin{equation}
Corr[\epsilon(\mathbf{x}^{(i)}),\epsilon(\mathbf{x}^{(j)})] = \exp\big(- \sum_{h=1}^{k} \theta_h |x^{(j)}_h-x^{(j)}_h|^{p_h}\big)\;, \theta_h \geq 0\;, p_h \in [0,1]\;.
\label{eq:corr-fct}
\end{equation}</p>

<p>The parameters $\theta_h$’s and $p_h$’s controls how fast the degree of correlation changes between two errors terms as the (weighted) distance between the corresponding points changes as shown in the figure below. In general, $\theta_h$ measures the importance / activity of the $h^{th}$ variable $x_h$ $($the higher $\theta_h$ is, the faster the correlation drops as the corresponding points move away in the coordinate direction $h)$, while $p_h$ captures the function smoothness in the coordinate direction $h$ $(p_h$ = 2 is for smooth functions$)$.</p>

<p align="center">
  <img src="http://ash-aldujaili.github.io/blog/assets/err_corr.jpg" width="400" />
<br /><br />
  Figure 1. Correlation Function $($Eq. \ref{eq:corr-fct}$)$ with different parameters.
</p>

<p>Having modelled the correlation between our observations, fitting the regression term to our observations is less of a concern now. In fact, this forms the basis of the stochastic process model presented next.</p>

<hr />

<h4>2. <strong>DACE</strong> $($Design and Analysis of Computer Experiments [1]$)$</h4>

<p>Based on the correlation model $($Eq. \ref{eq:corr-fct}$)$, one can view the
error term $\epsilon(\mathbf{x})$ as a stochastic process - a set of correlated random variables indexed by the $k$-dimensional space of $\mathbf{x}$. Therefore, the observations can be modeled with the following stochastic process:</p>

<p>\begin{equation}
y^{(i)} = \mu + \epsilon(\mathbf{x}^{(i)})\; (i=1, \ldots, n).
\label{eq:dace}
\end{equation}</p>

<p>In comparison to Eq. \ref{eq:lin-reg}, $\mu$ $($the mean of the stochastic process$)$ is the simplest regression term $($ a constant $)$; $\epsilon(\mathbf{x}^{(i)})\sim \mathcal{N}(0, \sigma^2)$ with a correlation given by Eq. \ref{eq:corr-fct}. This model is commonly referred to as <strong>DACE</strong> - an acronym to the paper that popularized it [1].</p>

<p>We can estimate DACE’s $2k+2$ parameters by maximizing the likelihood of our sample $\mathcal{D}$. With the correlation between two error terms defined as above, the vector $\mathbf{y}=[y^{(1)},\ldots, y^{(n)}]$ of observed function values is a normal random $n$-vector. That is, $\mathbf{y}\sim\mathcal{N}(\mu, \sigma^2\mathbf{R})$. Thus, the likelihood function of our samples can be written as follows.<sup><a href="#myfootnote1">1</a></sup></p>

<p>\begin{equation}
f(\mathbf{y};\theta,p, \mu, \sigma) = 
\frac{1}
{(2\pi)^{n/2}(\sigma^2)^{n/2}|\mathbf{R}|^{(1/2)}}
\exp\big(-\frac{(\mathbf{y}-\mathbf{1}\mu)^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\mu)}{2\sigma^2}\big)
\label{eq:likelihood}
\end{equation}</p>

<p>Eq. $\ref{eq:likelihood}$ can be optimized with respect to $\mu$ and $\sigma^2$ in closed form by taking the partial derivative of its log:</p>

<script type="math/tex; mode=display">\hat{\mu}=\frac{\mathbf{1}^{T}\mathbf{R}^{-1}\mathbf{y}}{\mathbf{1}^{T}\mathbf{R}^{-1}\mathbf{1}}</script>

<script type="math/tex; mode=display">\hat{\sigma}^2 = \frac{(\mathbf{y}-\mathbf{1}\hat{\mu})^{T} \mathbf{R}^{-1} (\mathbf{y}-\mathbf{1}\hat{\mu})}{n}</script>

<p><a name="myfootnote1">1</a>: Make use of: $det(cA)= c^n det(A)$, where A is an $n\times n$-matrix; $Cov[x^{(i)},x^{(j)}] = \sigma^2 \cdot Corr[x^{(i)},x^{(j)}]$; and the multivariate normal probability density function.</p>

<p><strong>TO BE COMPLETED…</strong></p>

<h4>References</h4>

<p>This post is based on the following papers:</p>

<ol>
  <li>
    <p><strong>Sacks, Jerome, et al.</strong> <em>Design and analysis of computer experiments.</em> Statistical science $($1989$)$: 409-423.</p>
  </li>
  <li>
    <p><strong>Jones, Donald R.; Schonlau, Matthias; Welch, William J.</strong> <em>Efficient global optimization of expensive black-box functions</em>. J. Global Optim. 13 $($1998$)$, no. 4, 455–492.</p>
  </li>
</ol>

        </div>
        
      </li>
    
  </ul>

  


</div>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Abdullah Al-Dujaili - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://ash-aldujaili.github.io//blog/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
