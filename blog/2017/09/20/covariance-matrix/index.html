<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>On the Positive Definitness of Multivariate Gaussian&#39;s  Covariance Matrix</title>
  <meta name="description" content="$ \newcommand{\vx}{\mathbf{x}} \newcommand{\vy}{\mathbf{y}} \newcommand{\vz}{\mathbf{z}} \newcommand{\covmat}{\Sigma} $ In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&amp;gt;0$ $\Sigma_{ij} = \Sigma_{ji}$. The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps: First, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from with $z_i =x_i - \bar{x}_i$, we have since $(\vy^T\vz)^2\geq 0$. Second, for $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i }_{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors Therefore,">
  

  <link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="canonical" href="http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Hunting Optima" href="http://ash-aldujaili.github.io/blog/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="On the Positive Definitness of Multivariate Gaussian&#39;s  Covaria...">
  <meta name="twitter:description" content="$ \newcommand{\vx}{\mathbf{x}} \newcommand{\vy}{\mathbf{y}} \newcommand{\vz}{\mathbf{z}} \newcommand{\covmat}{\Sigma} $ In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^...">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68645681-6', 'auto');
    ga('send', 'pageview');

  </script>


</head>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/blog/">Hunting Optima</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="http://ash-aldujaili.github.io/">Home</a>
      
        
        <a class="page-link" href="/blog/about/">About my blog</a>
      
        
        <a class="page-link" href="/blog/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/ash-aldujaili">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">On the Positive Definitness of Multivariate Gaussian&#39;s  Covariance Matrix</h1>
                                       

    <p class="post-meta"><time datetime="2017-09-20T00:00:00-08:00" itemprop="datePublished">Sep 20, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>$
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\covmat}{\Sigma}
$</p>

<p>In this short post, we show why the covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ of a multivariate Gaussian $\vx\in\mathbb{R}^n$ is always symmetric positive definite. That is,</p>

<ol>
  <li>for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy&gt;0$</li>
  <li>$\Sigma_{ij} = \Sigma_{ji}$.</li>
</ol>

<p>The latter condition $(2)$ follows from the definition of the covariance matrix and the commutative property of multiplication. For condition $(1)$, we can prove it in two steps:</p>

<p>First, for all non-zero vector $\vy \in \mathbb{R}^n$, $\vy^T\Sigma \vy \geq 0$. This follows from
<script type="math/tex">{\vx}^T\Sigma \vx=\sum_{ij} y_i y_j \Sigma_{ij}=\sum_{ij}y_i y_j E[(x_i - \bar{x}_i)(x_j - \bar{x}_j)]=E\big[\sum_{ij}y_i y_j (x_i - \bar{x}_i)(x_j - \bar{x}_j)\big]\;,</script>
with $z_i =x_i - \bar{x}_i$, we have <script type="math/tex">% <![CDATA[
\vx^T\Sigma \vx=E[\sum_{ij} y_i z_iy_jz_j]=E[\sum_{i} y^2_iz^2_i + 2 \sum_{i<j} y_iz_i y_j z_j]=E[(\vy^T\vz)^2]\geq 0\;, %]]></script> since $(\vy^T\vz)^2\geq 0$.</p>

<p>Second, for $\Sigma$ to hold as a covariance matrix for a multivariate Gaussian, it must be invertible. That is to say, its rank is $n$, i.e., $n$ non-zero eigenvalues ${\lambda_i }_{1\leq i \leq n}$ which are also positive $($from step 1$)$. This means that all non-zero vectors $\vx \in \mathbb{R}^n$ can be written as linear combinations of $\Sigma$’s eigen vectors 
<script type="math/tex">\{\mathbf{v}_i\}_{1\leq i \leq n}\;.</script></p>

<p>Therefore, <script type="math/tex">{\vx}^T \Sigma \vx= \sum_{ij} w_i \mathbf{v}^T_i \lambda_j w_j \mathbf{v}_j= \sum_{i} \lambda_i w^2_i > 0\;.</script></p>


  </div>

  
    <div id="disqus_thread"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
  
    var disqus_config = function () {
    this.page.url = "http://ash-aldujaili.github.io/blog/2017/09/20/covariance-matrix/"; //'http://ash-aldujaili.github.io/blog/2017/06/06/mse-ego-dace/';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "/2017/09/20/covariance-matrix"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ash-aldujaili.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

   

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Abdullah Al-Dujaili - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://ash-aldujaili.github.io/blog/feed.xml">RSS</a>

    </p>

  </div>

</footer>


    
  </body>

</html>
